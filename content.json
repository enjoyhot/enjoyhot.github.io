{"meta":{"title":"ENJOYHOT","subtitle":null,"description":null,"author":"enjoyhot","url":"http://enjoyhot.github.io"},"pages":[{"title":"About","date":"2017-03-31T14:13:04.713Z","updated":"2017-03-31T14:13:04.685Z","comments":true,"path":"about/index.html","permalink":"http://enjoyhot.github.io/about/index.html","excerpt":"","text":"Someone you would know from articles.Someday you would get what you want.Something you would know from space below.· I Am· Jiawei Gu,· from· South China University of Technology."},{"title":"Categories","date":"2016-07-16T12:17:39.534Z","updated":"2016-07-16T12:17:39.534Z","comments":true,"path":"categories/index.html","permalink":"http://enjoyhot.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2016-07-16T12:17:39.534Z","updated":"2016-07-16T12:17:39.534Z","comments":true,"path":"tags/index.html","permalink":"http://enjoyhot.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Apache大数据组件","slug":"Apache大数据组件","date":"2017-04-30T09:24:08.000Z","updated":"2017-04-30T09:53:32.592Z","comments":true,"path":"2017/04/30/Apache大数据组件/","link":"","permalink":"http://enjoyhot.github.io/2017/04/30/Apache大数据组件/","excerpt":"概述大数据的处理模式大体可以分为批处理（也可称为离线计算）、流式计算（数据实时性高）、在线处理（即时响应）和交互式分析（允许分钟级）四种。 大数据的4V属性数量（Volume），多样性（Variety），速度（Velocity），真实性（Veracity） HDFS组成组件","text":"概述大数据的处理模式大体可以分为批处理（也可称为离线计算）、流式计算（数据实时性高）、在线处理（即时响应）和交互式分析（允许分钟级）四种。 大数据的4V属性数量（Volume），多样性（Variety），速度（Velocity），真实性（Veracity） HDFS组成组件 HDFS是以master/slave模式运行的，其中NameNode、SecondaryNameNode 通常运行在master节点，DataNode运行slave节点。 NameNode 管理文件系统的Namespace，维护文件系统树以及树中所有文件和文件夹的元数据（metadata）；处理客户端请求。 SecondaryNameNode相当于一个Checkpoint Node，监控HDFS状态的辅助后台程序，与NameNode进行通信，以便定期地保存HDFS元数据的快照。 DataNode文件系统的工作节点，当需要通过客户端读/写某个数据时，先由NameNode告诉客户端去哪个DataNode进行具体的读/写操作，然后，客户端直接与这个DataNode服务器上的后台程序进行通信，并且对相关的数据块进行读/写操作。 不同版本的比较Hadoop 1.x单点故障问题由于只有一个NameNode，因此会出现单点故障问题。有多种方式可以解决： 将hadoop元数据写入到本地文件系统的同时再实时同步到一个远程挂载的网络文件系统（NFS）（在2.X中同样适用）； 运行一个secondaryNameNode进行冷备份，它的作用是与NameNode进行交互，定期将编辑日志文件(edit logs)合并为命名空间镜像，当NameNode发生故障时它会通过自己合并的命名空间镜像副本来恢复。 secondaryNameNode保存的状态总是滞后于NameNode，所以这种方式难免会导致丢失部分数据。 NameNode中存放的元信息文件是fsimage，操作期间所有对元信息的操作都保存在内存中并被持久化到另一个文件edit logs中。edit logs文件和fsimage文件会被SecondaryNameNode周期性的合并。注：http://blog.csdn.net/xh16319/article/details/31375197 fsimage ：保存的是上个检查点的HDFS的元信息； edits ：保存的是从上个检查点开始发生的HDFS元信息状态改变信息。 按照HDFS的设计，NameNode不会主动发起任何请求，只会被动接受来自客户端或DataNode的请求。 SecondaryNameNode合并流程： 首先，secondaryNameNode定时到NameNode去获取edit logs，并更新到fsimage上。[笔者注：Secondary NameNode自己的fsimage] 一旦它有了新的fsimage文件，它将其拷贝回NameNode中。 NameNode在下次重启时会使用这个新的fsimage文件，从而减少重启的时间。 Hadoop 2.xHA在2.X中，HDFS的变化，主要体现在增强了NameNode的水平扩展及可用性，可以同时部署多个NameNode，这些NameNodes之间是相互独立，也就是说他们不需要相互协调，DataNode同时在所有NameNodes注册，做为他们共有的存储节点，并定时向所有的这些NameNodes发送心跳块使用情况的报告，并处理所有NameNodes向其发送的指令。多个NameNode之间共享数据，可以通过Nnetwork File System或者Quorum Journal Node。 NFS方案； 基于Paxos的QJM(Quorum Journal Node)方案，它的基本原理就是用2N+1台JournalNode存储EditLog，每次写数据操作有大多数（&gt;=N+1）返回成功时即认为该次写成功。在HDFS运行时，同一时刻只有一个NameNode处于active状态，另一个处于standby状态。standbyNamenode作为“休眠方”，只进行数据同步，维护着数据状态，随时准备切换。 附：HDFS中的沟通协议 HDFS文件读写hdfs文件读取过程 客户端发起读请求； 客户端与NameNode得到文件的块及位置信息列表； 客户端直接和DataNoie交互读取数据； 读取完成关闭连接； hdfs文件写入过程hdfs有一个DistributedFileSystem实例，客户端通过调用这个实例的create()方法就可以创建文件： DistributedFileSystem会发送给NameNode一个RPC调用，在文件系统的命名空间创建一个新文件，在创建文件前NameNode会做一些检查，如文件是否存在，客户端是否有创建权限等，若检查通过，NameNode会为创建文件写一条记录到本地磁盘的EditLog，若不通过会向客户端抛出IOException。创建成功之后DistributedFileSystem会返回一个FSDataOutputStream对象，客户端由此开始写入数据。 客户端在向NameNode请求之前先写入文件数据到本地文件系统的一个临时文件； 待临时文件达到块大小时开始向NameNode请求DataNode信息； NameNode在文件系统中创建文件并返回给客户端一个数据块及其对应DataNode的地址列表（列表中包含副本存放的地址）； 客户端通过上一步得到的信息把创建临时文件块flush到列表中的第一个DataNode； 当文件关闭，NameNode会提交这次文件创建，此时，文件在文件系统中可见。 不适合的场景 大量小文件：文件的元数据(命名空间信息，块信息等)都存储在NameNode内存中，大量小文件会占用大量内存。 低延迟数据访问：hdfs是专门针对高数据吞吐量而设计的 多用户写入，任意修改文件，HDFS设计上是为了适合一次写入，多次使用 Kafka基本概念 topic：消息存放的目录即主题 Producer：生产消息到topic的一方 Consumer：订阅topic消费消息的一方 Broker：Kafka的服务实例就是一个broker HA Partition分布在集群的每一台server上，而每一个Partition在集群中都可以有多个备份，这个备份数量是可配置的。 每个Partition都有一个leader server，而其他备份的server都称为followers，只有leader服务器才会处理这个Partition上所有的读写请求，而其它followers则被动地复制leader上的数据。如果一个leader挂掉了，followers中的一个服务器则会自动升级为leader。同一个机器可以有多个Partition。 应用场景 Kafka基本应用图如下所示： 使用场景负责消费Partition的每个消费者都是一个消费进程，而且消费者本身也可以是多线程的应用程序，因为一个Partition只能属于一个消费者线程，所以存在如下几种不同的场景： 线程数量多于Partition的数量，有部分线程无法消费该topic下任何一条消息 线程数量少于Partition的数量，有一些线程会消费多个Partition的数据(这是最好的场景) 线程数量等于Partition的数量，则正好一个线程消费一个Partition的数据 消费模式消息传递通常由两种模式，queuing（队列）和publish-subscribe （发布-订阅） queuing: 每个Consumer从消息队列中取走一个消息 pub-scrib: 消息被广播到每个Consumer 实际上，kafka通过提供了一个对Consumer的抽象来同时实现这两种模式——ConsumerGroup。 当同一个 Topic （一个producer一个topic）的 consumer 配置 group.id 相同时，即为 queue模式。否则用不同的group.id时，则以public-subscribe模式工作。 消息顺序性Partition顺序性消息在一个Partition中的顺序是有序的，但是Kafka只保证消息在一个Partition中有序，如果要想使整个topic中的消息有序，那么一个topic仅设置一个Partition即可，或者在producer端控制每个partition消息应有的顺序。 Partition Offset 生产者的提交日志采用递增的offset连同消息内容一起写入到本地日志文件，生产者客户端本身不需要保存offset相关的状态。 消费者进程则要保存消费消息的offset，因此它是有状态的，这样消费者才能将消息的消费进度保存到ZK或者其他存储系统中。在顺序读取过程中，通过offset记录每条日志对于每个组的当前消费进度。 Rebalance 一个消费组有多个消费者，因此消费组需要维护所有的消费者，如果一个消费者宕掉了，分配给这个消费者的Partition需要被重新分配给相同组的其他消费者； 如果一个消费者加入了同一个组，之前分配给其他消费组的Partition需要分配给新加入的消费者。 实际上一旦有消费者加入或退出消费组，导致消费组成员列表发生变化，即使Kafka集群的Partition没有变化，消费组中所有的消费者也都要触发重新rebalance的工作。当然如果集群的Partition发生变化，即使消费组成员没有变化，所有的消费者也都要重新rebalance。 消费者消费消息时需要定时地将最新的消费进度保存到ZooKeeper中，当发生rebalance时，新的消费者拥有的新的Partition都可以从ZooKeeper中读取出来并恢复到最近的状态。 Push vs Pullkafka的consumer之所以没有采用push模式，是因为push模式很难适应消费者速率不同的消费者而且很难实现消息的回放功能，因为消息发送速率是由broker决定的。push模式的目标就是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞，而pull模式则可以根据consumer的消费能力以适当的速率消费message。http://kafka.apache.org/documentation.html#design_pull 客户端API 在0.9.0版本中：This new unified consumer API removes the distinction between the 0.8 high-level and low-level consumer APIs。https://kafka.apache.org/090/documentation.html#newconsumerapi。但还是可以用0.8之前的low level API，在0.10之后则官方doc连low level提也不提。可见high level API将渐渐成为开发者唯一操作接口。 Hight Level Consumer高级API提供了一个从Kafka消费数据的高层抽象，消费者客户端代码不需要管理offset的提交，并且采用了消费组的自动负载均衡功能，确保消费者的增减不会影响消息的消费； Low Level Consumer低级API通常针对特殊的消费逻辑（比如消费者只想要消费某些特定的Partition），低级API的客户端代码需要自己实现一些和Kafka服务端相关的底层逻辑，比如选择Partition的Leader，处理Leader的故障转移等。 磁盘存储特点Kafka使用磁盘进行数据的存储，默认有效期为7天，而不是采用内存，主要好处有： 磁盘缓存由Linux系统维护，减少了程序员的不少工作。 磁盘顺序读写速度超过内存随机读写。 JVM的GC效率低，内存占用大,使用磁盘可以避免这一问题。 系统冷启动后，磁盘缓存依然可用。http://blog.csdn.net/endlu/article/details/51392905 zookeeperZooKeeper是以Fast Paxos算法为基础，实现同步服务，配置维护和命名服务等分布式应用。对于Zookeeper集群而言，设定有2n+1台server，只要有n+1台依然能运行，就可以使用，继续提供服务。 配置1234# 集群配置，3台机器，2888为Leader服务端口，3888为选举时所用的端口server.1=cu01:2888:3888server.2=cu02:2888:3888server.3=cu03:2888:3888 数据模型zookeeper 会维护一个具有层次关系的数据结构，它非常类似于一个标准的文件系统： 概念 znode: 每个目录项称为znode; zookeeper 的客户端和服务器通信采用长连接方式，每个客户端和服务器通过心跳来保持连接，这个连接状态称为 session，如果 znode 是临时节点，这个 session 失效，znode 也就删除了； znode 是有版本的，每个znode中存储的数据可以有多个版本，也就是一个访问路径中可以存储多份数据。 四种形式的目录节点(String create()的createMode选项) PERSISTENT：持久化目录节点，这个目录节点存储的数据不会丢失； PERSISTENT_SEQUENTIAL：顺序自动编号的目录节点，这种目录节点会根据当前已经存在的节点数自动加 1，然后返回给客户端已经成功创建的目录节点名； EPHEMERAL：临时目录节点，一旦创建这个节点的客户端与服务器端口也就是 session 超时，这种节点会被自动删除； EPHEMERAL_SEQUENTIAL：临时自动编号节点。 zookeeper结构 角色 Leader作为整个ZooKeeper集群的主节点，负责响应所有对ZooKeeper状态变更的请求。它会将每个状态更新请求进行排序和编号，以便保证整个集群内部消息处理的FIFO。 Follower响应本服务器上的读请求外，follower还要处理leader的提议，并在leader提交该提议时在本地也进行提交。leader和follower构成ZooKeeper集群的法定人数，也就是说，只有他们才参与新leader的选举、响应leader的提议。 Observerobserver服务器用于提高读取的吞吐量。Observer和Follower比较相似，只有一些小区别：首先observer不属于法定人数，即不参加选举也不响应提议；其次是observer不需要将事务持久化到磁盘，一旦observer被重启，需要从leader重新同步整个名字空间。 节点状态每个集群中的节点都有一个状态LOOKING,FOLLOWING,LEADING,OBSERVING，每个节点启动的时候都是LOOKING状态，如果这个节点参与选举但最后不是leader，则状态是FOLLOWING，如果不参与选举则是OBSERVING，leader的状态是LEADING。 选取Leader的过程 专有概念 zxid：每个ZooKeeper服务器保存在磁盘的事务id,在初始阶段，每台服务器的这个值都是自己的id(高32位是epoch，低32位用于递增计数)； epoch：逻辑时钟的值，每次选举leader这个值会加1;用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch，标识当前属于那个leader的统治时期。 Zk的选举算法有两种一种是基于basic paxos实现的,另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。 Basic paxos： 当前Server发起选举的线程担任选举线程，其主要功能是对投票结果进行统计，并选出推荐的Server； 选举线程首先向所有Server发起一次询问(包括自己)； 选举线程收到回复后，验证是否是自己发起的询问(验证zxid是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议的leader相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中； 收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成下一次要投票的Server。在收集齐的过程中，可能有个别server网络好，就更新了自己要选的那个server； 如果此时获胜的Server获得n/2+1的Server票数，设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。 Fast Paxos： 每个Server会发出一个投票。初始情况，将自己作为Leader服务器来进行投票。每次投票包含的基本的元素包括：所推举的服务器的myid和ZXID,以(myid，ZXID)的形式来表示。因为是初始化阶段，因此无论是Server1还是Server2，都会投给自己，即Server1的投票为(1，0)，Server2的投票为(2,0)，然后各自将这个投票发给集群中其他所有机器。 接收来自各个服务器的投票每个服务器都会接收来自其他服务器的投票。集群中的每个服务器在接收到投票后，首先会判断该投票的有效性，包括检査是否是本轮投票、是否来自LOOKING 状态的服务器。 处理投票在接收到来自其他服务器的投票后，服务器都需要将别人的投票和自己的投票进行PK： 优先检査Epoch，Epoch高的作为Leader。 在检査ZXID。ZXID比较大的服务器优先作为Leader。 如果Epoch、ZXID相同的话，那么就比较myid。myid比较大的服务器作为Leader服务器。 对于Server1来说，它自己的投票是(1，0)，而接收到的投票为(2,0)。首先会对比两者的Epoch，我们假设两个的Epoch相同，再比较ZXID，因为都是0,所以无法决定谁是Leader。接下来会对比两者的myid，很显然，Server1发现接收到的投票中的myid是2,大于自id，于是就会更新自己的投票为(2,0)，然后重新将投票发出去。而对于Server2来说，不需要更新自己的投票信息。 统计投票。毎次投票后，服务器都会统计所有投票，判断是否已经有过半的机器接收到相同的投票信息。有的话，设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。 选完leader以后，zk就进入状态同步过程。 leader等待server连接； Follower连接leader，将最大的zxid发送给leader； Leader根据follower的zxid确定同步点； 完成同步后通知follower 已经成为uptodate状态； Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。 应用ZooKeeper主要是用来维护和监控一个目录节点树中存储的数据的状态，所有操作ZooKeeper和操作目录节点树大体一样，如 创建一个目录节点； 给某个目录节点设置数据； 获取某个目录节点的所有子目录节点； 给某个目录节点设置权限和监控这个目录节点的状态变化。 Zookeeper 从设计模式角度来看，是一个基于观察者模式设计的分布式服务管理框架，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应，从而实现集群中类似 Master/Slave 管理模式： 统一命名服务（Name Service）：分布式中统一命名； 配置管理（Configuration Management）：软件运行中可动态修改配置； 集群管理（Group Membership）：选出一个“总管”知道当前集群每台机器的服务状态，涉及zookeeper中Leader Election的功能； Zookeeper实现 Leader Election，也就是选出一个 Master Server。普通监听：每台 Server 创建一个 EPHEMERAL 目录节点，并调用父目录节点getChilden监听，此时不同的是它还是一个SEQUENTIAL 目录节点，所以它是个 EPHEMERAL_SEQUENTIAL目录节点。之所以它是 EPHEMERAL_SEQUENTIAL 目录节点，是因为可以给每台 Server 编号，可以选择当前最小编号的 Server 为 Master，假如这个最小编号的 Server 死去，由于是 EPHEMERAL 节点，死去的 Server 对应的节点也被删除，所以当前的节点列表中又出现一个最小编号的节点，我们就选择这个节点为当前 Master。这样就实现了动态选择 Master，避免了传统意义上单 Master 容易出现单点故障的问题。 共享锁：要获得锁的server创建EPHEMERAL_SEQUENTIAL目录节点，对父节点getChilden判断当前server编号是否是同级最小编号，是就获得了锁，释放就删除； 创建同步队列（用得较少）：通过不断增加目录，增加时到没到数目，到了就创建一个监控标志位目录（xx/start），各个节点监听到有这个节点就触发监听程序，然后移除成员，不到就继续等待。 HBase(注：Zookeeper的引入使得Master不再是单点故障) HBase主要优点 高效的储存空间利用率传统的行式数据库由于每个列的长度不一，为了预防更新的时候不至于出现一行数据(没值的字段需要补值)跳到另一个block上去，所以往往会预留一些空间。而面向列的数据库由于一开始就完全为分析而存在，不需要考虑少量的更新问题，所以数据完全是密集储存的。 不可见索引在已经读取了可能的数据块之后，对于类似age &lt; 65或job=’Axx’的，列式数据库并不需要扫描完整个block，因为数据已经排序了。如果读到第一个age=66或者Job=‘Bxx’的时候就会停止扫描了。这相当与行式数据库索引里的范围扫描。 压缩算法列式数据库由于其每一列都是分开储存的。所以很容易针对每一列的特征运用不同的压缩算法。 延迟物化列式数据库由于其特殊的执行引擎，在数据中间过程运算的时候一般不需要解压数据而是以指针代替运算，直到最后需要输出完整的数据时才解压缩。 列式数据库特点 优点 极高的装载速度（最高可以等于所有硬盘IO的总和，基本是极限了） 适合大量的数据而不是小数据 实时加载数据仅限于增加（删除和更新需要解压缩Block然后计算然后重新压缩储存） 高效的压缩率，不仅节省储存空间也节省计算内存和CPU。 非常适合做聚合操作。 缺点 不适合扫描小量数据 不适合随机的更新 批量更新情况各异，有的优化的比较好的列式数据库（比如Vertica）表现比较好，有些没有针对更新的数据库表现比较差。 不适合做含有删除和更新的实时操作 物理模型 每个column family存储在HDFS上的一个单独文件中，空值不会被保存； Table在行的方向上分割为多个Region，Region按大小分割的，每个表开始只有一个region，随着数据增多，region不断增大，当增大到一个阀值的时候，region就会等分会两个新region，之后会有越来越多的region； Region是Hbase中分布式存储和负载均衡的最小单元（最小存储单元是store——对应一个columnfamily），不同Region分布到不同RegionServer上。 Master 为Region server分配region； 负责Region server的负载均衡； 发现失效的Region server并重新分配其上的region； 管理用户对table的增删改查操作； 因此，无Master过程中： 数据读取仍照常进行； region切分、负载均衡等无法进行； HBase的compactHFile数量过多会降低读性能。为了避免对读性能的影响，可以对这些HFile(Store中的Storefile)进行compact操作，把多个HFile合并成一个HFile。compact操作需要对HBase的数据进行多次的重新读写，因此这个过程会产生大量的IO。因此compact操作的本质就是以IO操作换取后续的读性能的提高。 HBase的compact是针对HRegion的HStore进行操作的。compact操作分为major和minor两种： major会把HStore所有的HFile都compact为一个HFile，并同时忽略标记为delete的KeyValue（被删除的KeyValue只有在compact过程中才真正被”删除”），可以想象major会产生大量的IO操作，对HBase的读写性能产生影响。 minor则只会选择数个HFile文件compact为一个HFile，minor的过程一般较快，而且IO相对较低。在日常任务时间，都会禁止major操作，只在空闲的时段定时执行。 读写性能讨论 HBase的写入速度快是因为它其实并不是真的立即写入文件中，而是先写入内存，随后异步刷入HFile。所以在客户端看来，写入速度很快。另外，写入时候将随机写入转换成顺序写，数据写入速度也很稳定。因此，用户写操作只需要进入到内存即可立即返回，从而保证I/O高性能。 读取速度快是因为它使用了LSM树型结构。LSM树原理把一棵大树拆分成N棵小树，它首先写入内存中，随着小树越来越大，内存中的小树会flush到磁盘中，磁盘中的树定期可以做merge操作，合并成一棵大树，以优化读性能。LSM树实际上是牺牲了部分读性能，用来大幅提高写性能。 磁盘的顺序读取速度很快，但是相比而言，寻找磁道的速度就要慢很多。HBase的存储结构导致它需要磁盘寻道时间在可预测范围内，并且读取与所要查询的rowkey连续的任意数量的记录都不会引发额外的寻道开销。比如有5个存储文件，那么假设要到HFile中读取数据，最多需要5次磁盘寻道就可以。HBase读取首先会在缓存（BlockCache）中查找，它采用了LRU（最近最少使用算法），如果缓存中没找到，会从内存中的MemStore中查找，只有这两个地方都找不到时，才会加载HFile中的内容。在HFile中保存的内容是有序的，当数据写入HFile后，内存中的数据会被丢弃。HFile文件为磁盘顺序读取做了优化，按页存储。因此，读取HFile速度也会很快，因为节省了寻道开销。而关系型数据库，即使有索引，也无法确定磁盘寻道次数。 与zookeeper的关系-ROOT-和.META.是HBase的两张内置表，从存储结构和操作方法的角度来说，它们和其他HBase的表没有任何区别，你可以认为这就是两张普通的表，对于普通表的操作对它们都适用。它们与众不同的地方是HBase用它们来存贮一个重要的系统信息——Region的分布情况以及每个Region的详细信息。 参考：http://blog.csdn.net/woshiwanxin102213/article/details/17584043 调优更多：http://www.cnblogs.com/shitouer/archive/2012/08/07/2626377.html hbase中hfile（storefile）的默认最大值(hbase.hregion.max.filesize)是256MB，而google的bigtable论文中对tablet的最大值也推荐为100-200MB，这个大小有什么秘密呢？ 众所周知hbase中数据一开始会写入memstore，当memstore满64MB以后，会flush到disk上而成为storefile。当storefile数量超过3时，会启动compaction过程将它们合并为一个storefile。这个过程中会删除一些timestamp过期的数据，比如update的数据。而当合并后的storefile大小大于hfile默认最大值时，会触发split动作，将它切分成两个region。 链接中作者声明进行了持续insert压力测试，并设置了不同的hbase.hregion.max.filesize，根据结果得到如下结论： 值越小，平均吞吐量越大，但吞吐量越不稳定； 值越大，平均吞吐量越小，吞吐量不稳定的时间相对更小。 为什么会这样呢？推论如下： 当hbase.hregion.max.filesize较小时，触发split的机率更大，而split的时候会将region offline，因此在split结束的时间前，访问该region的请求将被block住，客户端自我block的时间默认为1s。当大量的region同时发生split时，系统的整体访问服务将大受影响。因此容易出现吞吐量及响应时间的不稳定现象 当hbase.hregion.max.filesize比较大时，单个region中触发split的机率较小，大量region同时触发split的机率也较小，因此吞吐量较之小hfile尺寸更加稳定些。但是由于长期得不到split，因此同一个region内发生多次compaction的机会增加了。compaction的原理是将原有数据读一遍并重写一遍到hdfs上，然后再删除原有数据。无疑这种行为会降低以io为瓶颈的系统的速度，因此平均吞吐量会受到一些影响而下降。 综合以上两种情况，hbase.hregion.max.filesize不宜过大或过小，256MB或许是一个更理想的经验参数。对于离线型的应用，调整为128MB会更加合适一些，而在线应用除非对split机制进行改造，否则不应该低于256MB。 rowkey设计原则 长度原则Rowkey的长度被很多开发者建议说设计在10~100个字节，不过建议是越短越好，不要超过16个字节。原因如下： 数据的持久化文件HFile中是按照KeyValue存储的，如果Rowkey过长比如100个字节，1000万列数据光Rowkey就要占用100*1000万=10亿个字节，将近1G数据，这会极大影响HFile的存储效率； MemStore将缓存部分数据到内存，如果Rowkey字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此Rowkey的字节长度越短越好。 目前操作系统是都是64位系统，内存8字节对齐。控制在16个字节，8字节的整数倍利用操作系统的最佳特性。 散列原则 如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver实现负载均衡的几率。 如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个 RegionServer上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。 hbase shell操作delete操作 每个记录有个保存于memstore的记录，在持久化前，记录新旧程度的”ts”123put &apos;key&apos;, &apos;value&apos;, ts=1put &apos;key&apos;, &apos;value&apos;, ts=2del &apos;key&apos;, &apos;value&apos;, ts=3 如上所示，进行两次put操作再加一次delete操作，合并后，就只有delete的操作了。 Hbase有一个TTL(time to live)，可以标识数据的有效期，比如，可以把TTL设置成86400*1000，也就是说数据将于1天后过期。这是一个表级的设置，必须在建表时指定。本质上也是利用ts(timestamp)进行比较，当前ts与put之后的ts进行比较，若大于TTL则删除。注意，ts可人工改，要谨慎，不然太大就表示该记录长时间有效，影响正常操作。 创建表12# create [tableName],[family1],&lt;family2&gt;,...,&lt;familyN &gt;create &apos;t1&apos;,&apos;f1&apos; 添加数据1put &lt;table&gt;,&lt;rowkey&gt;,&lt;family:column&gt;,&lt;value&gt;,&lt;timestamp&gt; e.g.1hbase(main):003:0&gt; put &apos;t1&apos;,&apos;rowkey001&apos;,&apos;f1:col1&apos;,&apos;value01&apos; 查找数据1get &lt;table&gt;,&lt;rowkey&gt;,[&lt;family:column&gt;,....] e.g.1hbase(main)&gt; get &apos;t1&apos;,&apos;rowkey001&apos;, &apos;f1:col1&apos; 扫描表数据如下，获取前五条rowkey相对应的数据：123456789101112131415161718hbase(main):017:0&gt; scan &apos;FeatureIndex0.01_v2&apos;,&#123;LIMIT=&gt;5&#125;ROW COLUMN+CELL 0@1|0@0 column=label:avg, timestamp=1483110072314, value=0.0554465387563 0@1|0@0 column=label:stddev, timestamp=1483110072314, value=0.228849788944 0@1|0@0 column=label:sum, timestamp=1483110072314, value=500383.0 0@1|0@1 column=label:avg, timestamp=1483110072312, value=0.0510849704785 0@1|0@1 column=label:stddev, timestamp=1483110072312, value=0.220171615499 0@1|0@1 column=label:sum, timestamp=1483110072312, value=10149.0 0@1|1@0 column=label:avg, timestamp=1483110072323, value=0.0442181954694 0@1|1@0 column=label:stddev, timestamp=1483110072323, value=0.20557989582 0@1|1@0 column=label:sum, timestamp=1483110072323, value=12721.0 100@101|0@0 column=label:avg, timestamp=1483110244726, value=0.0549538811079 100@101|0@0 column=label:stddev, timestamp=1483110244726, value=0.227890231369 100@101|0@0 column=label:sum, timestamp=1483110244726, value=519447.0 100@101|0@1 column=label:avg, timestamp=1483110244726, value=0.061603404489 100@101|0@1 column=label:stddev, timestamp=1483110244726, value=0.240437952631 100@101|0@1 column=label:sum, timestamp=1483110244726, value=1795.05 row(s) in 0.1300 seconds 删除表12truncate &lt;table&gt;# 其具体过程是：disable table -&gt; drop table -&gt; create table Lambda架构123batch view = function(all data)realtime view = function(realtime view, new data)query = function(batch view, realtime view) 12batchview = function(all data)；query = function(batch view)。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://enjoyhot.github.io/categories/分布式/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://enjoyhot.github.io/tags/分布式/"},{"name":"Apache","slug":"Apache","permalink":"http://enjoyhot.github.io/tags/Apache/"}]},{"title":"Spark计算平台","slug":"Spark计算平台","date":"2017-04-30T09:23:09.000Z","updated":"2017-04-30T09:29:40.176Z","comments":true,"path":"2017/04/30/Spark计算平台/","link":"","permalink":"http://enjoyhot.github.io/2017/04/30/Spark计算平台/","excerpt":"前言官方文档 Spark doc 相关平台 databricks blog A place to discuss and ask questions about using Scala for Spark programming","text":"前言官方文档 Spark doc 相关平台 databricks blog A place to discuss and ask questions about using Scala for Spark programming Spark jira SPARKHUB 学习文档 Spark Internals CoolplaySpark 注： Spark更新较快，具体操作查看官网doc可能更详细。 架构集群基础图 来自官网说明的集群结构http://spark.apache.org/docs/latest/cluster-overview.html Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run. 由此可知： Spark说到底就是集群中的一系列由SparkContext协调的进程，而SparkContext对象驻留在driver程序中； SparkContext作为driver程序中最重要的部分，需要连接到cluster manager从而获取资源的分配，资源主要是executor（一种计算资源的抽象，可以理解为进程资源）； 当你运行Spark程序后，将通过SparkContext提交代码到executor中，并在之后SparkContext通过某种方式指定具体的task分配到executor中运行。 相关组件说明 Driver Program：请求Executor启动Task等，数据归约操作收集端； Cluster Manager：standalone 集群管理器或 Mesos/YARN Worker Node：一台机器默认一个worker (multi only for standalone) Executor Process : 默认一个worker节点的一个JVM实例，服务于单个spark app，执行 Task任务，一个worker可以有多个executor实例，对于yarn而言，可通过–num-executors加以设置，对于standalone而言，可通过–total-executer-cores和–executor-cores结合设置；一个Spark app可以有多个job（action），一个job可以有多个stage（shuffle data）,一个stage可以有多个task（partition, 开发者视角）； DAGScheduler：根据Job构建基于Stage的DAG，并提交Stage给TaskScheduler； TaskScheduler：将Taskset提交给Worker node 集群运行并返回结果,一个应用对应一个TaskScheduler； 启动程序运行模式Spark的运行模式取决于传递给SparkContext的MASTER环境变量的值。master URL可以是以下任一种形式： local 使用一个Worker线程本地化运行SPARK local[*]使用逻辑CPU个数数量的线程来本地化运行Spark local[K]使用K个Worker线程本地化运行Spark（理想情况下，K应该根据运行机器的CPU逻辑核数设定） standalone官方自己开发的集群模式，地址为spark://HOST:PORT，连接到指定的Spark standalone master。默认端口是7077。利用该模式不能解决单点故障问题，可以使用zookeeper解决该问题。 yarn yarn-client以客户端模式连接YARN集群。 yarn-cluster 以集群模式连接YARN集群。集群的位置可以在HADOOP_CONF_DIR 环境变量中找到，默认Hadoop namenode 8088端口。 mesosmesos://HOST:PORT 连接到指定的Mesos集群。默认接口是5050. 运行方式该部分查看Spark2.1源码入口程序而知，以后更新版本可能会有所改变不得而知。 Spark程序运行包括交互式运行(除spark-submit和restful api的形式)和脚本提交。以下从交互式方式说起。 程序提交 spark-shell bash脚本启动交互式scala环境，脚本内部执行main函数，调用spark-submit脚本，指定java程序的中main入口，比如：1&quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot; $@ 是脚本运行时带的所有参数 pyspark bash脚本启动交互式python环境，脚本内部执行main函数，调用spark-submit脚本，指定java程序的中main入口，比如：1exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit pyspark-shell-main --name &quot;PySparkShell&quot; &quot;$@&quot; spark-submit探讨 脚本说明程序很简单，将接收参数转到spark-class脚本的第二个参数。 1exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot; spark-classspark-class是所有提交程序的入口，代码相比另外的脚本复杂一些，归根到底执行一条命令，通过后面解释可知道，该命令执行scala程序： 1exec &quot;$&#123;CMD[@]&#125;&quot; CMD变量的构造关键在这里(调用java程序生成命令行字符串)：123456789build_command() &#123; &quot;$RUNNER&quot; -Xmx128m -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot; printf &quot;%d\\0&quot; $?&#125;CMD=()while IFS= read -d &apos;&apos; -r ARG; do CMD+=(&quot;$ARG&quot;)done &lt; &lt;(build_command &quot;$@&quot;) $RUNNER是本地机器java的地址。因此，我们可以知道，整个交互式的spark程序的入口程序类似如下语法：123exec (java -Xmx128m -cp xxx org.apache.spark.launcher.Main “org.apache.spark.deploy.SparkSubmit+ pyspark-shell-main --name \"PySparkShell\" \"$@\"(或 --class org.apache.spark.repl.Main --name \"Spark shell\" \"$@\")”) 对于单纯spark-submit的脚本提交方式，只是1org.apache.spark.deploy.SparkSubmit 后面带的参数不同而已。 org.apache.spark.launcher.Main该 Java入口程序 将输出以’\\0’分割的字符串命令 org.apache.spark.deploy.SparkSubmitscala程序，真正的 spark入口。 提交Spark应用对于交互式的传给你下，会在启动的时候自动构建SparkContext，名称为sc。其它方式由开发者新建SparkContext对象。 基本操作程序提交主要方式 官方方式 命令行 REST API SparkLauncher类接口 相关第三方工具： Spark-jobserver Livy Oozie 使用SparkPyspark1./spark/bin$ ./pyspark --executor-memory 4G --total-executor-cores 80 例子:123./spark/bin/pyspark --executor-memory 4G --total-executor-cores 20 --packages com.databricks:spark-csv_2.10:1.4.0./spark/bin/spark-shell --executor-memory 4G --total-executor-cores 20 --packages com.databricks:spark-csv_2.10:1.4.0 说明：通过添加packages，可以使一下转换操作合法，paquet转换为csv格式：1row_1000_df.write.format(\"com.databricks.spark.csv\").save(filepath) Submit例子:1./spark/bin/spark-submit --executor-memory 4G --total-executor-cores 80 --py-files loadDataSetSpark.py mainSpark.py Launcher例子:12345678910111213Map&lt;String, String&gt; env = new HashMap&lt;String,String&gt;();env.put(\"SPARK_PRINT_LAUNCH_COMMAND\", \"1\");Process spark = new SparkLauncher(env) .setAppResource(\"file:/data/home/gujw/projects/spark/jar_test/SparkDataSet-0.0.1-SNAPSHOT.jar\") .setMainClass(\"com.SparkMain\") .setConf(\"spark.cores.max\", \"20\") .addSparkArg(\"--verbose\") .setMaster(\"spark://192.168.0.11:7077\") .setConf(\"spark.dynamicAllocation.enabled\", \"true\") .setConf(SparkLauncher.DRIVER_MEMORY, \"2g\") .setVerbose(true) .launch(); 认识RDD概念 RDD：弹性分布式数据集，表示分布在多个计算节点上可以并行操作的元素集合。 操作：转化操作（transformation，以是否shuffle定stage的边界）和行动操作（action，决定了job的划分），可通过返回值区别，转化操作返回RDD类型，行动操作为其它类型。 如以下lines就是一个RDD：12lines = sc.parallelize(['pandas','i like pandas'])lines = sc.textFile('readme.md') lines是Spark的RDD，第二行的lines包含了在哪些机器上有file文件的块，信息是从HDFS加载而来。每文件块映射到RDD上就是一个分区。如果一个文件块128MB（默认），那么HDFS上一个1GB大小的文件就有8个文件块，由这个文件创建的RDD就会有8个分区。123456# 能把这个RDD缓存，降低action计算的成本（因为每次新的action操作都要重新计算一次），对应的就有RDD.unpersist()，当内存吃紧时可以用# persist(storageLevel=StorageLevel(False, True, False, False, 1))RDD.persist()# Persist this RDD with the default storage level (MEMORY_ONLY_SER).RDD.cache() RDD操作流程示例12345lines_data = sc.textFile(&apos;readme.md&apos;)lines = lines_data.filter()lines.persist()lines.count()lines.first() rdd.collect()操作是将数据存在单台机器的内存（driver）上; rdd.filter(lambda x: x &gt; 90)会分发整个self对象; RDD基本函数更多见官网 RDD操作： distinct() union(rdd) intersection(rdd) #交集 substract(rdd) #减去交集 转化操作： map(func) # 能实现list append flatMap(func) # 能实现list expend 行动操作: reduce(fun) top(n),take(n) pair RDD转化操作： mapValues(func): rdd.mapValues(lambda x : x+1) # key不变，value+1 reduceByKey(func) #接收对相同的key的2个value参数 keys() values() combineByKey():该函数用于对key的值进行各种操作，相比其它ByKey更原生，计算(key,mean)例子如下： 123456sumCount = keyValue .combineByKey((lambda x: (x,1), (lambda x,value:(x[0] + y,x[1] + 1)), lambda x,y: (x[0]+y[0],x[1]+y[1])))sumCount.map(lambda key,xy:(key,xy[0]/xy[1])).collectAsMap() 123# 其它：rdd.reduceByKey(func) == rdd.groupByKey().mapValues(lambda x:x.reduce(func))rdd1.join(rdd2) #&#123;(1,(2,3)),(2,(4,5)),...&#125; pair RDD行动操作： rdd.lookup(1) #{(1,2),(1,3),(2,3)} 返回[2,3] 存取文件存取HDFS并保存为一般格式12tow_state = sc.textFile('/user/enjoyhot/town_state.csv')tow_state.saveAsTextFile('/user/enjoyhot/town_state.csv') 在Hadoop namenode 50070端口可查看上传到分布式文件系统的情况。 存取pickle文件python独有1234adData = sc.pickleFile('/user/enjoyhot/2016-06-01’)title=Row('field1','field2','field3')ad_df = sqlContext.createDataFrame(adData.map(lambda e:title(*e)))ad_df.select(['field1','field2']).show() 存取parquet格式文件parquet格式，一种流行的文件列式存储格式，相对高效。 转换与保存123456789#from pyspark.sql import *#sqlc = SQLContext(sc)lines_rdd = sc.textFile('/user/gujw/town_state.csv').map(lambda line: line.split(\",\"))header = lines_rdd.first()rdd = lines_rdd.filter(lambda x:x!=header)tow_state_df = rdd.toDF(['id','town','state']) # ['id','town','state']tow_state_df.show()# tow_state_df.write.parquet(save_path)tow_state_df.saveAsParquetFile('/user/gujw/test/town_state.parquet') 读取与操作1234567from pyspark.sql import *sqlc = SQLContext(sc)df = sqlc.read.parquet(\"/user/gujw/test/town_state.parquet\")df.registerTempTable(\"dft\")sqlc.cacheTable(\"dft\")s = \"select id,town from dft\"sqlc.sql(s).show() 本地打印元素1234id_town_list = sqlc.sql(s).collect()for item in id_town_list: print item[0] break topK操作12345678hello = sc.parallelize([1,2,3,4,4])mapA = hello.map(lambda x:(x,1))reduceA = mapA.reduceByKey(lambda a,b:a+b)print reduceA.collect()sortedA = reduceA.map(lambda (x,y):(y,x)).sortByKey(ascending=False).take(3)sortedB = reduceA.map(lambda x:(x[1],x[0])).sortByKey(ascending=False).take(3)print sortedAprint sortedB 1234## 结果[(1, 1), (2, 1), (3, 1), (4, 2)][(2, 4), (1, 1), (1, 2)][(2, 4), (1, 1), (1, 2)] Shuffle概念Shuffle操作介于Map phase和Reduce phase之间，当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。由于shuffle涉及到了磁盘的读写和网络的传输，因此shuffle性能的高低直接影响到了整个程序的运行效率。如图： 可将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。## 前期Shuffle方式如图所示，为Spark和Hadoop最基本的shuffle方式。- 每一个Mapper会根据Reducer的数量创建出相应的bucket(一个抽象的概念)，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。- Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，默认的算法是根据key哈希到不同的bucket中去。- Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。&gt; bucket是一个抽象概念，在实现中每个bucket可以对应一个文件，可以对应文件的一部分或是其他等。PS:Spark的内存计算是指Job中间输出结果可以保存在内存中，不是说shuffle过程的中间实现，Map结果的分片数据Spark和MapReduce都存放在磁盘上。## 发展### hash-basedhttps://github.com/JerryLead/SparkInternals/blob/master/markdown/4-shuffleDetails.mdhttp://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation/Spark 0.8.1做了改进后还是有缺点：- shuffle write过程中会产生大量的shffle文件，总体来说多少个reduce task,一台机器就有多少个文件,封装在shuffleFileGroup概念的文件中（图中横向的一行），不再是MXR，bucket数量依然为MXR，但此时M代表的是一次map的write task数量，即CPU核数，【一个bucket将会对应writer handler的buffer，内存开销依然很大，可能有误】。因此必要的措施是减少Mapper和Reducer的数量。看图：- 另一方面，shffle read中在做诸如groupByKey操作时，需要将每个partition中的value都保存到同一key对应的hashMap中，就得确保Map操作对应的partition足够小到内存能够容纳，因此合理的设计是增加task的数量，task数量增多又会带来buffer开销更大的问题，因此陷入了内存使用的两难境地。- 来自知乎&gt; 以前对 shuffle write/read 的分类是 sort-based 和 hash-based。MapReduce 可以说是 sort-based，shuffle write 和 shuffle read 过程都是基于key sorting 的 (buffering records + in-memory sort + on-disk external sorting)。早期的 Spark 是 hash-based，shuffle write 和 shuffle read 都使用 HashMap-like 的数据结构进行 aggregate (without key sorting)。但目前的 Spark 是两者的结合体，shuffle write 可以是 sort-based (only sort partition id, without key sorting)，shuffle read 阶段可以是 hash-based。因此，目前 sort-based 和 hash-based 已经“你中有我，我中有你”，界限已经不那么清晰。参考：https://www.zhihu.com/question/27643595### SortBasedShuffle“取代”Hash BasedShuffle作为默认选项的原因是什么？- hashbased shuffle的每个mapper都需要为每个reducer写一个文件，需要产生MR个数量的文件，如果mapper和reducer的数量比较大，产生的文件数会非常多。hashbased shuffle设计的目标之一就是避免不需要的排序（Hadoop Map Reduce被人诟病的地方，很多不需要sort的地方的sort导致了不必要的开销）。但是它在处理超大规模数据集的时候，产生了大量的DiskIO和内存的消耗，这无疑很影响性能。- hashbased shuffle也在不断的优化中，为了更好的解决这个问题，*Spark 1.1 引入了Sortbased shuffle。首先，每个Shuffle Map Task不会为每个Reducer生成一个单独的文件；相反，它会将所有的结果写到一个文件里，同时会生成一个index文件，Reducer可以通过这个index文件取得它需要处理的数据。避免产生大量的文件的直接收益就是节省了内存的使用和顺序Disk IO带来的低延时。节省内存的使用可以减少GC的风险和频率。而减少文件的数量可以避免同时写多个文件对系统带来的压力。- Spark2.0中已明确指出移除掉hash-based shuffle，详见release-note。# 提高速度的方法## 读取文件12RDD = sc.textFile(dir | part-*.txt) # 读取目录或正则匹配pairRDD = sc.wholeTextFile(dir | part-*.txt)wholeTextFile如果是读一个文件，一次读取所有行，返回1rdd(filepath,contents)## 代码逻辑层面### 对数据分区- 如pair RDD采用partitionBy(100)进行自定义哈希分区,避免不必要的混洗，但注意需要persist持久化才能避免重新分区；对RDD的结果有分区的是 - cogroup - groupWith - join,leftOuterJoin,rightOuterJoin - groupByKey,reduceByKey,combineByKey - partitionBy - sort - mapValues(父RDD需有分区),flatMapValues(父RDD需有分区)- 自定义分区如下代码，拥有相似的URL页面可能会被分到完全不同的节点上，然而，同一个域名下的网页更有可能相互链接，因此，分区时考虑将rdd中拥有同一个域名的url放在一起。12345678910\"\"\"&gt;&gt;&gt; print urlparse.urlparse(\"http://www.baidu.com\").netlocwww.baidu.com&gt;&gt;&gt; print urlparse.urlparse(\"http://www.baidu.com/a/b\").netlocwww.baidu.com\"\"\"import urlparsedef hash_domain(url): return hash(urlparse.urlparse(url).netloc)rdd.partitionBy(100,hash_domain)更详细的实战推荐两篇美团的文章：- Spark性能优化指南-基础篇- Spark性能优化指南-高级篇# 实时计算相关## Spark Streaming### 消费数据Spark可以接受来自文件系统, Akka actors, rsKafka, Flume, Twitter, ZeroMQ和TCP Socket的数据源或者你自己定义的输入源。- 读取TCP源数据12// Create a DStream that will connect to hostname:port, like localhost:9999JavaReceiverInputDStream&lt;String&gt; lines = jssc.socketTextStream(\"localhost\", 9999);简单测试的方法：运行Netcat工具作为数据服务器,在netcat服务器中输入的每一行都会被读取，在Spark streaming程序中做好统计即可。1$ nc -lk 9999- 消费kafka数据1234567891011SparkConf conf = new SparkConf().setAppName(\"streaming_top10\").setMaster(\"local[4]\");JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(5));jssc.checkpoint(Constants.CHECKPOINT);String zk_quorum = Constants.ZK_QUORUM;String consumer_group_id=\"test-consumer-group\";Map&lt;String, Integer&gt; topicMap = new HashMap &lt;String, Integer&gt;();topicMap.put(\"topic-kafka\", 1);JavaPairReceiverInputDStream&lt;String,String&gt; messages = KafkaUtils.createStream(jssc, zk_quorum, consumer_group_id, topicMap);- 待续### Spark Streaming vs StormSpark流模块先汇聚批量数据然后进行数据块分发（视作不可变数据进行处理），而Storm是只要接收到数据就实时处理并分发。- 延迟 - 根本的区别在于处理模型，Storm处理的是每次传入的一个事件，而Spark Streaming是处理某个时间段窗口内的事件流。因此,Storm处理一个事件可以达到秒内的延迟，而Spark Streaming则有几秒钟的延迟。- 容错性 - Spark Streaming提供了更好的支持容错状态计算。在Storm中,每个单独的记录当它通过系统时必须被跟踪，所以Storm能够至少保证每个记录将被处理一次，但是在从错误中恢复过来时候允许出现重复记录。这意味着可变状态可能不正确地被更新两次。 - 对于Storm而言，其优势在于延迟低，如果对严格的一次处理保证有比较高的要求，此时也可考虑使用Trident。不过这种情况下其他流处理框架如spark streaming也许更适合。## SparkSQL实际上该功能是否真正实时依然由业务具体决定，对于比较轻量级的操作，可以直接返回，做到准实时。# 资源管理系统## Yarn### 结构Hadoop2.0对MapReduce框架做了彻底的设计重构，称Hadoop2.0中的MapReduce为MRv2或者Yarn。- Hadoop1.x主要组件JobTracker和TaskTracker- Hadoop2.X中引入yarn之组件 - ResourceManger：全局的资源管理器进程，它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Application Manager，ASM）。ResourceManager 将各个资源部分（计算、内存、带宽等）精心安排给基础 NodeManager（YARN 的每节点代理）。ResourceManager 还与 ApplicationMaster 一起分配资源，与 NodeManager 一起启动和监视它们的基础应用程序。 - 应用程序管理器（Application Manager）应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。 - ApplicationMaster：对于Map-Reduce计算模型而言有它自己的ApplicationMaster实现，用于向ResourceManager（全局的）申请计算资源（Containers）并且和NodeManager交互来执行和监控具体的task。 - NodeManager：YARN中每个节点上的代理，管理Hadoop集群中单个计算节点，包括与ResourceManger保持通信，监督Container的生命周期管理，监控每个Container的资源使用（内存、CPU等）情况，追踪节点健康状况，管理日志和不同应用程序用到的附属服务（auxiliary service）。 - containerYarn对计算机计算资源的抽象，它其实就是一组CPU和内存资源。### 调度器在Yarn中有三种调度器可以选择：- FIFO Scheduler可能导致其它应用被阻塞- Capacity Scheduler有一个专门用来运行小任务的队列，意味着一些需要大量资源的任务执行时间会相比可用上所有资源的FIFO调度器时间长。- FairS cheduler资源运行时动态调整。 粗细粒度探讨细粒度可以提高CPU的利用率，但对于短作业延迟大。待补充…… 进一步了解Spark基于Spark1.6 应用调度策略两种调度策略笔者认为，网上各种job的说法其实都是在说应用，而不是spark中的job，在这里我们就将job理解为普通作业，即app即可。实际上，app对应一个调度池，而每个APP每个stage对应一个带有JobId和TaskSetManagerId的TaskSetMananger，调度池先根据JobId进行排序，再根据TaskSetManagerId排序，小的优先调度，因此job依然是通过FIFO进行调度的。 FIFO在默认情况下，Spark的调度器以FIFO（先进先出）方式调度Job的执行，standalone也只支持这一种。第一个Job优先获取所有可用的资源，接下来第二个Job再获取剩余资源。以此类推，如果第一个Job并没有占用所有的资源，则第二个Job还可以继续获取剩余资源，这样多个Job可以并行运行，否则，第二个Job就需要等待第一个任务执行完，释放空余资源，再申请和分配Job。在mesos和yarn下，有多队列调度器，如本文yarn调度器部分，通过合理设置多个队列分配资源，可以做到多个作业并行执行。 FAIR在FAIR共享模式调度下，Spark在多Job之间以轮询（round robin）方式为任务分配资源，所有的任务拥有大致相当的优先级来共享集群的资源。 参数设置 schedulingMode：该属性的值可以是FIFO或者FAIR，用来控制作业在调度池中排队运行（默认情况下）或者公平分享调度池资源。 weight：控制调度池在集群之间的分配。默认情况下，所有调度池的weight值都是为1。例如：如果你指定了一个调度池的值为2，那么这个调度池就比其它调度池多获得2倍的资源。设置一个更高的weight值，例如1000，就可以实现线程池之间的优先权——实际上，weight值为1000的调度池无论什么时候作业被激活，它都总是能够最先运行。 minShare：除了一个整体的权重，如果管理员喜欢，可以给每个调度池指定一个最小的shares值（也就是CPU的核数目）。公平调度器通过权重重新分配资源之前总是试图满足所有活动调度池的最小share。在没有给定一个高优先级的其他集群中，minShare属性是另外的一种方式来确保调度池能够迅速的获得一定数量的资源（例如10核CPU），默认情况下，每个调度池的minShare值都为0。 （scheduling mode 值是FIFO，weight值为1，minShare值为0）。 Spark map reduce基本原理RDD可理解为关系数据库里的一个个操作，比如 map，filter，Join 等。在 Spark 里面实现了许多这样的RDD类，即可以看成是操作类。 当我们调用一个map接口，底层实现是会生成一个MapPartitionsRDD对象，当RDD真正执行时，会调用MapPartitionsRDD对象里面的compute方法来执行这个操作的计算逻辑。 但是不同的是，RDD是lazy模式，只有像count，saveasText这种action动作被调用后再会去触发runJob动作。 RDD基本组成在 Spark1.6 中，有以下四个函数比较重要： def compute(split: Partition, context: TaskContext): Iterator[T]作用：用于计算，主要负责的是父RDD分区数据到子RDD分区数据的变换逻辑 protected def getPartitions: Array[Partition]作用：获取分片消息 protected def getDependencies: Seq[Dependency[_]]作用：获取父RDD的依赖关系，依赖分二种——如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency(如map)；若依赖于多个 Child RDD 分区，则称之为 wide dependency(如join)。 protected def getPreferredLocations(split: Partition): Seq[String]作用：获取Spark的执行模式，local等。 RDD,DataFrame,DataSet RDD缺点 序列化和反序列化的性能开销无论是集群间的通信,还是IO操作都需要对对象的结构和数据进行序列化和反序列化。 GC的性能开销频繁的创建和销毁对象, 势必会增加GC。 DataFrame特点 schemaRDD每一行的数据,结构都是一样的。这个结构存储在schema中，Spark通过schame就能够读懂数据,因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了。 off-heapSpark能够以二进制的形式序列化数据(不包括结构)到off-heap中,内存直接受操作系统管理（而不是JVM）。当要操作数据时, 就直接操作off-heap内存。由于Spark理解schema, 所以知道该如何操作。 通过schema和off-heap,DataFrame解决了RDD的缺点,但是却丢了RDD的优点。DataFrame不是类型安全的, API也不是面向对象风格的。 DatasetDataSet以Catalyst逻辑执行计划表示，Dataset跟RDD相似，但是Dataset并没有使用Java序列化库和Kryo序列化库，而是使用特定Encoder来序列化对象。并且，序列化后数据以编码的二进制形式被存储，不需要反序列化就可以执行sorting、shuffle等操作。 blockManager实现缓存的重要类，通过类似于在内部再构建了一个KV系统，K表示每个分区 ID 号，V 表示这个分区计算后的结果。 例如在streaming计算时，每个batch会去消息队列上拉取这个时间段的数据，每个Recevier接收过来数据形成block块并存放到blockManager上，为了可靠性，这个block块可以远程备份，后续的batch计算就直接在之前已读取的block块上进行计算，这样不断循环迭代来完成流处理。 计算任务流程的梳理划分Stage当某个操作触发计算，向DAGScheduler提交作业时，DAGScheduler需要从RDD依赖链最末端的RDD出发，遍历整个RDD依赖链，划分Stage任务阶段，并决定各个Stage之间的依赖关系。 提交 理解Stage—&gt;TaskSet 12345DAGScheduler.call(TaskSet)&#123;res = TaskScheduler.submit(TaskSet)&#123; TaskSetManager(res) &#125;&#125; 文字说明每个Stage的提交，最终是转换成一个TaskSet任务集的提交，DAGScheduler通过TaskScheduler接口提交stage(TaskSet)，每个TaskSet最终会触发TaskScheduler构建一个TaskSetManager（调度单位）的实例来管理这个TaskSet的生命周期，对于DAGScheduler来说提交Stage的工作到此就完成了。而TaskScheduler的具体实现则会在得到计算资源的时候，进一步通过 TaskSetManager调度具体的Task到对应的Executor节点上进行运算。 ExecutorBackend：在Worker上执行Task的线程组 SchedulerBackend：主要用来与Worker中的ExecutorBackend建立连接，用来向Executor发送要执行任务，或是接受执行任务的结果，也可以用来创建AppClient(包装App信息，包含可以创建CoarseGrainedExecutorBackend实例Command)，用于向Master汇报资源需求 监控DAGScheduler就必然需要监控当前Job/Stage乃至Task的完成情况。这是通过对外（主要是对TaskScheduler）暴露一系列的回调函数来实现的。 任务结果的获取一个具体的任务在Executor中执行完毕以后，其结果需要以某种形式返回给DAGScheduler，根据任务类型的不同，任务的结果的返回方式也不同： 对于FinalStage所对应的任务（触发action的那个，对应的类为ResultTask）返回给DAGScheduler的是运算结果本身； 而对于 ShuffleMapTask，返回给DAGScheduler的是一个MapStatus对象，MapStatus对象管理了ShuffleMapTask的运算输出结果在BlockManager里的相关存储信息，而非结果本身，这些存储位置信息将作为下一个Stage的任务获取输入数据的依据。参考：http://blog.csdn.net/laiwenqiang/article/details/50032171 适用场景不适合超大数据量的计算Spark适用于那些在多个并行操作之间重用数据的应用，通过rdd是基于内存的优点免去了传统MapReduce不断读写磁盘的IO损耗，但基于内存的rdd操作，可想而知，数据量一大就容易产生OOM的问题。 不适合异步更新模型由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如增量的web爬虫和索引，以及在一些机器学习和数据挖掘（MLDM）算法上表现并非最优，spark ML与Mahout都是采用Iterative MapReduce架构，都是同步迭代，而关于迭代式算法： 迭代式算法，许多机器学习算法都用一个函数对相同的数据（优点）进行重复的计算，更新同一个模型（局限）。 同步迭代的缺点，存在木桶效应，可参考该图： 图参考：链接 就是说，对于那种增量修改的应用模型不太适合。因为rdd的操作可以理解为分散式的，每个分散的任务不是针对同一个共同体。（笔者认为，假如硬是要通过spark共享一个共同体，一般实现是每个任务完成后重写共同体，共同体全局可见，因此对于细粒度而言，任务一多，通信开销可想而知，但这对于CPU密集型，网络开销小的硬件如GPU不是问题）。这是一个模型并行化和数据并行化的问题。","categories":[{"name":"Spark","slug":"Spark","permalink":"http://enjoyhot.github.io/categories/Spark/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://enjoyhot.github.io/tags/Spark/"}]},{"title":"Java高级篇","slug":"Java高级篇","date":"2017-04-30T09:21:09.000Z","updated":"2017-04-30T09:29:40.148Z","comments":true,"path":"2017/04/30/Java高级篇/","link":"","permalink":"http://enjoyhot.github.io/2017/04/30/Java高级篇/","excerpt":"前言本系列文章将分为三部分：Java基础篇 语法篇 数据结构篇 集合框架","text":"前言本系列文章将分为三部分：Java基础篇 语法篇 数据结构篇 集合框架 Java高级篇 JVM知识 多线程 Java IO Java安全 设计模式 Java实践篇 并发编程相关 分布式相关 框架介绍 有些【疑问】可能会留在笔记中，在学习中补充… 相关链接 javase 1.8 online doc javase 1.7 online doc javase 1.8 JVM Specification online doc javase 1.7 JVM Specification online doc JVM知识内存模型JMM结构 基本结构 分为class loader； 运行时数据区； 执行引擎； 本地方法接口 基本示意图如下： 类在JVM中的唯一性通过类的全限定名和加载这个类的类加载器ID确定唯一性。 JVM相关概念定义 程序计数器（Program Counter Register）：当前线程执行字节码的行号指示器。通过改变这个指示器的值来选取下一条需要执行的字节码指令。由于多线程间切换时要恢复每一个线程的当前执行位置，所以每个线程都有自己的程序计算器。这个内存区域是Java虚拟机唯一一个没有定义OutOfMemeryError情况的区域。 Java虚拟机栈（Java Visual Machine Stacks）：虚拟机栈描述的是Java方法执行的内存模型：每个方法执行是都会创建栈帧（Stack Frame）用于存储局部变量（包括引用变量），操作栈，方法信息，动态链接，方法出口等信息。栈帧由三部分组成：局部变量区（以一个字长为单位，用数组下标实现定位局部变量）、操作数栈（以一个字长为单位，用数组表示，但采用先进后出的策略）、帧数据区（用于常量池解析、正常方法返回以及异常派发机制）。&gt;在java虚拟机规范中，对于这两个区域规定了两种情况的异常：1）如果线程请求的栈深度大于虚拟机所允许的深度将会抛出StackOverFlowError异常; 2）如果Java虚拟机可以动态扩展，当无法申请到足够的内存时会抛出OutOfMemeryError。 本地方法栈（Native Method Stacks）本地方法栈与Java虚拟机栈区别是Java虚拟机栈为虚拟机执行Java方法服务，而本地方法栈是虚拟机使用到的Native方法服务。如JNI引用的对象。所以本地方法栈也可能出现两种与Java虚拟机栈相同的异常。 Java堆（Java Heap）Java堆是Java虚拟机管理的最大的一块内存区域，java堆是被所有Java线程共享的，在Java虚拟机启动时创建，此内存的唯一目的就是存放对象实例。几乎所有的对象实例都要分配在堆中，垃圾回收的主要区域就是这里（还可能有方法区）。（随着JIT编译器的发展，逃逸分析技术的逐渐成熟，栈上分配，标量替换等优化技术，使得部分对象不再分配在堆上。）JNI调用的程序new的空间不受这里管理。Java堆的大小通过: -Xmx和-Xms两个参数控制。但是当堆的内存再无法扩展时，就会出现OutOfMemeryError。 方法区（Method Area）方法区与Java堆一样，是各个线程共享的内存区域，他用于存储类信息，常量，静态变量以及及时编译后的代码等数据。当方法区无法满足内存分配需求时，将抛出OutOfMemeryError. 简图 相关测试方法 Java类加载机制 什么是类的加载类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 类加载的时机类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误（LinkageError错误）如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误。参考：http://www.cnblogs.com/ityouknow/p/5603287.html 类加载流程 装载：查找和导入Class文件； 链接：把类的二进制数据合并到JRE中； 校验：检查载入Class文件数据的正确性； 准备：给类的静态变量分配存储空间，并将其初始化为默认值，普通static为0，final static为设定值； 解析：将符号引用转成直接引用； 初始化：对类的静态变量，静态代码块执行初始化操作 在这五个阶段中，加载、验证、准备和初始化这四个阶段发生的顺序是确定的，而解析阶段则不一定，它在某些情况下可以在初始化阶段之后开始，这是为了支持Java语言的运行时绑定（也成为动态绑定或晚期绑定）。另外注意这里的几个阶段是按顺序开始，而不是按顺序进行或完成，因为这些阶段通常都是互相交叉地混合进行的，通常在一个阶段执行的过程中调用或激活另一个阶段。 三个类加载器 Bootstrap ClassLoader根类加载器，负责核心类的加载，如System,String等，还有jdk中jre的lib目录的rt.jar,也可以通过指定参数-Xbootclasspath再加; Extension ClassLoader扩展类加载器，负责jdk中jre的lib目录下的ext目录，或者通过-Djava.ext.dirs再加； Application ClassLoader用于加载用户路径上指定的类库。由-classpath或-cp选项定义,或者是JAR中的Manifest的classpath属性定义。 Tip: 用户自定义类加载器，默认情况下父加载器是系统类加载器，也可以指定其它两个。 Tomcat 为每个 App 创建一个 Loader,里面保存着此 WebApp 的 ClassLoader。需要加载 WebApp 下的类时,就取出 ClassLoader 来使用 JVM在判定两个class是否相同时（相等包括代表类的Class对象的equals()方法、isAssignableFrom()方法、isInstance()方法、instanceof关键字的结果），不仅要判断两个类名是否相同，而且要判断是否由同一个类加载器实例加载的。 类加载器相关例子：12345Object obj = myClassLoader.loadClass(\"com.test.ClassLoaderTest\").newInstance();System.out.println(obj.getClass());// return false// com.test.ClassLoaderTest类默认使用Application ClassLoader加载，而obj是通过自定义类加载器加载的，类加载不相同，因此不相等System.out.println(obj instanceof com.test.ClassLoaderTest); 双亲委托模型应用程序都是由以上三种类加载器互相配合进行加载的，还可以加入自己定义的类加载器，称为类加载器的双亲委派模型。 类加载器之间的父子关系一般不会以继承的关系来实现，而是都使用组合关系来复用父加载器的。 ClassLoader本身没有父类加载器，但可以用作其它ClassLoader实例的的父类加载器。 当一个ClassLoader实例需要加载某个类时，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器Bootstrap ClassLoader试图加载，如果没加载到，则把任务转交给Extension ClassLoader试图加载，如果也没加载到，则转交给App ClassLoader 进行加载; 如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等URL中加载该类。如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。 为什么要使用双亲委托这种模型呢？ JVM的GC垃圾回收算法 Mark-Sweep（标记-清除）算法 分标记和回收两个阶段，内存碎片问题。 Copying（复制）算法 内存空间利用率不高。 Mark-Compact（标记-整理）算法 分标记和整理两个阶段，弥补Copying算法的缺陷，标记后移动，再清除。 以上三种是传统最常见的方法，具体演示参考：http://www.cnblogs.com/dolphin0520/p/3783345.htm 分代回收算法在Hotspot JVM中采用分代回收，现在在各种其它JVM中也比较常用。 JVM区域的划分（Java1.7）从GC的角度，jvm区域总体分两类，heap区（Java堆）和非heap区（其它）。 heap区：Eden Space（伊甸园）、Survivor Space(幸存者区)、Tenured Gen（老年代-养老区）；eden和survivor默认比例是8:1:1（新生代实际可用的内存空间为 9/10（即90%）的新生代空间）。 非heap区：CodeCache(代码缓存区)、PermGen（永久代）等。 关于永久代（主要存放的是Java类定义信息，生成大量动态类可能溢出） 当永久代满时也会引发Full GC，会导致Class、Method元信息的卸载，一般需要避免，因此就不会发生GC了； Java8不设永久代，类的元信息等被移到了一个与堆不相连的本地内存区域，这个区域就是元空间（metaspace）。 a. 对永久代进行调优是很困难的，永久代中的元数据可能会随着每一次FullGC发生而进行移动。并且为永久代设置空间大小也是很难确定的，因为这其中有很多影响因素，比如类的总数，常量池的大小和方法数量等。b. 同时，HotSpot虚拟机的每种类型的垃圾回收器都需要特殊处理永久代中的元数据。将元数据从永久代剥离出来，不仅实现了对元空间的无缝管理，还可以简化Full GC以及对以后的并发隔离类元数据等方面进行优化。 如何分代回收 Minor GC：当Eden区满时，触发Minor GC。新生代采用了复制算法，分为一个Eden 区域和两个Survivor区域，真正使用的是一个Eden区域和一个Survivor区域，GC的时候，会把存活的对象放入到另外一个Survivor区域中（总有一块保持为空作为to，用于复制，满就复制到老年区），然后再把这个Eden区域和Survivor区域清除。在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代(年轻代被分为3个部分——Enden区和两个Survivor区（From和to）)所有对象的总空间（这一段似乎有误，有说法是说survival from和to区是否够）： 如果大于则进行Minor GC; 如果小于则看HandlePromotionFailure设置是否允许担保失败。 如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于则尝试Minor GC（如果尝试失败也会触发Full GC），如果小于则进行Full GC。 如果不允许，则直接Full GC。 如何晋升为老年代：多次Minor GC后还存活的对象会晋升。 Major GC：清理老年代，许多是由 Minor GC 触发的，老年代采用的是Mark-Compact（标记-整理）算法(HotSpot VM，其它的JVM实现并不一定是这样做的)。 Full GC：老年代空间不足：如上分析，要么是前期的检查，要么是晋升期间。当频繁Full GC时，可通过jstat、GC日志、对象内存工具等查看是什么原因，例如JMap查看。据知乎： fullgc频繁说明old区很快满了，即剩余空间不足（来的多或留的多）。 如果是一次fullgc后，剩余对象不多。那么说明你eden区设置太小，导致短生命周期的对象进入了old区。 如果一次fullgc后，old区回收率不大，那么说明old区太小。 哪些对象需要回收每个应用程序都包含一组根（root）。每个根包含指向引用类型对象的一个指针，该指针要么引用托管堆中的一个对象，要么为null，GC会收集那些不是GC roots且没有被GC roots引用的对象。java中可作为GC Root的对象有： 虚拟机栈中引用的对象（本地变量表）； 本地方法栈中引用的对象（Native对象）; 方法区中静态属性引用的对象； 方法区中常量引用的对象（对比而言单例可能会被回收）；参考： http://www.importnew.com/15820.html 垃圾回收器 Serial/Serial Old ParNew Parallel Scavenge Parallel Old CMSCMS（Current Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，它是一种并发收集器，采用的是Mark-Sweep算法，默认的回收线程数是(CPU个数+3)/4。CMS的缺点也比较明显，CMS会牺牲更多的硬件资源、吞吐量及Heap使用量等，所以在Java1.7之后引入了G1 收集器来全面替代CMS。 G1(1.7开始引进)G1收集器是当今收集器技术发展最前沿的成果，它是一款面向服务端应用的收集器，它能充分利用多CPU、多核环境。因此它是一款并行与并发收集器，并且它能建立可预测的停顿时间模型。整体上是基于标记整理，局部采用复制回收算法。 什么时候触发GC：http://www.tuicool.com/articles/3yuqMzz 泛型擦除泛型可以说是Java中最常用的语法糖之一，因此虚拟机是不支持这些语法，在编译时转化为Object，继承的时候利用桥方法动态调用，据此应考虑泛型在开发过程中的约束和局限性。具体用法解释可参考【Java基础篇】。 多线程更多内容请查看Java实践篇 锁内置锁 synchronized 对于类而言，静态方法的锁是Test.class,而普通方法的锁是this。如果对类中的两个方法用synchronized进行修饰，要么都是静态的，或都是成员方法，这两个方法才会同步，如下例子，类中的普通成员方法都采用this作为锁对象。 12345class Test&#123; private in value; synchronized int get()&#123;return value;&#125; sychronized void set(int value)&#123;this.value = value;&#125;&#125; 显式锁java.util.concurrent.locks包中定义了两个锁类，ReentrantLock和ReentrantReadWriteLock类。 ReentrantReadWriteLock在读多于写的情况下，利用读写锁分离访问。 12345678910111213private ReentrantReadWriteLock rwl = new ReentrantReadWriteLock();private Lock readLock = rwl.readLock();private Lock writeLock = rwl.writeLock();public double getTotalBalance()&#123; readLock.lock(); try&#123;...&#125; finally&#123;readLock.unlock();&#125;&#125;public double setBalance()&#123; writeLock.lock(); try&#123;...&#125; finally&#123;writeLock.unlock();&#125;&#125; ReentantLock类似的，也是需要lock(),并在finally中unlock(),而一般常用的方法，是将其与Condition配合使用，使线程在某些情况下，挂起等待，某些情况下，被唤醒。 12345678910111213141516171819202122private Lock suspendLock = new ReentranLock();private Condition suspendCondition = suspendLock.newCondition();// 在某个安全操作下public double getTotalBalance()&#123; suspendLock.lock(); try&#123; while(a==0)&#123; suspendCondition.await(); // 暂时放弃锁 &#125; &#125; finally&#123;suspendLock.unlock();&#125;&#125;public double setBalance()&#123; suspendLock.lock(); // 尝试获得锁，假如某个操作线程放弃锁就继续执行 try&#123; // do someting a++; suspendCondition.signalAll(); // 唤醒其它线程 &#125; finally&#123;suspendLock.unlock();&#125;&#125; 原子操作 除了long型字段和double型字段外，Java内存模型确保访问任意类型字段所对应的内存单元都是原子的。此外，volatile long 和volatile double也具有原子性。Java编程规范的文档说明。 对于64big的环境来说，单次操作可以操作64bit的数据，但在32bit操作系统下，指令单次操作能处理的最长长度为32bit，因此在读写的时候，JVM分成两次操作，每次读写32位。因为采用了这种策略，所以64位的long和double的读与写都不是原子操作。 原子性不能确保获得的是任意线程写入之后的最新值，需要额外编程实现同步（原子和同步是不同的概念）。因此，原子性保证通常对并发程序设计的影响很小。 协定每个共享的和可变的变量都应该只由一个锁来保护，从而维护人员知道是哪一个锁。 一种常见的加锁规定是，将所有的可变状态都封装在对象内部，并通过对象的内置锁对所有访问可变状态的代码路径进行同步，使得在该对象上不会发生并发访问。 同步队列基本操作 peek：返回头元素，不删除，队列空为nullpoll：返回头元素，删除，队列空为nulltake：返回头元素，删除，队列空阻塞 offer：添加元素，队列满为falseput：添加元素，队列满阻塞 常用类每一个BlockingQueue都有一个容量，让容量满时往BlockingQueue中添加数据时会造成阻塞，当容量为空时取元素操作会阻塞。 LinkedBockingQueue默认容量没有上限，也可以指定最大容量 ArrayBlockingQueue构造时需要指定容量 PriorityBlockingQueue堆的实现可利用PriorityQueue，这里加了Blocking，可以认为是多线程的版本。 ArrayBlockingQueue VS LinkedBlockingQueue 队列中锁的实现不同 ArrayBlockingQueue实现的队列中的锁是没有分离的，即生产和消费用的是同一个锁； LinkedBlockingQueue实现的队列中的锁是分离的，即生产用的是putLock，消费是takeLock，而在遍历以及删除元素则要两把锁都锁住。 在生产或消费时操作不同 ArrayBlockingQueue实现的队列中在生产和消费的时候，是直接将枚举对象插入或移除的； LinkedBlockingQueue实现的队列中在生产和消费的时候，需要把枚举对象转换为Node进行插入或移除，会影响性能 队列大小初始化方式不同 ArrayBlockingQueue实现的队列中必须指定队列的大小； LinkedBlockingQueue实现的队列中可以不指定队列的大小，但是默认是Integer.MAX_VALUE（如果生产者的速度一旦大于消费者的速度，也许还没有等到队列满阻塞产生，系统内存就有可能已被消耗殆尽） 线程间数据交互 Exchanger同步的主要作用是保证一致性，或者说逻辑上的顺序性。但某些场景我们只是想两个线程共享数据，利用锁保证读写分离是没问题的，但还有更优雅的方式。可在两个线程之间交换数据，只能是2个线程，他不支持更多的线程之间互换数据。实际上，一个线程向缓冲区填入数据，另一个线程消耗这些数据，一方线程完成操作，通过交换缓冲区的方式让另一个线程消费数据。如下，我们不必关系同步不同步的问题。12345Exchanger&lt;List&lt;Integer&gt;&gt; exchanger = new Exchanger&lt;&gt;();new Consumer(exchanger).start();new Producer(exchanger).start();// 内部操作list = exchanger.exchange(list); 具体例子参考：http://blog.csdn.net/andycpp/article/details/8854593 障栅例如我们想所有线程运行到某个点，再做合并操作（类似MapReduce），可以考虑用CyclicBarrier类，与内存栅障类似：运算线程集内的线程必须等待该集合中所有其他线程都执行完某个运算后，才能继续向下执行。确保在所有线程全部到达某个逻辑执行点前，任何线程都不能越过该逻辑点。当一个线程运行完它那部分，则让其运行到障栅出，一旦所有线程都到达了这个障栅，障栅自动撤销，所有线程继续运行。12345// 表示至少三个线程到障栅才释放CyclicBarrier barrier = new CyclicBarrier(3);// 传到每个线程的run方法中doWork();barrier.await(); 对比CountDownLatch：闭锁用来等待事件，而栅栏用于等待其他线程，大家相互等待，只要有一个没完成，所有人都得等着。即闭锁用来等待的事件就是countDown事件,只有该countDown事件执行后所有之前在等待的线程才有可能继续执行; 信号量（Semaphores）当一个线程想要访问某个共享资源，首先，它必须获得semaphore。如果semaphore的内部计数器的值大于0，那么semaphore减少计数器的值并允许访问共享的资源。计数器的值大于0表示，有可以自由使用的资源，所以线程可以访问并使用它们。123456789101112131415private final Semaphore semaphore;public Class()&#123; semaphore = new Semaphore(1);&#125;// in functiontry &#123; semaphore.acquire(); // do something&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125;finally&#123; semaphore.release();&#125; 线程池常用例子:123456import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;ExecutorService executor = Executors.newFixedThreadPool(3);// 实现Runnable接口Runnable runner = new ExecutorThread();executor.execute(runner); 类的介绍 Executor接口，该 接口 只包含一个方法： 1void execute(Runnable command); ExecutorService也是一个接口，继承Executor接口。 12public interface ExecutorService extends Executor &#123;&#125; ExecutorsExecutors类，提供了一系列工厂方法用于创建线程池，返回的线程池都实现了ExecutorService接口。 ThreadPoolExecutor继承ExecutorService接口，Executors工厂类中提供的线程池的底层实现，所以自定义灵活度高，通常建议用Executors已经配置好了的。继承关系简单如下所示： 如下最常见的newFixedThreadPool方法源码:12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 参数： corePoolSize线程池的基本大小，如果当前线程数目小于该配置，无论是否其中有空闲的线程，都会给新的任务产生新的线程。如果刚好等于或大于，会尝试将新任务添加到任务缓存队列当中，若添加成功，则该任务会等待空闲线程将其取出去执行；若添加失败（一般来说是任务缓存队列已满），则会尝试创建新的线程去执行这个任务； maximumPoolSize线程池中允许的最大线程数，线程池中的当前线程数目不会超过该值。如果队列中任务已满，并且当前线程个数小于maximumPoolSize，那么会创建新的线程来执行任务。如果线程数目达到maximumPoolSize，一般意味着队列也已经满了，则会采取任务拒绝策略进行处理。形象一点可看下图； keepAliveTime线程空闲时间超过keepAliveTime，线程将被终止。 poolSize线程池中当前线程的数量。 线程池中的线程初始化： 默认情况下，创建线程池之后，线程池中是没有线程的，需要提交任务之后才会创建线程。在实际中如果需要线程池创建之后立即创建线程，可以通过以下两个方法做到： prestartCoreThread()初始化一个核心线程； prestartAllCoreThreads()初始化所有核心线程 重要的方法： execute()无返回值。 submit()利用了Future来获取任务执行结果。 shutdown()关闭线程池。 shutdownNow()关闭线程池。 创建线程池的常用方法正如例子所述，可利用Executors工厂类快速创建线程池。 newCachedThreadPool创建一个无界可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。corePoolSize和maximumPoolSize的大小是一样的，而queue是无界的 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行 Tips 如果你提交任务时，线程池队列已满。会时发会生什么？许多人可能会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么ThreadPoolExecutor的submit()方法将会抛出一个RejectedExecutionException异常。 Java线程池中submit() 和 execute()方法有什么区别？两个方法都可以向线程池提交任务，execute()方法的返回类型是void，它定义在Executor接口中,而submit()方法可以返回持有计算结果的Future对象，它定义在ExecutorService接口中，它扩展了Executor接口，其它线程池类像ThreadPoolExecutor和ScheduledThreadPoolExecutor都有这些方法。 Java IO同步与阻塞参考：http://www.cnblogs.com/dolphin0520/p/3916526.html IO请求分两个阶段： 查看数据是否就绪； 进行数据拷贝（内核将数据拷贝到用户线程）； 同步与异步关键：内核发送通知 多个任务和事件发生时，一个事件的发生或执行是否会导致整个流程的暂时等待。 对于同步而言，是需要用户线程或内核去轮询，当数据就绪时，再将数据从内核拷贝到用户空间。 对于异步而言，IO两个阶段都由内核完成，然后发送通知告知用户，由内核主动拷贝数据到用户线程，可见异步需要操作系统底层的支持； 阻塞与非阻塞关键：操作返回标志位 对于阻塞而言，用户线程一直等待数据，直到就绪； 对于非阻塞而言，用户线程读取数据，立即返回标志位告知用户线程当前IO状态，下次再读取数据了，一旦就绪，内核就将数据拷贝到用户线程。 多路复用 select 遍历，效率线性下降； 支持的文件描述符数量太小，为1024； poll使用pollfd结构而不是select的fd_set结构,其它方面非常相似 epoll 水平触发LT(level triggered)水平触发：是缺省的工作方式，并且同时支持block和no-block socket.当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果你不作任何操作或没读写完，那么下次调用 epoll_wait()时，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。 边缘触发ET(edge-triggered)边缘触发：是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了。这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符。参考 http://www.tuicool.com/articles/VvU7fum 在Netty权威指南中写道： socket描述符数量受限 需要扫描所有的socket集合，在网络较差（如Wide Area Network-WAN），epoll效率远在前者之上，得益于epoll只有在活跃的socket上才会调用callback函数，而不是轮询。 epoll是将内核和用户空间通过mmap（内存映射）同一块内存实现FD消息在内核和用户空间的交互，从而避免不必要的内存拷贝。 BIO,NIO,AIO推荐阅读：java-nio tutorials，更多补充及应用场景参考【Java实践篇】。 BIO同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。 NIO 同步非阻塞IO(Java NIO)服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。用户进程也需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问。 异步阻塞IO（Java NIO）应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄（如果从UNP的角度看，select属于同步操作。因为select之后，进程还需要读写数据），从而提高系统的并发性。 Java AIO(NIO.2) 异步非阻塞IO在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。 Java安全待补充 设计模式分类共23种设计模式可分为三类： 创建型模式简单工厂模式（Simple Factory）(不属于23种设计模式)工厂方法模式（Factory Method）抽象工厂模式（Abstract Factory）创建者模式（Builder）原型模式（Prototype）单例模式（Singleton） 结构型模式外观模式/门面模式（Facade门面模式）适配器模式（Adapter）代理模式（Proxy）装饰模式（Decorator）桥梁模式/桥接模式（Bridge）组合模式（Composite）享元模式（Flyweight） 行为型模式模板方法模式（Template Method）观察者模式（Observer）状态模式（State）策略模式（Strategy）职责链模式（Chain of Responsibility）命令模式（Command）访问者模式（Visitor）调停者模式（Mediator）备忘录模式（Memento）迭代器模式（Iterator）解释器模式（Interpreter） 几种常见设计模式观察者模式一个实现主题接口Observable的类subject，可以往里面注册多个观察者Observer（List维护），名义上由观察者传入(订阅)subject，实际上观察者中还要调用subject的方法register：12subject s = new subject();new observer(s); // 其中调用register(this)，s.get等 s 做其它事情，并触发其中的方法把List中的对象进行逐一调用。 装饰者模式（如Java Stream各种类）本质上通过使用虚类（强制实现方法，例如cost），不断继承形成“种类”，装饰器是继承虚类最终的方法类，组合“种类”和“装饰器”：构造函数传入“种类”对象即可构造新种类对象，配合该被装饰对象实现cost方法。 工厂模式（包括简单工厂，工厂方法，抽象工厂） 本质上通过定义接口，将new对象封装在工厂中，而不是暴露在外面具体方法，通过传入type判断需要new什么对象（工厂方法对这些类实现共同接口），缺点就是对于新加类需要修改工厂创建对象的方法； 工厂方法就是直接在外面new一个对象，这个对象实现某个接口，然后通过固定方法返回特定类型对象； 抽象工厂方法侧重表示通过顶级接口、接口中工厂方法，来强制所有具体工厂需要实现某些方法。 单例模式 优点： 节省内存资源（只有在用这个实例调用方法时，方法才被加入到内存中，当对象不用的时候，gc会将方法回收，效率高了很多，而static常驻内存，但话不能这么说，单例需要在堆中申请资源，static不用）； 提供了对唯一实例的受控访问，因此便于控制； 主要缺点： 由于单例模式中没有抽象层（个人认为Java中的通过接口，虚类等实现的逻辑就是抽象层的表现），因此单例类的扩展有很大的困难。 单例类的职责过重，在一定程度上违背了“单一职责原则”。（记住数据库不要用，否则效率） 例子 饱汉式1234567891011// 线程安全的懒汉式=饱汉式（lazy loading）：public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 特点：加锁会影响效率，一般不用这种方法。不加线程同步可能get的时候会生成多个。 饿汉式12345678// 饿汉式（not lazy loading占内存）：public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125;&#125; 特点：常用，线程安全；容易产生垃圾对象，可能存在多个类加载器的情况，如一些servlet容器对每个servlet使用完全不同的类装载器。 静态内部类12345678910// 静态内部类public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 区别：这种达到lazy loading的效果，Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance。 想象一下，如果实例化instance很消耗资源，我想让他延迟加载，另外一方面，我不希望在Singleton类加载时就实例化，因为我不能确保Singleton类还可能在其他的地方被主动使用从而被加载，那么这个时候实例化instance显然是不合适的。这个时候，这种方式相比就显得很合理。 双重锁形式1234567891011121314151617181920// 可能是单例模式最优方案（双重锁形式）：public class SingletonTest &#123; private SingletonTest() &#123; &#125; //定义一个静态私有变量(不初始化，不使用final关键字，使用volatile保证了多线程访问时instance变量的可见性，避免了instance初始化时其他变量属性还没赋值完时，被另外线程调用) private static volatile SingletonTest instance; //定义一个共有的静态方法，返回该类型实例 public static SingletonTest getIstance() &#123; // 对象实例化时与否判断（不使用同步代码块，instance不等于null时，直接返回对象，提高运行效率） if (instance == null) &#123; //同步代码块（对象未初始化时，使用同步代码块，保证多线程访问时对象在第一次创建后，不再重复被创建） synchronized (SingletonTest.class) &#123; //未初始化，则初始instance变量 if (instance == null) &#123; instance = new SingletonTest(); &#125; &#125; &#125; return instance; &#125;&#125; 注意，反射可以调用private构造方法，然而我们不管它。 双重锁为什么使用volatile，而C++不用答案：禁止指令重排序。参考：http://www.cnblogs.com/dolphin0520/p/3920373.htmlhttp://blog.csdn.net/dl88250/article/details/5439024volatile可以保证Java虚拟机对代码执行的指令重排序，也会保证它的正确性。 synchronized虽然保证了原子性，但却没有保证指令重排序的正确性。因此，线程A对单例中的对象进行初始化过程中，可能出现构造函数里面的操作太多了（在 Java 中双重检查模式无效的原因是在不同步的情况下引用类型不是线程安全的），经过重排序之后，A线程单例实例还没有造出来，但已经被赋值了。而B线程这时过来了，错以为单例被实例化出来，一用才发现其中的功能并不完善（出现不可预知的结果），尚未被初始化。我们的线程虽然可以保证原子性，但程序可能是在多核CPU上执行，JVM进行指令重排序的可能性就更大。 适配器模式适配器模式有类的适配器模式和对象的适配器模式两种不同的形式。 类适配器模式： 目标(Target)角色：这就是所期待得到的接口。注意：由于这里讨论的是类适配器模式，因此目标不可以是类。 源(Adapee)角色：现在需要适配的接口。 适配器(Adaper)角色：适配器类是本模式的核心。适配器把源接口转换成目标接口。显然，这一角色不可以是接口，而必须是具体类。本质上：继承Adaptee父类同时实现Target接口； 对象适配器模式：实现接口，构造函数中传入源，装饰为Adapter类，实现更多方法以适配，与装饰者模式类似；","categories":[{"name":"Java","slug":"Java","permalink":"http://enjoyhot.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://enjoyhot.github.io/tags/Java/"}]},{"title":"Java基础篇","slug":"Java基础篇","date":"2017-04-30T09:20:09.000Z","updated":"2017-04-30T09:29:40.144Z","comments":true,"path":"2017/04/30/Java基础篇/","link":"","permalink":"http://enjoyhot.github.io/2017/04/30/Java基础篇/","excerpt":"前言本系列文章将分为三部分：Java基础篇 语法篇 数据结构篇 集合框架","text":"前言本系列文章将分为三部分：Java基础篇 语法篇 数据结构篇 集合框架 Java高级篇 JVM知识 多线程 Java IO Java安全 设计模式 Java实践篇 并发编程相关 分布式相关 框架介绍 有些【疑问】可能会留在笔记中，在学习中补充… 相关链接 javase 1.8 online doc javase 1.7 online doc javase 1.8 JVM Specification online doc javase 1.7 JVM Specification online doc 语法章手动编译Java工程目前的代码打包工具很多，前有ant，后有Maven, Gradle, SBT, Ivy, Grape…，手动编译运行，是怎样的。这里简单举个例子，假如文件组织如下: src/ classes/ a.java b.java test/ main.java(import classes) 编译java文件 1javac a.java b.java main.java 无顺序之分 生成的class文件默认在原目录 -s 能指定编译结果要置于哪个目录（directory） 运行进入src目录 12// -cp: 等价于-classpath，多个则用分号；分开，也可带上jar包java -cp . test.main 于是，当需要对工程进行编译，则需要先编译好，在打包时，为了能带上环境变量，往往把包路径移至import的那一层，然后打成war包或jar包等。 数组常用操作 填充 1234int test[] = new int[4];int testDeep[][] = new int[4][5];// 仅限一维Arrays.fill(test, 8); 打印 1234// [8, 8, 8, 8]System.out.println(Arrays.toString(test));// [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]System.out.println(Arrays.deepToString(testDeep)); 拷贝 12345678910int test_copy[] = new int[test.length];System.arraycopy(test, 0, test_copy, 0, test.length);test[0] = 9;// [8, 8, 8, 8]System.out.println(Arrays.toString(test_copy));int[] test_copy_v2 = Arrays.copyOf(test, test.length);test[0] = 10;// [9, 8, 8, 8]System.out.println(Arrays.toString(test_copy_v2)); 效率: System.arraycopy &gt; Arrays.copyOf(本质上新建了数组，并调用了前者) &gt; for循环赋值 构造函数 类中的普通方法可以和类名同名，和构造方法唯一的区分是，构造方法没有返回值； 抽象类可以有构造方法，接口中不能有构造方法。 类的继承方法的重写(override)，两同两小一大原则： 方法名相同，参数类型相同 子类返回类型小于等于父类方法返回类型， 子类抛出异常小于等于父类方法抛出异常， 子类访问权限大于等于父类方法访问权限。 程序中代码的加载顺序假设在主函数中new一个子类构造函数，执行顺序为： 父类Ｂ静态代码块-&gt;子类Ａ静态代码块-&gt;父类Ｂ非静态代码块-&gt;父类Ｂ构造函数-&gt;子类Ａ非静态代码块-&gt;子类Ａ构造函数。 静态代码块和静态变量的加载看书写顺序； 原因：涉及类加载过程中初始化顺序，详看【高级篇】 静态类静态类只能是内部类，可以被继承1234public class InterfaceTest &#123; static class Sun&#123;&#125;&#125;class Ak extends InterfaceTest.Sun&#123;&#125; 继承需要注意的子类定义了父类没有的方法，而用父类指向子类对象，不能直接调用子类方法，需要强制转换：1(SON)base.methodB(); 例子 父类构造函数调用自己的方法，若子类继承其方法，则运行时调用的是子类的方法，如下输出为null，纵使Sub是静态内部类：1234567891011121314151617181920public class Base &#123; private String baseName = \"base\"; public Base() &#123; callName(); &#125; public void callName() &#123; System.out.println(baseName); &#125; static class Sub extends Base &#123; private String baseName = \"sub\"; public void callName() &#123; System.out.println (baseName) ; &#125; &#125; public static void main(String[] args) &#123; Base b = new Sub(); &#125;&#125; 包访问权限类似C++中的friendly，不过还是有人吐槽，包访问权限就是个鸡肋的存在。主要有几个要点： 对于其它包类，该成员为private； 继承者与被继承者都处于同一个包才能访问包访问权限成员； 接口与抽象类 抽象类的修饰符可以为public或abstract 123// 以下合法，public和abstract都可以abstract interface InterfaceTest &#123;&#125; 内部接口修饰符还可以protected,private 12345public class TT &#123; private interface KL&#123; public void teset(); &#125;&#125; 接口可以继承接口，不可以继承类 123// 接口可以继承接口，不可以继承类interface CallableStatement extends PreparedStatement&#123;&#125; 接口的成员特点： 1234567891011121314public interface Inteface1 &#123; void sayHello(); String name = \"enjoyhot\";&#125;class Enjoyhot implements Inteface1&#123; @Override public void sayHello() &#123; // TODO Auto-generated method stub &#125;&#125;// enjoyhotSystem.out.println(Inteface1.name); 成员变量: 只能是常量。默认自动添加(也只能是)修饰符public static final成员方法: 只能是抽象方法。默认自动添加(也只能是)修饰符public abstract内部类：自动地(也只能是)修饰符public static 内部类 可声明为public，然后在外部调用： 1父.子 test = 父.new 子(); 内部类操作可直接引用外部类域(包括private),内部类可以直接访问外部类属性，包括private修饰的属性，可通过类似MyOuterClass.this的操作来获得外部类的引用； 内部类是一种编译器现象，JVM毫不知情；值得注意的是，当内部类调用外部类的私有域时候，编译器将产生一个static方法，该方法具有包访问权限，因此就提供给黑客一个修改私有域方法的切入点:-D； 在内部类不需要访问外部类对象时，应使用静态内部类，这样该内部类就不能随意访问外部非static域，达到安全的目的，否则如上点，自动转为static方法； 局部类 例子 12345678910111213141516public class LocalClass &#123; &#123; class AA&#123;&#125;//块内局部类 &#125; public LocalClass()&#123; class AA&#123;&#125;//构造器内局部类 &#125; public static void main(String[] args)&#123; &#125; public void localClass()&#123; class AA&#123;&#125;//方法内局部类 &#125; public static void main()&#123;&#125;&#125;//编译后，形成诸如：外部类名称+$+同名顺序+局部类名称//LocalClass$1AA.class LocalClass$2AA.class LocalClass$3AA.class LocalClass.class 特点 局部类的修饰符一定是包权限； 此时不能访问外部类域，访问外部调用的参数需要声明为final，编译后该final局部变量为该内部类的final实值域； 原因：假如只是外部传递过来的普通变量，调用外部方法后，这个变量将因返回而释放内存消失，这时就会出现内部类引用非法。而用final修饰后，编译器会在内部类中生成一个外部变量的拷贝。 总结而言，封闭的作用域引用了外部变量必须定义为final，在做spark的MapReduce操作时经常会遇到。网上一个应用例子如图所示： 自动拆箱装箱 自动装箱和拆箱从Java1.5开始引入，将int的变量转换成Integer对象，这个过程叫做装箱，反之将Integer对象转换成int类型值，这个过程叫做拆箱。装箱和拆箱是自动进行的非人为转换，所以就称作为自动装箱和拆箱。 byte short char int long float double boolean Byte Short Character Integer Long Float Double Boolean 举例说明 12345Integer i01=59;// Integer i011=59; //与io01比较也会自动拆装箱，范围-128~127int i02=59;Integer i03=Integer.valueOf(59);// 地址比较，1字节内才使用常量池，所以范围(-128~127)Integer i04=new Integer(59); // 自动拆装箱，对i03不拆装箱 i01和i02相比将自动拆装箱，数值相比; // 那究竟是拆还是装【疑问】 i01和i03比的是地址（在编译前定义的数值一般会从常量获取，除了valueOf操作需要值的范围-128~127（1字节）才是从常量中获取，为什么【疑问】），i01和i03都是编译之前定义的，所以是常量池中的同一个对象; i03和i04比较的是地址，因为i04是编译之后又new出来的对象，所以它的地址必然不在常量池中，所以i03==i04的结果为false。 类型Integer和int采用进行自动拆装箱比较，而equals是比值操作，无所谓引用不引用。 总而言之：a. 只要比较双方类型或者值有一方是基本类型，就会进行自动拆装箱比较。b. 当类型都是Integer时，不管值是什么类型，怎样生成的，都需要注意范围-128~127； Iterator vs IterableIterable：jdk1.8源码Iterator：jdk1.8源码 二者都是接口，foreach操作可用于任何实现Iterable接口的类对象，集合Collection、List、Set都是Iterable的实现类，所以他们及其他们的子类都可以使用for循环增强进行迭代； 通过源码可看到，Iterable调用Iterator()方法将返回一个Iterator对象。而实现了Iterator接口的对象在不同方法间进行传递的时候，由于当前迭代位置不可知，所以next()的结果也不可知。除非再为Iterator接口添加一个reset()方法，用来重置当前迭代位置。 实现Iterable接口的对象则不然：func(A){A.Iterator()}每次调用都返回一个从头开始的迭代器，各个迭代器之间互不影响。 在jdk1.8中可看到，Iterable多了两个default具体方法12345678910111213141516/** * * 为了Lambda操作 * items.forEach(k -&gt; System.out.println(\"Item:\"+k)); * */default void forEach(Consumer&lt;? super T&gt; action) &#123; Objects.requireNonNull(action); for (T t : this) &#123; action.accept(t); &#125;&#125;// Spliterator(splitable iterator可分割迭代器)接口是Java为了并行遍历数据源中的元素而设计的迭代器default Spliterator&lt;T&gt; spliterator() &#123; return Spliterators.spliteratorUnknownSize(iterator(), 0);&#125; Object JDK7中所有方法clone,equals,finalize,getClass,notify,notifyAll,hashCode,toString,wait finalize()：一旦垃圾回收器准备好释放对象占用的存储空间，GC会判断该对象是否覆盖了finalize方法，若未覆盖，直接回收；若覆盖，将对象放入F-Queue队列，由一低优先级线程执行该队列中对象的finalize方法。并且在下一次垃圾回收动作发生时，如果对象还没复活，才会真正回收对象占用的内存。 基本类型不是扩展于Object类，而数组类型[]是; Objects.equals(a,b)可以防止一方为null的情况，使用a.equals(b)前提是a!=null; 一般而言，继承Object重写equals需重写hashCode()方法，这是一种常规协定，可参考hashMap原理理解为什么；（参考Java核心技术卷I第9版, 5.2.3） IntHolderInteger与int一样，在方法参数中属于按值传递，而Integer对象不可变，因此包装器内容不会变。假如想编写一个修改数据值参数的方法就需要使用持有者类型。12345678910public class Test &#123; public static void main(String[] args) &#123; IntHolder a = new IntHolder(1000); change(a); System.out.println(a.value); &#125; private static void change(IntHolder a)&#123; a.value = a.value * 3; &#125;&#125; String引用对Java中的一个变量引用一个字符串时，可能创建对象： 如果”ABC”这个字符串在java String池里不存在，会在java String池创建这个一个String对象； 如果存在，变量直接引用这个String池里的对象，因为String是final的，可以共用。 举个说明 12345678String a = \"ABC\";String b = \"AB\";String c = b+\"C\";String d = \"A\" + \"BC\";String e = new String(\"ABC\");System.out.println(a==c); // falseSystem.out.println(a==d); // trueSystem.out.println(e==d); // false a和b都是字符串常量，所以在编译期就被确定了，c中有个b是引用不是字符串常量所以不会在编译期确定，因此前者false后者true; e会在java heap先创建一个String对象，如果”ABC”这个字符串在java String池里不存在，则在String池中再新建一个【疑问】 String.intern() String.intern()方法是native方法，用来检测在String pool是否已经有这个String存在,java1.7介绍道： When the intern method is invoked, if the pool already contains a string equal to this String object as determined by the equals(Object) method, then the string from the pool is returned. Otherwise, this String object is added to the pool and a reference to this String object is returned. It follows that for any two strings s and t, s.intern() == t.intern() is true if and only if s.equals(t) is true. All literal strings and string-valued constant expressions are interned. String literals are defined in section 3.10.5 of the The Java™ Language Specification.Returns: a string that has the same contents as this string, but is guaranteed to be from a pool of unique strings. 参考美团技术团队一篇文章http://tech.meituan.com/in_depth_understanding_string_intern.html JAVA 使用 jni 调用c++实现的StringTable的intern方法,StringTable的intern方法跟Java中的HashMap的实现是差不多的, 只是不能自动扩容。默认大小是1009，如果放进String Pool的String非常多，就会造成Hash冲突严重，从而导致链表会很长，而链表长了后直接会造成的影响就是当调用String.intern时性能会大幅下降，可参考本文HashMap介绍。 intern()的实现JDK1.6和JDK1.7实现不同，为了避免干扰，我们主要关心1.7以后的。jdk7的版本中，字符串常量池从Perm区移到正常的Java Heap区域了，jdk8直接取消了Perm区，详情参考Java高级篇；下面通过例子说明一下。 1234567891011121314151617String s = new String(\"1\"); //生成了2个对象，常量池中的“1” 和JAVA Heap中的字符串对象s.intern(); //s对象去常量池中寻找后发现 “1” 已经在常量池里了String s2 = \"1\"; //生成一个s2的引用指向常量池中的“1”对象System.out.println(s == s2); // false,Object和引用相比，不相等s = s.intern();System.out.println(s == s2); // true//生成字符串常量池中的“1”和JAVA Heap中的s3引用指向的对象StringObj(其实还有两个匿名对象，不过已经失去引用)//此时s3引用对象内容是\"11\"，但此时常量池中是没有 “11”对象的。String s3 = new String(\"1\") + new String(\"1\");s3.intern(); //将s3中的“11”字符串放入String常量池中,现在s3间接指向了“11”// 下图应该有误，经过intern之后，jdk7能使s3直接指向常量池，不经过StringObjString s4 = \"11\"; //创建的时候发现已经有这个对象了System.out.println(s3 == s4); //falses3 = s3.intern();System.out.println(s3 == s4); //true String、StringBuffer、StringBuilder 可变与不可变 String内部采用final,所以是不可变的； StringBuilder与StringBuffer可变； 是否多线程安全 String内部是常量，所以是线程安全的； StringBuilder并没有加同步的操作，不是线程安全的； StringBuffer操作加了内置同步锁，因而是线程安全的； final的常见使用场景final class 阻止被继承，其中方法将自动转为final，域不会自动转； 不能修饰接口，抽象类，原因显而易见，二者都是需要“继承”来发挥作用； final method 阻止继承之后的重写，不允许子类插手一些重要的处理，因此该方法就“失去虚拟特征”，意义： 没有动态绑定 ——&gt; 编译器优化为内联函数 ——&gt; 例如(e.getName转为e.name)拓展：虚拟机中的即时编译器处理能力不断增强，如果方法很简短，被频繁调用且没有真正地被覆盖(如经常调用父类的方法)，则即时编译器就会将这个方法进行内联处理。而如果子类这里被JVM加载进来，覆盖了内联方法，优化器将取消内联，这个过程很慢，但很少发生，所以即时编译器还是很高效的。 final variablefinal变量的值只能在构造函数中赋值或初始化时定好。1234567final String test1 = null;// errortest1 = \"\";final String test2;// acceptedtest2 = \"\"; Java中的编码 Java一律采用Unicode编码方式，每个字符无论中文还是英文字符都占用2个字节； 123456char han = '永';System.out.format(\"%x\",(short)han);//对第二个参数（短整型）格式化为十六进制输出，0x开头//输出6c38char han1 = 0x6c38;System.out.println(han1);//输出永 GBK–&gt;UTF-8 123dst = new string (src,\"GBK\").getbytes(\"UTF-8\")// ordst = new String(src.getBytes(\"GBK\"),\"UTF-8\") 面向对象的五大基本原则 单一职责原则（SRP） 开放封闭原则（OCP） 里氏替换原则（LSP） 依赖倒置原则（DIP） 接口隔离原则（ISP） 基本数据类型的转换问题 例1 12short a = 128；// 00000000 10000000byte b = (byte) a; // b = -128(后8位) 例210原码： 0000000000000000,0000000000001010；-10补码：1111111111111111,1111111111110110（取反再+1）因此 ~10 =1111111111111111,1111111111110101，减1再取反后31位得-11所以 ~10 = -11 例3==号，低精度自动转为高精度比较，如long与float，会自动转为float，+-*/亦然。 低———————————————&gt;高byte,short,char-&gt; int -&gt; long -&gt; float -&gt; double高到低需要强制转换，如：int aa = (int)4.0; switch参数是只能放int,String类型，但是放byte，short，char类型的也可以，是因为byte，short，shar可以自动提升（自动类型转换）为int,也不能boolean类型,任何类型不能转换为boolean类型型，强转也不行。 final相关 12345// 被fianl修饰的变量不会自动改变类型byte b1=1,b2=2,b3,b6;final byte b4=4,b5=6;b3=(b1+b2); /*语句1*/ // 错，高到低需要强制转换b6=b4+b5; /*语句2*/ // 对，b4+b5受final影响，是byte 引用类型Java中有强引用、软引用、弱引用、虚引用这四个概念，引用类型包含在包java.lang.ref中。 强引用（StrongReference）强引用不会被GC回收，并且在java.lang.ref里也没有实际的对应类型。举个例子来说： 1Object obj = new Object(); 软引用（SoftReference）软引用在JVM报告内存不足的时候才会被GC回收，否则不会回收，正是由于这种特性软引用在caching和pooling中用处广泛。软引用的用法 1234Object obj = new Object();SoftReference&lt;Object&gt; softRef =new SoftReference(obj);// 使用softRef.get() 获取软引用所引用的对象Object objg = softRef.get(); SoftReference 会尽可能长地保留引用直到 JVM 内存不足时才会被回收(虚拟机保证), 这一特性使得 SoftReference 非常适合缓存应用。 弱引用（WeakReference）当GC一但发现了弱引用对象，将会释放WeakReference所引用的对象，具体来说，就是当所引用的对象在JVM内不再有强引用时, GC后weak reference将会被自动回收。 虚引用（PhantomReference）当GC一但发现了虚引用对象，将会将PhantomReference对象插入ReferenceQueue队列，而此时PhantomReference所指向的对象并没有被GC回收，而是要等到ReferenceQueue被你真正的处理后才会被回收。虚引用的用法： 123456789Object obj = new Object();ReferenceQueue&lt;Object&gt; refQueue =new ReferenceQueue&lt;Object&gt;();PhantomReference&lt;Object&gt; phanRef =new PhantomReference&lt;Object&gt;(obj, refQueue);// 调用phanRef.get()不管在什么情况下永远返回nullObject objg = phanRef.get();// 如果obj被置为null，当GC发现了虚引用，GC会将phanRef插入进我们之前创建时传入的refQueue队列// 注意，此时phanRef所引用的obj对象，并没有被GC回收，在我们显式地调用refQueue.poll返回phanRef之后// 当GC第二次发现虚引用，而此时JVM将phanRef插入到refQueue会插入失败，此时GC才会对obj进行回收Reference&lt;? extends Object&gt; phanRefP = refQueue.poll(); 反射 类型类Object类中包含一个方法名叫getClass，利用这个方法就可以获得一个实例的类型类。类型类指的是代表一个类型的类，因为一切皆是对象，类型也不例外，在Java使用类型类来表示一个类型。所有的类型类都是Class类的实例。 123456A a = new A();// print equalif(a.getClass()==A.class) System.out.println(\"equal\");else System.out.println(\"unequal\"); 获取对象方法 public Method[] getDeclaredMethods()返回类或接口声明的所有方法，包括public, protected, default (package) 访问和private方法的Method对象，但不包括继承的方法。当然也包括它所实现接口的方法。 public Method[] getMethods()返回某个类的所有public方法，包括其继承类的公用方法，当然也包括它所实现接口的方法。 对于private方法的反射调用，会抛出IllegalAccessException，因此访问的话，需绕过安全管理器的控制： 1234567891011121314151617181920import java.lang.reflect.Method;public class ReflectPrivate &#123; public static void main(String[] args) throws Exception &#123; PrivateClass p = new PrivateClass(); Class&lt;?&gt; classType = p.getClass(); // 获取Method对象 Method method = classType.getDeclaredMethod(\"sayHello\", new Class[] &#123; String.class &#125;); method.setAccessible(true); String str = (String) method.invoke(p, new Object[] &#123; \"enjoyhot\" &#125;); System.out.println(str); &#125;&#125;class PrivateClass &#123; private String sayHello(String name) &#123; return \"Hello: \" + name; &#125;&#125; native方法native方法是由另外一种语言（如c/c++，FORTRAN，汇编）实现的本地方法 public final static native int w(); // 合法 abstract方法不能用native来修饰，因为native暗示这些方法是有实现体的，只不过这些实现体是非java的，但是abstract却显然的指明这些方法无实现体。 泛型擦除泛型可以说是Java中最常用的语法糖之一，因此虚拟机不支持这些语法，在编译时转化为Object，继承的时候利用桥方法动态调用，据此应考虑泛型在开发过程中的约束和局限性。一个较典型的局限性和约束是，java不支持泛型数组：123456// compile errorList&lt;String&gt;[] ls = new ArrayList&lt;String&gt;[10];// without compile errorList&lt;String&gt;[] ls = new ArrayList[10];List&lt;?&gt;[] ls = new ArrayList[5]; 第一种编译器检测出来直接报错，第二种逃过编译器的检测，设计者必须心理有数。不支持泛型数组的原因是，ls对象将编译为Object[]，再对该变量进行各种赋值操作都将逃过编译器的捕捉，假如编译器不小心指定错了类型，例如：1List&lt;Integer&gt; ——&gt; List&lt;String&gt;[] 在运行时才会报ClassCastException。 异常 throwable(接口) Error (unchecked) Exception RuntimeException (unchecked) IOException (checked) 一般而言，对于知道怎么处理的异常需要catch(A|B e),不知道的继续向上传递，通过在函数头尾部显式throws A,B； 断言 assert一般用于开发/测试中，当代码发布时，这些插入的检测语句将自动地移走，即类加载器将跳过断言代码； 补充 自加运算 123int i = 0;// 0 + 2int s=(i++)+(++i); 关于finally一般而言,不管采取什么操作，只要JVM在运行finally都会执行，除非执行某些操作终止JVM进程，譬如： 1System.exit(0); 关于finnally的例子：12345678910111213141516171819202122232425public static void main(String[] args) &#123; int k = f_test(); System.out.println(k);&#125;public static int f_test()&#123; int a = 0; try&#123; a = 1; return a; &#125; finally&#123; System.out.println(\"It is in final chunk.\"); a = 2; return a; &#125;&#125;// 输出：// It is in final chunk.// 2// 如果将 return a; 注释掉，将输出// It is in final chunk.// 1 Java标识符由数字，字母和下划线（_），美元符号（$）组成 数据结构章ArrayListFor jdk1.7 &amp; 1.8 继承关系123public class ArrayList&lt;E&gt;extends AbstractList&lt;E&gt;implements List&lt;E&gt;, RandomAccess, Cloneable, Serializable 几种构造函数 ArrayList()Constructs an empty list with an initial capacity of ten.ArrayList(Collection&lt;? extends E&gt; c)Constructs a list containing the elements of the specified collection, in the order they are returned by the collection’s iterator.ArrayList(int initialCapacity)Constructs an empty list with the specified initial capacity. 用法要点 ArrayList的底层是由一个Object[]数组构成的，Object[]数组，默认的长度是10 。当调用size时，计算的是逻辑长度，即“空元素不被计算”。 java自动增加ArrayList大小的思路是: 向ArrayList添加对象时，原对象数目加1; 如果大于原底层数组长度，则以适当长度(50%+1)新建一个原数组的拷贝，并修改原数组，指向这个新建数组; 原数组自动抛弃（java垃圾回收机制会自动回收）; size则在向数组添加对象后，自增1； ArrayList扩容通过ensureCapacity判断后可扩容50%+1，Vector是默认扩展1倍。 ArrayList()构造一个空列表，在添加第一个元素时，会自动扩展。而对于new ArrayList(20)则没有进行扩容行为; importnew：关于ArrayList的5道面试题 同步方法123456// 1//List list = Collections.synchronizedList(new ArrayList(...));// 2CopyOnWriteArrayList CopyOnWriteArrayList顾名思义，写时复制，写数组时，先复制一份出来，然后向新的容器里添加元素，可以做到安全地进行并发读。因为写的时候有加锁（源码）并且不改变旧内存，再将原容器的引用指向新的容器，因此多线程写是同步的。通过读写分离实现安全的优点，适合使用在读操作远远大于写操作的场景（与volatile类似），如缓存。 缺点： 内存占用问题进行写操作时（如add），内存中驻留两个对象内存，可能造成频繁的GC； 数据一致性问题只能保证数据的最终一致性，不能保证实时一致性。 HashMap结构 在jdk1.7中，是数组（单位称为桶）与链表（jdk1.8中改为基于红黑树的实现）的结合体。 链表的基本元素Entry，内部类有key,value,hash和next四个字段，其中next也是一个Entry类型。 操作 默认的负载因子大小为0.75，当一个map填满了75%的bucket时候，将会创建原来HashMap大小的2倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。拓展，为什么HashMap初始化的大小需要2的指数次幂：参考 再查找哈希表的内在位置时[0,length-1]有个如下的操作，从而确定h所在的位置（笔者认为觉得这是寻找桶时候的类似开放定址法的实现），这样相比不是2^n-1的全111……求余而言，需要自己去实现求余，会比较高效。 123static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。 当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，然后找到bucket位置来储存值对象。 当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap vs HashSetHashSet是基于HashMap实现的，HashSet底层采用 HashMap 来保存所有元素，所有放入HashSet中的集合元素实际上由 HashMap 的 key 来保存，而 HashMap的value则存储了一个公共的PRESENT对象，它是一个静态的Object对象。如添加操作：123public boolean add(E e) &#123; return map.put(e, PRESENT) == null;&#125; HashMap vs HashTable Hashtable是线程安全的，通过synchronized保证线程安全，并且是安全失败的。 Java快速失败与安全失败迭代器 :java迭代器提供了遍历集合对象的功能，集合返回的迭代器有快速失败型的也有安全失败型的，快速失败迭代器在迭代时如果集合类被修改，立即抛出ConcurrentModificationException异常，而安全失败迭代器不会抛出异常，因为它是在集合类的克隆对象上操作的。ArrayList，Vector，HashMap等集合返回的迭代器都是快速失败类型的。而对于Hashtable而言： the iterator in Hashtable is fail-fast but the enumerator is not fail-safe. 参考：stackoverflow Hashtable、HashMap都使用了 Iterator。而由于历史原因，Hashtable还使用了Enumeration的方式 。 哈希值的使用不同，HashTable直接使用对象的hashCode。而HashMap重新计算hash值，而且用与代替求模，如上操作中代码所示。 HashTable中hash数组默认大小是11，增加的方式是 old*2+1。HashMap数组table的长度length大小必须为2的n次方，而size默认大小是16，size和扩容后一定是原来2的指数，默认是2倍。 HashMap可以接受null键值和值，而Hashtable则不能； HashMap vs ConcurrentHashMap ConcurrentHashMap允许多个修改操作并发进行，是线程安全的； HashMap在每个链表节点中储存键值对对象（Entry对象）。链表中next不是final，所以支持往后插入。然而，HashMap在多线程情况下rehash可能出现环形链表。 参考：HashMap多线程并发问题分析参考：关于HashMap的经典面试题 ConcurrentHashMap段数量默认有16个，最大个数为1 &lt;&lt; 16= 65536 实现线程安全HashTable是一个线程安全的类，它使用synchronized来锁住整张Hash表来实现线程安全，即每次锁住整张表让线程独占，线程竞争激烈的情况下HashTable的效率非常低下。而对于ConcurrentHashMap： ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。它使用了多个锁来控制对hash表的不同部分进行的修改。 ConcurrentHashMap内部使用段(Segment)来表示这些不同的部分，每个段其实就是一个小的Hashtable，它们有自己的锁。只要多个修改操作发生在不同的段上，它们就可以并发进行。只有对全局需要改变时锁定的是所有的segment，如size()。 对于一个key，需要经过三次hash操作（哪个段，哪个桶，链表哪一位），才能最终定位这个元素的位置。 基本实现图如下【jdk1.7】 在散列时如果产生“碰撞”，将采用“分离链接法”来处理“碰撞”：把“碰撞”的 HashEntry 对象链接成一个链表。由于 HashEntry 的 next 域为 final 型，所以新节点只能在链表的表头处插入。参考：ConcurrentHashMap高并发性的实现机制 ConcurrentHashMap1.6使用的是Segement（继承自ReentrantLock）分段锁的技术来保证同步的，使用synchronized关键字保证线程安全的是HashTable。1.8之后ConcurrentHashMap改变了实现方式，将原来的Segment用单向链表来替代，put的时候对目标链表的头节点加锁，而这时用的就是synchronized。 ConcurrentHashMap1.6使用的是Segement（继承自ReentrantLock）分段锁的技术来保证同步的，使用synchronized关键字保证线程安全的是HashTable。1.8之后ConcurrentHashMap改变了实现方式，将原来的Segment(table)用单向链表来替代，put的时候对目标链表的头节点加锁，而这时用的也是synchronized。 效率：System.arraycopy &gt; System.copyOf(本质上新建了数组，并调用了前者) &gt; for循环 集合框架篇安全的集合： ArrayList，Vector，HashMap等集合返回的迭代器都是快速失败类型的。通过抛出ConCurrenceModificationException的异常保证安全。 常用对象分类Collection List LinkedList 内在为链表实现，插入，删除效率高于ArrayList ArrayList 底层为数组实现，每次扩容都需要把整个数据复制 Vector (安全，但已经很少使用了) Stack (安全) Set TreeSet 插入时按照红黑树排序，速率相比普通Set会慢一些，时间复杂度为log2N HashSet set的常用对象 Queue PriorityQueue（大数据量求TopK操作） 迭代器非按照元素的排列顺序排列，但remove时是按照优先级数最小的元素进行取出，即优先级最高的元素。 优先级队列中的元素可以按照任意的顺序插入，却总是按照升序的顺序进行检索。无论何时调用remove方法，总会获得当前优先级队列中的最小元素，但并不是对所有元素都排序。它是采用了堆（一个可以自我调整的二叉树），执行增加删除操作后，可以让最小元素移动到根。 使用普通同步容器(Vector, Hashtable)的迭代器,也需要外部锁来保证其原子性。因为普通同步容器产生的迭代器是非线程安全的。 Map HashMap 分离链接法，next不是final,因此往后插入。 TreeMap 实现了SortedMap接口，默认保证按照键的升序排列的映射表 WeakHashMap 参考http://www.cnblogs.com/Skyar/p/5962253.html Hashtable (安全) HashTable中hash数组默认大小是11，增加的方式是 old*2+1，保证奇数。分离链接法、开放定址法。","categories":[{"name":"Java","slug":"Java","permalink":"http://enjoyhot.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://enjoyhot.github.io/tags/Java/"}]},{"title":"Linux 系统管理常用命令（不定时更新）","slug":"linux-management-command","date":"2017-04-29T09:23:09.000Z","updated":"2017-04-30T09:29:40.152Z","comments":true,"path":"2017/04/29/linux-management-command/","link":"","permalink":"http://enjoyhot.github.io/2017/04/29/linux-management-command/","excerpt":"运行脚本的方法常用运行方法:123456$ bash a.sh$ chmod +x a.sh$ ./a.sh$ source a.sh","text":"运行脚本的方法常用运行方法:123456$ bash a.sh$ chmod +x a.sh$ ./a.sh$ source a.sh 用bash和用source的不同在于： 用bash执行时，shell script其实是在在父程序bash下新建了一个 bash子程序，之后这个子程序中执行，当程序执行完后，shell script里定义的变量都会随子程序的结束而消失； 用source执行时，是在父程序bash中执行,shell script里定义的变量都还在。 shell script的追踪与Debug12sh -n xx.sh # 语法检查sh -x xx.sh # 列出xx.sh的执行过程 脚本输入输出例子12345678910111213141516171819# !/bin/bash# Program:# This program is used to ouput parameter of the shell script# History:# 2013/2/3 on_1y First releasePATH=$PATHexport PATHecho &quot;The script&apos;s name is ==&gt; $0&quot;echo &quot;Total parameter number is ==&gt; $#&quot;# Check whether number of the parameter is less than 2[ &quot;$#&quot; -lt 2 ] &amp;&amp; echo &quot;The number of parameter is less than 2.Stop here.&quot; &amp;&amp; exit 0echo &quot;The whole parameter is ==&gt; &apos;$@&apos;&quot;echo &quot;The first parameter is ==&gt; $1&quot;echo &quot;The first parameter is ==&gt; $2&quot;exit 0 输入和输出为：123456$ bash sh05.sh 1a 2b 3c 4dThe script&apos;s name is ==&gt; sh05.shTotal parameter number is ==&gt; 4The whole parameter is ==&gt; &apos;1a 2b 3c 4d&apos;The first parameter is ==&gt; 1aThe first parameter is ==&gt; 2b test命令1test -e a.txt &amp;&amp; echo &quot;exists&quot; || echo &quot;not exists&quot; 当a.txt存在时，执行echo “exists”，否则执行echo “not exists”，多个命令用&amp;&amp;连接即可。改变test参数可以进行多种决策判断，可以研究一下。 关闭apache版本一1/etc/init.d/httpd stop 版本二1/etc/init.d/apache2 stop 查看端口1234567netstat -anp | grep 9200(Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.)tcp 0 0 192.168.0.1:53570 192.168.0.1:9200 ESTABLISHED -tcp6 0 0 192.168.0.1:9200 :::* LISTEN -tcp6 0 0 192.168.0.1:9200 192.168.0.1:53570 ESTABLISHED -unix 3 [ ] STREAM CONNECTED 105509200 - 参数说明：123-p process-a all,显示所有连接和监听端口。-n 以数字显示地址和端口（否则忽略虚拟机动态ip，127.0.0.1会以localhost显示） 进程相关top相关命令：c,m,u等1234// 实时查看某个进程创建的线程情况$ top -H -p &lt;pid&gt;// 类似有，但还是top信息更多$ ps -T -p &lt;pid&gt; ps显示瞬间行程(process)的动态 使用权限：所有使用者使用方式：1ps [options] [--help] 参数：ps的参数非常多,在此仅列出几个常用的参数并大略介绍含义1234-A 列出所有的进程-w 显示加宽可以显示较多的资讯-au 显示较详细的资讯-aux 显示所有包含其他使用者的行程 可添加过滤用户：1| grep xx 杀死进程：123kill -s 9 xxx(pid)# 等价于kill -9 xxx(pid) 查看运行进程的绝对路径先通过top或其它命令查看pid，再通过以下命令查看真实运行路径：12$ ls -al /proc/24416/exelrwxrwxrwx 1 hadoop hadoop 0 Apr 1 21:51 /proc/24416/exe -&gt; /home/hadoop/jdk1.8.0_40/bin/java 文件编码当将可执行的文件从windows系统搬到linux下，可能会报类似’\\r’ 找不到命令的错误，那是因为shell命令中格式与dos命令格式不对造成的，只需要运行：1dos2unix file.sh 文件统计 对当前路径下文件的存储进行从大到小的排序并打印，显示G,MB,K等 1du -sh * | sort -rh 统计第二行逗号出现的次数 1sed -n &apos;2 p&apos; test.csv/part-00000 |grep -o &quot;,&quot;|wc -l 合并操作感受下paste的魅力： 1paste -d &quot; &quot; - - - - &lt; 16_1.fastq 用户操作增加用户 方法1 12useradd -d /data/home/user1 -s /bin/bash -m user1passwd user1 方法2 12adduser --home /software/home/xxx xxxvim /etc/passwd 添加组12# add to hadoop groupssudo usermod -a -G hadoop user1 移除组1gpasswd -d 用户名 组名 无密码ssh 一般做法 生成两个重要文件1ssh-keygen -t rsa id-rsa #私钥id-rsa.pub #公钥 将公钥中的内容追加到另一台机器文件中1~/.ssh/authorized_keys 这样另一台机器就可以无密码ssh到生成公钥对应的机器。同理，其它机器也要这样做，那么假如有n台机器，每台机器authorized_keys文件需要n-1行，代表其它n-1台机器的公钥。优点麻烦，用第二种方法。 共用公私钥 生成两个重要文件1ssh-keygen -t rsa id-rsa #私钥id-rsa.pub #公钥 复制和分发新建一个authorized_keys文件，内容为id-rsa.pub中的一行，然后将这三个文件共享到其它节点的~/.ssh目录下，切记，3个文件都要拷贝。 tmux工具 删除panel: C-x + x 迅速切换排版（水平或垂直）: C-x + 空格 创建新窗口（panel) C-x + c 进行切换: C-x + 数字键 复制粘贴: C-x [ 进入复制模式 参考上表移动鼠标到要复制的区域，移动鼠标时可用vim的搜索功能”/“,”?” 安空格键开始选择复制区域 选择完成后安enter键退出 C-x ] 粘贴 清除区域内容：C-l 开机自启动Example:Ubuntu下，作为利用apt-get的常规软件，以Nginx为例 编辑开机nginx启动脚本（自动安装的忽略） /usr/local/nginx.sh 在/etc/init.d目录下创建链接文件到前面的脚本： ln -s /usr/local/nginx.sh /etc/init.d/nginx 设置nginx脚本可执行权限（自动安装的忽略） 1chmod u+x /etc/init.d/nginx 进入/etc/init.d目录，将该脚本设为开启自启动 1update-rc.d -f nginx defaults [startnum] [killnum] 其中的[startnum]表示启动顺序，[killnum]表示退出顺序，都为可选参数，取值范围是0-99。序号越大的越晚执行。 如果想取消开机自启动1update-rc.d -f nginx remove -f选项表示强制执行。 端口映射nohup portforward :8080 cu01:8080 &amp; 更改文件权限chmod ［who］ ［+ | - | =］ ［mode］ 文件名 权限范围：u ：目录或者文件的当前的用户g ：目录或者文件的当前的群组o ：除了目录或者文件的当前用户或群组之外的用户或者群组a ：所有的用户及群组 权限代号：r ：读权限，用数字4表示w ：写权限，用数字2表示x ：执行权限，用数字1表示s ：特殊权限，如root般操作该文件 e.g 为所有用户添加执行test.log的权限1chmod a+x test.log 用代号是最灵活的方式，但当只涉及对所有用户的，并且是直接设定某个权限，用数字是最直接的。1chmod 764 test.log 表示用户拥有所有权限，所在组其它成员拥有读写权限，其它用户只有读权限。 git初始化远程仓库1234cd /SHARE/repositories/mkdir webpy &amp; cd webpymkdir webpy.git &amp; cd webpy.gitgit init --bare 初始化本地仓库1234cd local_projectgit initgit add .git commit -m &quot;xxx&quot; 查看、添加、修改远程仓库12345git remote -vgit remote add origin gujw@192.168.0.2:/SHARE/repositories/webpy/webpy.gitgit remote set-url origin gujw@192.168.0.2:/SHARE/repositories/webpy/webpy.git Tip：对于共享目录的仓库而言，修改client端的ip即可，远程仓库地址不变。 修改github仓库12git remote add origin git@github.com:username/RepoName.gitgit push origin master","categories":[{"name":"Linux","slug":"Linux","permalink":"http://enjoyhot.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://enjoyhot.github.io/tags/Linux/"}]},{"title":"数据结构基础","slug":"数据结构基础","date":"2017-04-29T09:22:09.000Z","updated":"2017-04-30T09:29:40.184Z","comments":true,"path":"2017/04/29/数据结构基础/","link":"","permalink":"http://enjoyhot.github.io/2017/04/29/数据结构基础/","excerpt":"Tree完全二叉树只有最下面的两层结点度小于2，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树，不一定有排序； 满二叉树除了叶结点外每一个结点都有左右子叶且叶结点都处在最底层的二叉树。 平衡二叉树（AVL）平衡二叉树（Balanced Binary Tree）是二叉查找树（也称为排序二叉树，左小于右）的一个进化体，也是第一个引入平衡概念的二叉树。1962年，G.M. Adelson-Velsky 和 E.M. Landis发明了这棵树，所以它又叫AVL树。平衡二叉树要求对于每一个节点来说，它的左右子树的高度之差不能超过1，如果插入或者删除一个节点使得高度之差大于1，就要进行节点之间的旋转，将二叉树重新维持在一个平衡状态。这个方案很好的解决了二叉查找树退化成链表的问题，把插入，查找，删除的时间复杂度最好情况和最坏情况都维持在O(logN)。但是频繁旋转会使插入和删除牺牲掉O(logN)左右的时间，不过相对二叉查找树来说，时间上稳定了很多。","text":"Tree完全二叉树只有最下面的两层结点度小于2，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树，不一定有排序； 满二叉树除了叶结点外每一个结点都有左右子叶且叶结点都处在最底层的二叉树。 平衡二叉树（AVL）平衡二叉树（Balanced Binary Tree）是二叉查找树（也称为排序二叉树，左小于右）的一个进化体，也是第一个引入平衡概念的二叉树。1962年，G.M. Adelson-Velsky 和 E.M. Landis发明了这棵树，所以它又叫AVL树。平衡二叉树要求对于每一个节点来说，它的左右子树的高度之差不能超过1，如果插入或者删除一个节点使得高度之差大于1，就要进行节点之间的旋转，将二叉树重新维持在一个平衡状态。这个方案很好的解决了二叉查找树退化成链表的问题，把插入，查找，删除的时间复杂度最好情况和最坏情况都维持在O(logN)。但是频繁旋转会使插入和删除牺牲掉O(logN)左右的时间，不过相对二叉查找树来说，时间上稳定了很多。 可能有这种说法 一种二叉搜索树1.所有非叶子结点至多拥有两个儿子（Left和Right）；2.左边小于右边，大局观也是如此； 特点1.当B树的所有非叶子结点的左右子树的结点数目均保持差不多（平衡），那么B树的搜索性能逼近二分查找（从线性到log2N），有利于增删改查；2.同时它比连续内存空间的二分查找的优点是，这些操作不需要移动大段的内存数据，甚至通常是常数开销； 使用尽量将元素构建为左边平衡的结构: Treap树堆作用：Treap的特点是实现简单，且能基本实现随机平衡的结构,避免变为链式结构。 一棵treap是一棵修改了结点顺序的二叉查找树。通常树内的每个结点x都有一个关键字值key[x]，另外，还要为结点分配priority[x]，treap的结点排列成让关键字遵循二叉查找树性质，并且优先级遵循最小堆顺序性质： 如果v是u的左孩子，则key[v] &lt; key[u]. 如果v是u的右孩子，则key[v] &gt; key[u]. 如果v是u的孩子，则priority[u] &gt; priority[v]. treap插入操作：1.按照二叉树的插入方法，将结点插入到树中2.根据堆的性质(我们这里为最小堆)和优先级的大小调整结点位置，旋转不破坏排序树的特征。 treap删除操作：1.找到相应的结点2.若该结点为叶子结点，则直接删除；若该结点为只包含一个叶子结点的结点，则将其叶子结点赋值给它；若该结点为其他情况下的节点，则进行相应的旋转，直到该结点为上述情况之一，然后进行删除。 具体例子见：http://blog.csdn.net/yang_yulei/article/details/46005845 B-树 M叉搜索树 定义任意非叶子结点最多只有M个儿子；且M&gt;2； 根结点的儿子数为[2, M]； 除根结点以外的非叶子结点的儿子数为[M/2, M]； 每个结点存放至少M/2-1（取上整）和至多M-1个关键字；（至少2个关键字） 非叶子结点的关键字个数=指向儿子的指针个数-1； 非叶子结点的关键字：K1, K2, …, K[M-1]；且K[i] &lt; K[i+1]； 非叶子结点的指针：P1, P2, …, P[M]；其中P1指向关键字小于K1的子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树； 所有叶子结点位于同一层； 特点 关键字集合分布在整颗树中； 任何一个关键字出现且只出现在一个结点中； 搜索有可能在非叶子结点结束； 其搜索性能等价于在关键字全集内做一次二分查找； 自动层次控制； 根节点使用关键字个数：最多M-1,最少M/2-1，向上取整 使用M=3:例子：http://blog.csdn.net/u013400245/article/details/52824744 B+树 与B-树的差异 有n棵子树的结点中含有n个关键字，每个关键字不保存数据，只用来索引，所有数据都保存在叶子节点。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的非终端结点可以看成是索引部分，结点中仅含其子树（根结点）中的最大（或最小）关键字。通常在B+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点。 特点其定义基本与B-树同，除了： 非叶子结点的子树指针与关键字个数相同(据July，有争议)； 非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])的子树（B-树是开区间）； 所有关键字都在叶子结点出现； 一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针，如下图： 在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针(链指针)，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 使用 B+树的基本操作：查找操作对B+树可以进行两种查找运算： a.从最小关键字起顺序查找； b.从根结点开始，进行随机查找。 在查找时，若非终端结点上的剧组机等于给定值，并不终止，而是继续向下直到叶子结点。因此，在B+树中，不管查找成功与否，每次查找都是走了一条从根到叶子结点的路径。其余同B-树的查找类似。插入操作 B+树的插入与B树的插入过程类似。不同的是B+树在叶结点上进行，如果叶结点中的关键码个数超过m，就必须分裂成关键码数目大致相同的两个结点，并保证上层结点中有这两个结点的最大关键码。(算法见百度百科)删除操作 B+树的删除也仅在叶子结点进行，当叶子结点中的最大关键字被删除时，其在非终端结点中的值可以作为一个“分界关键字”存在。若因删除而使结点中关键字的个数少于m/2 （m/2结果取上界，如5/2结果为3）时，其和兄弟结点的合并过程亦和B-树类似。 如M=3: 1. 所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好是有序的；2. 非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层；3. 更适合文件索引系统（见FAQ）；## B-树B-树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针：M=3: 与B+-树区别： B+树的分裂：当一个结点满时，分配一个新的结点，并将原结点中1/2的数据复制到新结点，最后在父结点中增加新结点的指针；B+树的分裂只影响原结点和父结点，而不会影响兄弟结点，所以它不需要指向兄弟的指针； B*树的分裂：当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分数据移到兄弟结点中，再在原结点插入关键字，最后修改父结点中兄弟结点的关键字（因为兄弟结点的关键字范围改变了）；如果兄弟也满了，则在原结点与兄弟结点之间增加新结点，并各复制1/3的数据到新结点，最后在父结点增加新结点的指针； 所以，B*树分配新结点的概率比B+树要低，空间使用率更高；在B+树基础上，为非叶子结点也增加链表指针，将结点的最低利用率从1/2提高到2/3； 红黑树（不追求“完全平衡”）1）每个结点要么是红的，要么是黑的。2）根结点是黑的。3）每个叶结点，即空结点（NIL）是黑的。4）如果一个结点是红的，那么它的俩个儿子都是黑的。5）对每个结点，从该结点到其子孙结点的所有路径上包含相同数目的黑结点。 这些约束的好处是：保持了树的相对平衡，同时又比AVL的插入删除操作的复杂性要低许多。 由于它的设计，任何不平衡都会在三次旋转之内解决,AVL树有最多O(logN)次旋转。 如果插入一个node引起了树的不平衡，AVL和RB-Tree都是最多只需要2次旋转操作，即两者都是O(1)；但是在删除node引起树的不平衡时，最坏情况下，AVL需要维护从被删node到root这条路径上所有node的平衡性，因此需要旋转的量级O(logN)，而RB-Tree最多只需3次旋转，只需要O(1)的复杂度。 其次，AVL的结构相较RB-Tree来说更为平衡，在插入和删除node更容易引起Tree的unbalance，因此在大量数据需要插入或者删除时，AVL需要rebalance的频率会更高。因此，RB-Tree在需要大量插入和删除node的场景下，效率更高。自然，由于AVL高度平衡，因此AVL的search效率更高。 map的实现只是折衷了两者在search、insert以及delete下的效率。总体来说，RB-tree的统计性能是高于AVL的。 http://www.importnew.com/21818.html 为什么需要红黑树？map,set底层都提供了排序功能，且查找速度快。红黑树实际上是AVL的一种变形，但是其比AVL(平衡二叉搜索树)具有更高的插入效率，当然查找效率会平衡二叉树稍微低一点点，毕竟平衡二叉树太完美了。但是这种查找效率的损失是非常值得的。它的操作有着良好的最坏情况运行时间，并且在实践中是高效的: 它可以在O(logn)时间内做查找，插入和删除，这里的n是树中元素的数目。 排序堆排序输入无序序列 { 1, 3, 4, 5, 2, 6, 9, 7, 8, 0 } ，如下所示，先建最大堆，建立最大堆的过程是先将原序列按照完全二叉树的顺序进行排列，然后从N/2个节点（向下取整）开始进行交换。建完堆之后，再利用堆调整过程不断取堆顶，最终获得排序后结果。 几种常见的数据结构的操作性能对比 关于树的FAQ 红黑树用于内存搜索，磁盘搜索用的多是多路树？因为磁盘读取效率是很慢的，读取次数是基于树高度，所以像数据库中的B/B+索引都是多路树，每一个block中存放多个子节点信息，减少磁盘读取次数。 为什么说B+-tree比B树更适合实际应用中操作系统的文件索引和数据库索引？ B+-tree的磁盘读写代价更低B+-tree的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B 树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。 举个例子，假设磁盘中的一个盘块容纳16bytes，而一个关键字2bytes，一个关键字具体信息指针2bytes。一棵9阶B-tree(一个结点最多8个关键字)的内部结点需要2个盘快（28+28）。而B+树内部结点只需要1个盘快。当需要把内部结点读入内存中的时候（查找是基于节点信息），B 树就比B+ 树多一次盘块查找时间(在磁盘中就是盘片旋转的时间)。 B+-tree的查询效率更加稳定（查询时间的均衡）由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 读者点评： 本文评论下第149楼，fanyy1991针对上文所说的两点，道：个人觉得这两个原因都不是主要原因。数据库索引采用B+树的主要原因是（上述从检索的角度，这里从遍历的角度）：B-树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。正是为了解决这个问题，B+树应运而生。B+树只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作（或者说效率太低）。 不同的引擎对于索引有不同的支持：Innodb和MyISAM默认的索引是Btree索引（之后版本B+树）；而Memory默认的索引是Hash索引（哈希值可能重复，所以检索效率未必就高）。","categories":[{"name":"Basis","slug":"Basis","permalink":"http://enjoyhot.github.io/categories/Basis/"}],"tags":[{"name":"Basis","slug":"Basis","permalink":"http://enjoyhot.github.io/tags/Basis/"}]},{"title":"MySQL","slug":"MySQL","date":"2017-04-29T09:21:09.000Z","updated":"2017-04-30T09:29:40.160Z","comments":true,"path":"2017/04/29/MySQL/","link":"","permalink":"http://enjoyhot.github.io/2017/04/29/MySQL/","excerpt":"设计三大范式范式越高，数据冗余越少，但是增加查询复杂度，表链接时效率更低。范式低，数据冗余高，但是查询会更快。 第一范式，确保属性的原子性，如“地址”属性应该再细分为国家，省，市，区等； 第二范式，如果一个关系满足第一范式,并且除了主键以外的其它列,都依赖于该主键,则满足第二范式； 第三范式，表中每列都与主键直接相关，如订单记录中，虽说顾客编号和顾客名称都与主键（订单编号）相关，满足第二范式，但顾客名称不是直接相关；","text":"设计三大范式范式越高，数据冗余越少，但是增加查询复杂度，表链接时效率更低。范式低，数据冗余高，但是查询会更快。 第一范式，确保属性的原子性，如“地址”属性应该再细分为国家，省，市，区等； 第二范式，如果一个关系满足第一范式,并且除了主键以外的其它列,都依赖于该主键,则满足第二范式； 第三范式，表中每列都与主键直接相关，如订单记录中，虽说顾客编号和顾客名称都与主键（订单编号）相关，满足第二范式，但顾客名称不是直接相关； 存储引擎简介 MyISAM这种引擎是mysql最早提供的。这种引擎又可以分为静态MyISAM、动态MyISAM 和压缩MyISAM三种： 静态MyISAM：如果数据表中的各数据列的长度都是预先固定好的，服务器将自动选择这种表类型。因为数据表中每一条记录所占用的空间都是一样的，所以这种表存取和更新的效率非常高。当数据受损时，恢复工作也比较容易做。 动态MyISAM：如果数据表中出现varchar、xxxtext或xxxBLOB字段时，服务器将自动选择这种表类型。相对于静态MyISAM，这种表存储空间比较小，但由于每条记录的长度不一，所以多次修改数据后，数据表中的数据就可能离散的存储在内存中，进而导致执行效率下降。同时，内存中也可能会出现很多碎片。因此，这种类型的表要经常用optimize table 命令或优化工具来进行碎片整理。 压缩MyISAM：以上说到的两种类型的表都可以用myisamchk工具压缩。这种类型的表进一步减小了占用的存储，但是这种表压缩之后不能再被修改。另外，因为是压缩数据，所以这种表在读取的时候要先时行解压缩。但是，不管是何种MyISAM表，目前它都不支持事务，行级锁和外键约束的功能。 MyISAM Merge引擎这种类型是MyISAM类型的一种变种。合并表是将几个相同的MyISAM表合并为一个虚表。常应用于日志和数据仓库。 InnoDBInnoDB表类型可以看作是对MyISAM的进一步更新产品，它提供了事务、行级锁机制和外键约束的功能。 Memory(heap)这种类型的数据表只存在于内存中。它使用散列索引，所以数据的存取速度非常快。因为是存在于内存中，所以这种类型常应用于临时表中。 archive这种类型只支持select和insert语句，而且不支持索引。常应用于日志记录和聚合分析方面。 MySql支持的表类型不止上面几种。不同的引擎对于索引有不同的支持，如Innodb和MyISAM默认的索引是Btree索引（之后版本B+树）；而Memory默认的索引是Hash索引（哈希值可能重复，所以检索效率未必就高） 数据库锁 锁粒度MySQL有三种锁的级别：页级、表级、行级。表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 常见存储引擎采用锁机制MyISAM和MEMORY存储引擎采用的是表级锁（table-level locking）；BDB(BerkeleyDB)存储引擎采用的是页面锁（page-level locking），但也支持表级锁；InnoDB存储引擎既支持行级锁（row-level locking），也支持表级锁，但默认情况下是采用行级锁。 锁机制 共享锁锁粒度是行或者元组（多个行），读操作 排它锁锁粒度是行或者元组（多个行），写操作 说明 e.g.如果事务t1获取了一个元组的共享锁，事务t2还可以立即获取这个元组的共享锁，但不能立即获取这个元组的排它锁（必须等到t1释放共享锁之后）。如果事务t1获取了一个元组的排它锁，事务t2不能立即获取这个元组的共享锁，也不能立即获取这个元组的排它锁（必须等到t1释放排它锁之后）。 锁的应用共享锁、排它锁、意向共享锁、意向排它锁相互之前都是有兼容/互斥关系的，可以用一个兼容性矩阵表示(y表示兼容，n表示不兼容): 12345 X S IX ISX n n n nS n y n yIX n n y yIS n y y y 兼容性矩阵为什么是这个样子的？X和S的相互关系在上文中解释过了，IX和IS的相互关系全部是兼容，这也很好理解，因为它们都只是“有意”，还处于YY阶段，没有真干，所以是可以兼容的； 事务操作 InnoDB支持事务处理，InnoDB写操作快，update快，外键操作效率高 MyISAM不支持事务操作，读取快，count操作快（因为保存了行数）。 比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！123delete from userinfo where ~~~delete from mail where ~~delete from article where~~ 如果没有事务处理，在你删除的过程中，假设出错了，只执行了第一句，那么其后果是难以想象的！但用事务处理，像这样类似的一系列操作，要么全部正确执行，要么全部不执行。事务处理就是来做这个事情的。参考：http://blog.csdn.net/z702143700/article/details/46048495具体例子：https://segmentfault.com/q/1010000002952450 Tips 使用表锁： 当事务需要更新大部分数据时，表又比较大，如果使用默认的行锁，不仅效率低，而且还容易造成其他事务长时间等待和锁冲突。 事务比较复杂，很可能引起死锁导致回滚。 关于死锁： MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。 避免死锁 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。 常规表操作读操作 脏读（Dirty Read)脏读意味着一个事务读取了另一个事务未提交的数据,而这个数据是有可能回滚。 不可重复读(Unrepeatable Read)也称为虚读，不可重复读意味着，在数据库访问中，一个事务范围内两个相同的查询却返回了不同数据。这是由于查询时系统中其他事务修改的提交而引起的。例如：事务B中对某个查询执行两次，当第一次执行完时，事务A对其数据进行了修改。事务B中再次查询时，数据发生了改变 幻读(phantom read)幻读,是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 表复制 类型1,table2-&gt;table1，前提：table1需存在 1insert into table1(field1,field2) select field1,field2 from table2; 类型2,table2+table3-&gt;table1，前提：table1需存在 1insert into table1(field1,field2) select * from(select field1,field2 from table2 join table3) as table23; 类型3，table2-&gt;table1，前提：table1不能存在 1select field1,field2 into table1 from table2; 临时表作用：专用性和私有性，速度上和效率上更高，使用时需先建表，在从大表中抽取数据，在连接关闭后会自动销毁。12345create temporary table table1( id Integer Not null Auto increasement, name varchar(255) not null, primary key(id)); 内存表作用：数据在内存中，表结构持久化到磁盘，show tables时可见，其执行引擎为memory，一般较少用。 关联查询mysql默认的join是inner join。 ### cross join交叉连接，以例子说明：1select * from table1 cross join table2;笛卡尔乘积，从table1循环取出每一条记录，每条都与table2匹配，都保留，实际场景中较少用。### inner [outer] join(有无outer都一样，下同)内连接，根据on匹配，相当于求交集（多个匹配则相乘），没有on则求并集，与交叉连接一样；1SELECT * FROM TableA INNER JOIN TableB ON TableA.name=TableB.name;### left [outer] join以左表为主表，从右表中寻找匹配的记录进行拼接，没有匹配的为null，right join同理。1SELECT * FROM TableA LEFT OUTER JOIN TableB ON TableA.name = TableB.name； full [outer] join产生A和B的并集。对于没有匹配的记录，则会以null做为值。1SELECT * FROM TableA FULL OUTER JOIN TableB ON TableA.name = TableB.name; 可以通过is NULL将没有匹配的值找出来：12SELECT * FROM TableA FULL OUTER JOIN TableB ON TableA.name = TableB.nameWHERE TableA.id IS null OR TableB.id IS null union联合查询1234# 求并集select * from table1 union select * from table2;# 求叠加（记录数是二者的总和）select * from table1 union all select * from table2; 总结 子查询1234select * from table1 where f1=(select * from table2 where f2=\"xxx\");# 等价于select * from table1 where f1 in (select * from table2 where f2=\"xxx\");select * from table1 where exists (select * from table2 f1&gt;=200); where字句常用方式 where age in(13,14); where date betweent “21” and “24”; where studentName like ‘小明%’； 分组函数与having一般而言，where能用，having也能用，但having效率低，一般只用在where不能用的场合，诸如聚合操作(sum,min,max,avg,count)作用后产生的列，where不能用，现在就只能使用having，如下场景只能用having:1234select field1,count(*) from table1 having count(*) &gt; 4;select filed1,count(*) as num from table1 having num &gt; 4;# 当然这样是不可以的select goods_name,goods_number from sw_goods having goods_price &gt; 100 //报错！！！因为前面并没有筛选出goods_price 字段 JDBCJDBC提供了Statement、PreparedStatement 和 CallableStatement三种方式来执行查询语句，其中: Statement 用于通用查询 PreparedStatement 用于执行参数化查询 CallableStatement则是用于存储过程。 索引先快速看几篇文章http://www.cnblogs.com/cy163/archive/2008/10/27/1320798.html Mysql有BTree索引和Hash索引，而oracle除了这两种选择，还有Bitmap位图索引。 索引的作用索引用于快速找出在某个列中有一特定值的行。不使用索引，MySQL必须从第1条记录开始然后读完整个表直到找出相关的行，还需要考虑每次读入数据页的IO开销。而如果采取索引，则可以根据索引指向的页以及记录在页中的位置，迅速地读取目标页进而获取目标记录。 使用索引需要注意 只对WHERE和ORDER BY需要查询的字段设置索引，避免无意义的硬盘开销； 组合索引支持前缀索引； 更新表的时候，如增删记录，MySQL会自动更新索引，保持树的平衡；因此更多的索引意味着更多的维护成本; 可通过explain语句查看SQL语句的分析结果; 什么情况下设置了索引但无法使用① 以“%”开头的LIKE语句，模糊匹配,如:123like %keyword #索引失效，使用全表扫描。但可以通过翻转函数+like前模糊查询+建立翻转函数索引=走翻转函数索引，不走全表扫描。like keyword% #索引有效like %keyword% #索引失效，也无法使用反向索引。 ② OR语句前后没有同时使用索引，需要将where语句的字段也加上order，如：假设已经建立以下索引：12CREATE INDEX mytable_categoryid_userid_adddate ON mytable (category_id,user_id,adddate); 进行查找时应：123SELECT * FROM mytable WHERE category_id=1 AND user_id=2 ORDER BY category_id DESC,user_id DESC,adddate DESC; ③ 数据类型出现隐式转化（如varchar不加单引号的话可能会自动转换为int型）；12SELECT * FROM `mytable` WHERE `a`='12345' #正确版本SELECT * FROM `mytable` WHERE `a`=12345 #错误版本，虽可查到，但变慢 索引的一种分法 index —-普通的索引,数据可以重复 fulltext—-全文索引，用来对大表的文本域(char，varchar，text)进行索引。语法和普通索引一样。 unique —-唯一索引,唯一索引,要求所有记录都唯一 primary key —-主键索引,也就是在唯一索引的基础上相应的列必须为主键 事务数据库事务的四大特性(简称ACID) 原子性(Atomicity)事务的原子性指的是，事务中包含的程序作为数据库的逻辑工作单位，它所做的对数据修改操作要么全部执行，要么完全不执行。这种特性称为原子性。例如银行取款事务分为2个步骤(1)存折减款(2)提取现金。不可能存折减款，却没有提取现金。2个步骤必须同时完成或者都不完成。 一致性(Consistency)事务的一致性指的是在一个事务执行之前和执行之后数据库都必须处于一致性状态。这种特性称为事务的一致性。假如数据库的状态满足所有的完整性约束，就说该数据库是一致的。例如完整性约束a+b=10，一个事务改变了a，那么b也应随之改变。 分离性(亦称独立性Isolation)分离性指并发的事务是相互隔离的。即一个事务内部的操作及正在操作的数据必须封锁起来，不被其它企图进行修改的事务看到。假如并发交叉执行的事务没有任何控制，操纵相同的共享对象的多个并发事务的执行可能引起异常情况。 持久性(Durability)持久性意味着当系统或介质发生故障时，确保已提交事务的更新不能丢失。即一旦一个事务提交，DBMS保证它对数据库中数据的改变应该是永久性的，即对已提交事务的更新能恢复。持久性通过数据库备份和恢复来保证。 解决高并发 分库，分表，分布式，增加二级缓存等； 水平分库分表，由单点分布到多点数据库中，从而降低单点数据库压力。 读写分离策略：极大限度提高应用中Read数据的速度和并发量。无法解决高写入压力。 解决高并发锁的争用 行锁，但似乎高并发情况下效率会下降较多； 换用Redis，有个watch命令（乐观锁），监听key是否被改变，而一般真实环境下Redis又是单进程的，command是one by one的。但是watch用在分布式操作上却是有用的。","categories":[{"name":"Basis","slug":"Basis","permalink":"http://enjoyhot.github.io/categories/Basis/"}],"tags":[{"name":"Basis","slug":"Basis","permalink":"http://enjoyhot.github.io/tags/Basis/"}]},{"title":"或明","slug":"or-know","date":"2016-08-01T13:23:50.000Z","updated":"2017-04-16T14:48:30.662Z","comments":true,"path":"2016/08/01/or-know/","link":"","permalink":"http://enjoyhot.github.io/2016/08/01/or-know/","excerpt":"时值8月，离毕业更近了。","text":"时值8月，离毕业更近了。在入学时就开始算计留在学校还有多少时间，毕竟，在学校的日子将会一去不复返，总得做些有意义的事情，弥补一些遗憾或空缺。 依托客观存在的24小时，寻找忙碌的理由，为未来打算，是多年来的行事法则。希望每天过得充实而又收获，但往往做到一半（呵呵），这又是另外一种境遇。有个计划，遵守计划，能完成78成，就很成功。 懂得舍弃，生活不乏单调，或许，光明只有一步之遥。附上最近脑海里浮现的歌。","categories":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/categories/Life/"}],"tags":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/tags/Life/"}]},{"title":"ensemble-stacking","slug":"ensemble-stacking","date":"2016-05-08T09:04:30.000Z","updated":"2017-04-16T14:48:30.634Z","comments":true,"path":"2016/05/08/ensemble-stacking/","link":"","permalink":"http://enjoyhot.github.io/2016/05/08/ensemble-stacking/","excerpt":"测试blog 相比 Blending，Stacking 能更好地利用训练数据。以 5-Fold Stacking 为例，它的基本原理如图所示：","text":"测试blog 相比 Blending，Stacking 能更好地利用训练数据。以 5-Fold Stacking 为例，它的基本原理如图所示： 整个过程很像 Cross Validation。对于一个model，首先将训练数据分为 5 份，接下来一共5个迭代，每次迭代时，将 4 份数据作为 Training Set 对每个 Base Model 进行训练，然后在剩下一份 Hold-out Set 上进行预测。那么一个model训练最终将合成一个完整对于原始训练样本的预测单列矩阵。 同时也要将其在测试数据上的预测保存下来。这样，每个 Base Model 在每次迭代时会对训练数据的其中 1 份做出预测，对测试数据的全部做出预测。 多个model迭代都完成以后我们就获得了一个 #训练数据行数 x #Base Model 数量 的矩阵，这个矩阵接下来就作为第二层的 Model 的训练数据。当第二层的 Model 训练完以后，将之前保存的 Base Model 对测试数据的预测（因为每个 Base Model 被训练了 5 次，对测试数据的全体做了 5 次预测，所以对这 5 次求一个平均值形成一列，多个model形成多列，从而得到一个形状与第二层训练数据相同的矩阵）拿出来让它进行预测，就得到最后的输出。 总的来说，Stacking 方法比任何单一模型的效果都要好，而且不仅成功应用在了监督式学习中，也成功应用在了非监督式(概率密度估计)学习中。甚至应用于估计bagging模型的错误率。据论文Feature-Weighted Linear Stacking(Sill, J. and Takacs, G. and Mackey L. and Lin D., 2009, arXiv:0911.0460)而言，Stacking比Bayesian Model Averaging表现要更好！此外在Kaggle上，很多比赛多是通过Stacking获取优秀的结果！","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://enjoyhot.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://enjoyhot.github.io/tags/Machine-Learning/"}]},{"title":"MS-document-online-preview","slug":"MS-document-online-preview","date":"2016-03-12T10:41:00.000Z","updated":"2017-04-16T14:48:30.658Z","comments":true,"path":"2016/03/12/MS-document-online-preview/","link":"","permalink":"http://enjoyhot.github.io/2016/03/12/MS-document-online-preview/","excerpt":"doc,xls,ppt,pdf实现浏览器在线预览小记 一般有两种方式方式一：步骤 文档（PDF，Word，PPT等）–&gt;PDF–&gt;浏览器加载pdf插件在线浏览 转换pdf的软件： office:调用com接口，从而调用软件运行转换程序。 Openoffice： 目前隶属于Apache，从Openoffice.org沿袭而来。 LibreOffice： OpenOffice.org的分支,第一个正式版。 为3.3，对格式和字体的支持、对插件扩展与Openoffice有所侧重。","text":"doc,xls,ppt,pdf实现浏览器在线预览小记 一般有两种方式方式一：步骤 文档（PDF，Word，PPT等）–&gt;PDF–&gt;浏览器加载pdf插件在线浏览 转换pdf的软件： office:调用com接口，从而调用软件运行转换程序。 Openoffice： 目前隶属于Apache，从Openoffice.org沿袭而来。 LibreOffice： OpenOffice.org的分支,第一个正式版。 为3.3，对格式和字体的支持、对插件扩展与Openoffice有所侧重。 目前网络上各种转换方法，归根到底大多用到前两者，只是做了不同的代码封装。特别提一下，jacob.jar实现Word转换成PDF，一般用于Java，本质上也还是调用office。 doc2pdf软件优缺点（针对pythoner来说） office:自己安装com接口插件，调用软件运行转换程序,软件过于庞大，对于性能不太好的机器有时可能会崩溃。 Openoffice： 自带调用com接口API，python2.7.6内核，但是对于doc,docx格式兼容不好，转换格式不兼容的字体或者表格排版就呵呵了。 LibreOffice：自带调用com接口API，python3.3内核，暂时找不到2.7的，对doc,docx兼容较好。 目前自三种软件调用的脚步程序一抓一大把，都大致差不多，要注意的地方是，Dispatch, constants参数的慎重选择，如constants.wdXXX有时获取不靠谱，需要自己指定值。 浏览器pdf插件 下载自动安装有pdf插件的浏览器，目前chrome、搜狗、火狐都支持。 下载各种pdf阅读器进行关联，浏览器代理设置允许加载pdf阅读插件。 加载js插件，需要浏览器支持才行，不然还是会变成下载。http://www.cnblogs.com/58top/archive/2012/11/26/a-list-of-jquery-pdf-viewers-available-at-the-moment.html 试过：PDFObject，jQuery Media Plugin 方式二：步骤 文档（PDF，Word，PPT等）–&gt;PDF–&gt;转换为SWF–&gt; 使用FlexPaper在线浏览 软件介绍 Openoffice： word转pdf，同上分析。 SWFTools：将PDF转换为swf。 FlexPaper：一个开源轻量级的在浏览器上显示各种文档的组件，已经嵌入了flash播放器。 目前的优缺点 转换成功后crash的可能应该比直接用pdf低，保密性也较好，防复制防保存的功能可以进一步开发。 兼容性较好，无需担心浏览器插件问题。 处理流程多，对系统负担加大。 另外: 转换为html非常不靠谱。转换为html非常不靠谱。转换为html非常不靠谱。","categories":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/tags/Python/"}]},{"title":"Git 协作纪要","slug":"git-notes","date":"2015-12-12T10:41:00.000Z","updated":"2017-04-16T14:48:30.634Z","comments":true,"path":"2015/12/12/git-notes/","link":"","permalink":"http://enjoyhot.github.io/2015/12/12/git-notes/","excerpt":"一、常用开发模式先看一张图 注意：1、少用pull或不用pull；2、本地的多个分支，如果不是用命令行的话，对文件修改是对所有分支内容修改。 git clone -&gt; 修改 -&gt;","text":"一、常用开发模式先看一张图 注意：1、少用pull或不用pull；2、本地的多个分支，如果不是用命令行的话，对文件修改是对所有分支内容修改。 git clone -&gt; 修改 -&gt; 以下分情况讨论： 1、当你采用本地commit再远程push的连贯方式 (将远程master获取到本地仓库假设为tmp，为后期查看log或merge等)git fetch origin master -&gt;(查看本地master和刚才fetch的origin/master直接2条差异，-p查看详细)git log master origin/master -n 示例： -&gt;(合并origin/master到本地的master，这一步往往比较揪心，可能有冲突，解决后需：git add xx git commit xx) 当别人比你先提交时，会出现这个错误： 解决：git stash git pull git stash pop (一个个文件查看改动，无问题则add，然后commit，然后git diff –stat看看还有多少)git diff -w app/auth/views.py(add 一个少一个)git diff --stat (记得commit)git merge origin/master-&gt;(提交到远程origin/master)git push origin/master2、当你采用本地commit，过一段较长时间再远程push的方式 -&gt;git fetch origin master-&gt;git merge origin/master这时那些不一致的就会发生冲突 修改文件内容的冲突后，这时你通过git add和git commit可解决 -&gt;git push origin/master 3、tips 当查看别人甚至做了一些调试，要撤销pull以来（或者说commit之后）的操作，则用git checkoutgit checkout -- filename用暂存区中filename文件来覆盖工作区中的filename文件(慎用【git checkout . 】表示所有)。 二、改进工作流习惯了merge之后，对git的工作流维护也是相当重要，主要是方便差错，对于各个commit的整理也是有益的。1、git rebase重新定义(re)起点(base),整合工作流。 merge git checkout mywork git merge origin 会生成如下的树（借用网上的图）： 这样C5、C6都属于工作流的一部分，当你回退到C5时就没应用到C4的修改，需要再进行操作。 如果用rebase： git checkout mywork git rebase origin 把orgin的最新commit C4 作为当前分支mywork的基础，则生成的树图示如下： 配合远程 git pull --rebase origin master –rebase选项告诉Git，在同步了中央仓库的修改之后，将Mary所有的提交移到master分支的顶端。 假如冲突 git add . git rebase --continue 注：rebase的操作相对于merge操作，因为发生了时间跳跃，当发生冲突时会比较混乱（虽然 我还没遇到）。 三、分支1、新建本地分支git branch gujw2、查看本地分支git branch3、切换本地分支git checkout gujw示例： 4、查看远程分支git branch -a示例： ###三、仓库移动 当远程仓库域名什么的发生变化，应该是比较常见的情况。 1、查看当前本地仓库链接到的远程仓库git remote -v2、按照格式更改url即可git remote set-url XXXXX ###四、撤销操作 如果你推送到remote的commit没有被其他人pull过，那么你可以使用git reset --hard &lt;commit-hash&gt; //可以用客户端Amend代替 git push -f origin master …","categories":[{"name":"Git","slug":"Git","permalink":"http://enjoyhot.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://enjoyhot.github.io/tags/Git/"}]},{"title":"通过欢迎度对网页排名","slug":"通过欢迎度对网页排名","date":"2015-05-03T07:30:50.000Z","updated":"2017-04-16T14:48:30.690Z","comments":true,"path":"2015/05/03/通过欢迎度对网页排名/","link":"","permalink":"http://enjoyhot.github.io/2015/05/03/通过欢迎度对网页排名/","excerpt":"声明：由于此次第一次使用动态加载的表达式，hexo对LaTex支持可能不太好，因此这里有些公式显示奇奇怪怪，排版比较好一点的参考csdn:博客链接,下次就直接上图片算了。免得这边不兼容。 一、欢迎度历史1998年 Jon Kleinberg致力于HITS的万维网搜索引擎项目，其中的算法使用了万维网的超链接结构来改进搜索引擎所得的结果。 同时，在离他很近的斯坦福大学里，两名博士生正在从事一项名为PageRank的类似项目，分别是Sergey Brin和Larry Page。","text":"声明：由于此次第一次使用动态加载的表达式，hexo对LaTex支持可能不太好，因此这里有些公式显示奇奇怪怪，排版比较好一点的参考csdn:博客链接,下次就直接上图片算了。免得这边不兼容。 一、欢迎度历史1998年 Jon Kleinberg致力于HITS的万维网搜索引擎项目，其中的算法使用了万维网的超链接结构来改进搜索引擎所得的结果。 同时，在离他很近的斯坦福大学里，两名博士生正在从事一项名为PageRank的类似项目，分别是Sergey Brin和Larry Page。这两个模型之间的联系令人吃惊。然而，Jon并未试图将HITS发展成为一家公司，不过后来企业家们却试着去做了，并因此而赋予了HITS那迟到的商业成功。搜索引擎Teoma的技术基础就是HITS算法的一个扩展。 在介绍之前，我们先将万维网的超链接结构形成一个巨大的有向图，有向链接分为岀链和入链。 网络有向图 1.1 PageRankPagerank的论点是，如果一个网页被其他重要的页面所指向，那它就是重要的。值得注意的是，推荐者的地位对推荐的作用是有一定联系的，也与推荐者推荐的总数有所关联。 1.2 HITSHITS定义了枢纽（hub）和权威（authority），这是网页的属性，当然一个网页可以两者都是。论点是，如果一个页面指向好的权威网页，那它就是一个好的枢纽网页（从而配上一个高的枢纽评分）；而如果一个网页被好的枢纽网页所指向，那它就是一个好的权威网页。 1.3 查询相关性如果每个页面的欢迎度评分是离线确定的，并且对于无论哪个查询而言（直至下次更新前）均保持恒定，则排名称为查询无关的PageRank是查询无关的，HITS的原始版本是查询相关的。它们二者都可以修改为相反的那个类型。 二、谷歌的PageRank数学2.1 求和公式某个页面的Pi的PageRank记为r(Pi)，它是所有指向Pi的页面的PageRank之和。$$ r(Pi) =\\sum{Pj \\in B{P_i}} \\dfrac{r(P_j)}{|Pj|} $$Bpi为指向Pi的页面集合，|Pj|是由Pj发出的岀链数量。通过迭代的方式，解决通过未知页面的PageRank值得出另外未知页面的PageRank值。为了定义这一迭代过程，我们引入更多的记号，令$r{k+1}(Pi)$表示为页面Pi在第k+1次循环时的PageRank，则$$ r{k+1}(Pi) =\\sum{Pj \\in B{P_i}} \\dfrac{r_k(P_j)}{|Pj|}(1) $$将所有页面Pi均具有$r{0}(P_i) =1/n$开始，并一直重复下去，直至收敛到稳定值。 2.2求和方程的矩阵表示打个比方，以下面这个图为例我们引入一个n x n阶的矩阵H和一个1 x n阶的行向量$\\pi^T$。矩阵H是一个行归一化超链接矩阵，则上图相应H为：$$ H= \\begin{matrix} \\pmatrix{ 0 &amp; 1/2 &amp; 1/2 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 1/3 &amp; 1/3 &amp; 0 &amp; 0 &amp; 1/3 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1/2 &amp; 1/2 \\ 0 &amp; 0 &amp; 0 &amp; 1/2 &amp; 0 &amp; 1/2 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\} \\end{matrix} $$向引入行向量$\\pi^{(k)T}$，它表示在第k次循环时的PageRank向量。则(1)式可表示为：$$\\pi^{(k+1)T}=\\pi^{(k)T}H (2)$$从这个H矩阵可以看出一些端倪，书中总结出4个，我觉得关键在于在计算中通过矩阵计算降低复杂度，这在现实中是个稀疏矩阵，复杂度对于理论来说由平方降为1次。 2.3迭代过程问题是否会收敛？收敛到唯一向量吗？是否与初始向量有关？收敛时间？ 以下分析为两位Google创始人解决的迭代问题。 布林和佩奇最初利用$\\pi^{(0)T}=1/ne^T$来开始迭代过程，但这可能导致排名下沉(rank sink)，譬如出现多个0，可比性降低，排名下沉很大程度上与悬挂节点（岀链为0）的出现有关。 为了解决这个问题，他们使用了随机上网者的概念，对H矩阵进行调整，随机上网者特点为： 随机岀链，包括悬挂节点 随机上网者在某个特定页面上所停留的时间比例便是该页面相对重要性的一个度量 于是，我们将H矩阵变为随机矩阵S：$$ S= \\begin{matrix} \\pmatrix{ 0 &amp; 1/2 &amp; 1/2 &amp; 0 &amp; 0 &amp; 0 \\ 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 \\ 1/3 &amp; 1/3 &amp; 0 &amp; 0 &amp; 1/3 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1/2 &amp; 1/2 \\ 0 &amp; 0 &amp; 0 &amp; 1/2 &amp; 0 &amp; 1/2 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\} \\end{matrix} $$可以证明，S是由H的一个秩一修正所给出的，即$S=H + a(1/n e^T)$，其中，若页面i为悬挂结点则ai=1，否则为0。二值向量a为悬挂结点向量，为列向量。 然而，仅靠这一点还不够，未能达成存在唯一的正向量$\\pi^T$ ,且方程能很快地收敛到$\\pi^T$。要做到这一点，布林和佩奇做了另外一个调整，所谓的素性调整。有了这个调整，所得的矩阵就是随机的并且是素的。一个素矩阵是不可约的并且非周期的，因此，链的稳态向量存在且唯一。 转换为素矩阵是基于这样一个论述的。上网者有时会在浏览器URL地址栏输入一个新的目的地，为了对这个行为进行数学建模，布林和佩奇创造了一个新的矩阵G如下： $$ G=\\alpha S + (1-\\alpha)1/n ee^T$$ 式中，a是一个0到1的标量(在之后的章节会反复提到a在谷歌中取0.85)，表示随机上网者根据万维网的超链接结构进行浏览的时间百分比。G成为谷歌矩阵，$E=1/n ee^T$可称为跳转矩阵，是均匀分布的，即在进行跳转时是公平跳转。 在书中，分为6点简单概括G带来的后果： G是随机的。 G是不可约的。 G是非周期的，Gii&gt;0导致。 G是素的。 G是完全稠密的。从计算的角度，这是很糟糕的。但G可以写为非常稀疏的超链接矩阵H的秩一修正。 \\begin{align}G &amp;=\\alpha S + (1-\\alpha)1/n ee^T \\&amp; =\\alpha (H + a(1/n e^T)) + (1-\\alpha)1/n ee^T \\&amp; =\\alpha H+[\\alpha(a)+(1-\\alpha)e]1/ne^T\\end{align} 因此，谷歌修正后的PageRank法为$$\\pi^{（k+1）T}=\\pi^{（k）T}G$$ 2.4 PageRank向量的计算将求解$\\pi^T$转换为如下特征向量问题$$\\pi^T=\\pi^TG \\ \\pi^Te=1$$将求解$\\pi^T$转换为求解如下的性齐次系统：$$\\pi^T(E-G)=0^T \\ \\pi^Te=1$$第一个系统为找到对应于主特征值$\\lambda_1=1$的G的归一化主左特征向量。在第二个系统中，目标则是找到E-G的归一化左零向量。两个系统均服从归一化方程$\\pi^Te=1$，该方程确保$\\pi^T$是一个概率向量。 2.4.1 幂法计算PageRank矩阵G的特点使得幂法成为了显而易见的最佳选择。幂法是求取矩阵主特征值和特征向量的最古老同时也是最简单的迭代方法之一。缺点的计算速度缓慢。\\begin{align} \\pi^{（k+1）T} &amp; =\\pi^{（k）T}G \\ &amp; =\\alpha S + (1-\\alpha)1/n \\pi^{（k）T} ee^T \\ &amp; =\\alpha (H + a(1/n e^T)) + (1-\\alpha)1/n ee^T \\ &amp; =\\alpha \\pi^{（k）T} H+[\\alpha \\pi^{（k）T} a+1-\\alpha]e^T/n \\end{align}式中，向量-矩阵乘法$\\alpha \\pi^{（k）T} H$执行于H这个极端稀疏的矩阵之上，而S和G从来就没有被实际生成或存储过，每个向量-矩阵乘法的复杂度为O(n)，因为H的每一行有差不多10个非零元素，这就是使用幂法的主要原因。这里还有几个优点： 幂法中矩阵系数仅通过向量-矩阵乘法运算的程序来进行访问，而没有实际对矩阵加以操作。 幂法存储方面，除了稀疏矩阵H和悬挂结点向量a之外，就只需要保存\\alpha \\pi^{（k）T}$了，这个向量是完全稠密的。 迭代次数仅需50~100次。 为什么仅需50次呢，解释一下，在马尔科夫链理论中给出了答案，一般而言，应用于某个矩阵的幂法的渐进收敛速率依赖于绝对值最大的两个特征值$\\lambda_1$和$\\lambda2$的比值。确切来说，渐进收敛速率就是$ |\\lambda/\\lambda_1|^k \\to 0 $的速率。 对于G这样的随机矩阵，$\\lambda_1=1$，由于G也是素的，因此$|\\lambda_2|&lt;1$。 通过一些理论的支撑，有以下关系：$$\\lambda_k=\\alpha \\mu_k$$其中$\\mu_k&lt;=1$ 在万维网结构中，$\\mu_k$约等于1，因此，$|\\mu_2(G)| \\approx \\alpha$。 因此，渐进收敛速率在50次的情况为：$$\\alpha ^{50}=0.85^{50} \\approx 0.000296$$即在第50次迭代时，可以期望近似PageRank向量的精度大约为小数点后2~3位。这个精度已经足够了，当然要真正区分需要小数点后10位精度，但是当PageRank评分和内容评分结合起来之后，高精度就变得不那么重要了。","categories":[{"name":"Search Engine","slug":"Search-Engine","permalink":"http://enjoyhot.github.io/categories/Search-Engine/"}],"tags":[{"name":"Search Engine","slug":"Search-Engine","permalink":"http://enjoyhot.github.io/tags/Search-Engine/"},{"name":"PageRank","slug":"PageRank","permalink":"http://enjoyhot.github.io/tags/PageRank/"}]},{"title":"网络搜索引擎简介","slug":"网络搜索引擎简介","date":"2015-04-21T05:23:50.000Z","updated":"2017-04-16T14:48:30.690Z","comments":true,"path":"2015/04/21/网络搜索引擎简介/","link":"","permalink":"http://enjoyhot.github.io/2015/04/21/网络搜索引擎简介/","excerpt":"一、传统信息检索回顾传统信息检索文档集的搜索有三种基本的计算机辅助技术：布尔模型、向量空间模型和概率模型。这些模型发展与20世纪60年代，直到2000年6月，便存在3500种不同的搜索引擎技术，并且大多数搜索引擎仍然依赖于以上三种基本模型的一种或数种。下图摘自《搜索引擎-原理技术与系统》，显示了搜索的主干流程。","text":"一、传统信息检索回顾传统信息检索文档集的搜索有三种基本的计算机辅助技术：布尔模型、向量空间模型和概率模型。这些模型发展与20世纪60年代，直到2000年6月，便存在3500种不同的搜索引擎技术，并且大多数搜索引擎仍然依赖于以上三种基本模型的一种或数种。下图摘自《搜索引擎-原理技术与系统》，显示了搜索的主干流程。 ##1.1 布尔搜索引擎信息检索中最早而且最简单的检索方法之一。布尔逻辑检索也称作布尔逻辑搜索，严格意义上的布尔检索法是指利用布尔逻辑运算符连接各个检索词，然后由计算机进行相应逻辑运算，以找出所需信息的方法。它使用面最广、使用频率最高。布尔逻辑运算符的作用是把检索词连接起来，构成一个逻辑检索式。 ##1.2向量空间模型搜索引擎向量空间模型将文本数据变换为数值向量和矩阵，然后使用矩阵分析方法来发现文档集中的关键特征和联系。某些高级向量空间模型，如LSI（Latent Semantic Indexing,隐性语义索引）等能访问文档集中隐含的语义结构，如搜索car,能返回automobile相关文档。该模型还有另外两个优点是相关性评分和相关性反馈。缺点是计算开销大，查询时必须计算每个文档和查询之间的距离度量，因而也伴随着另一个缺点——向量空间模型无法很好地扩展。 1.3概率模型搜索引擎用户给定一个查询请求，概率检索模型根据文档与用户请求的相关性排序文档，给出结果，举个简单的例子，对于信息检索的文档，最可能跟在information后面的词是retrieval，但独立性假设却认为任何词都会以等概率出现在information后面。重点在于相关性的定义与衡量。概率模型的构建和编程有可能十分困难，它们的复杂度上升得很快。 ##1.4元搜索引擎传统搜索引擎其实还有第四种模型，即元搜素引擎。它将以上三种经典模型合为一体。 ##1.5搜索引擎的比较两种最为常用的评价不同搜索方法的评价指标是查准率和查全率。查准率是指检索所得相关文献的数量占总的检索所得文献数量的比例；查全率是指检索所得相关文献数量占总的相关文献数量的比例。查准率和查全率越高，搜索引擎就越好。 #二、网络搜索引擎 爬虫模块：蜘蛛 页面仓库：蜘蛛满载页面而回，它们暂时以完整页面的形式存放在页面仓库中，而在被送到索引建立模块之前，新的页面将一直留在仓库中。 索引建立模块：取出每个新的未压缩页面，并从中仅抽取出最为重要的描述，以生成该页面在不同索引中的一个压缩描述。 索引：分为内容索引和特殊用途索引（如图像索引和PDF索引） 查询模块：将用户的自然语言查询转化为搜索系统可以理解的语言，然后查询不同的索引以便回答查询。 排名模块：接收相关页面集，并根据某个判断依据对其进行排名。区分能力的排名是结合两个分数得到的，它们分别是内容评分和欢迎度评分，共同确定了相关页面的总评分，并按照总评分的顺序将页面集呈现给用户。 #三、网络爬行、索引建立和查询处理 ##3.1 网络爬行特点：1.爬行是一个永不停歇的过程2.蜘蛛访问网页时，需做到有礼貌的访问，即对网站的影响降到最小，不然可能会被“惩处”3.多个蜘蛛协调合作，制定最佳爬行策略，节省时间和精力，尽可能提高效率 ##3.2 内容索引程序将分析页面内容并抽取有价值的信息，从而仅将页面中最为关键的核心部分传给适当的索引。有价值的信息存在于标题、描述和锚文本中，此外还有粗体显示的项、大字体显示的项和超链接等。建立索引后形成倒排文件，形如：什么是倒排文件？如下例子（其中001~004对应为文档编号）：001 xxx142 张三 男 18 元培002 xxx205 李四 女 17 哲学003 xxx187 王五 男 19 生物004 xxx325 赵六 女 18 元培而我们利用倒排文件来实现上述非关键码的查询，就能大大提高速度。对于前面的情况设计倒排表如下：男 001，003女 002，004 1617 00218 001，00419 00320 元培 001，004生物 003哲学 002 ##3.3查询处理查询处理的结果，将以文档的相关评分返回，举个例子。在文档集中查询项a和项b的组合词ab，返回的结果有：项a : 3[1,1,27],94[1,0,7],673[0,0,3]项b : 3[1,1,10,94[0,0,5] ,673[1,1,14]如94[1,0,7]中，1表示的是项a在页面94的标题中出现了，0表示项a在页面的描述标签未出现，7表示项a在页面94中出现了7次。因此，内容得分可以这样来计算：内容得分（页面3）= （1+1+27）x (1+1+10)=348内容得分(页面94)=（1+0+7）x (0+0+5)=40内容得分(页面673)=（0+0+3）x (1+1+14)=48 有多种方案可以利用许多其它的因子来构成内容评分，这里只是随便举了一种。 内容评分和欢迎度评分决定了一个网页的最终评分，由于本书的重点在与欢迎度评分，因为在之后的介绍中将不多涉及内容评分。","categories":[{"name":"Search Engine","slug":"Search-Engine","permalink":"http://enjoyhot.github.io/categories/Search-Engine/"}],"tags":[{"name":"Search Engine","slug":"Search-Engine","permalink":"http://enjoyhot.github.io/tags/Search-Engine/"},{"name":"PageRank","slug":"PageRank","permalink":"http://enjoyhot.github.io/tags/PageRank/"}]},{"title":"export CSDN blog to Markdown","slug":"CSDN2Markdown","date":"2015-03-29T08:40:00.000Z","updated":"2017-04-16T14:48:30.622Z","comments":true,"path":"2015/03/29/CSDN2Markdown/","link":"","permalink":"http://enjoyhot.github.io/2015/03/29/CSDN2Markdown/","excerpt":"#一、综述 最开始的博客用的是oschina, 自我感觉小清新，支持比较多风格的编辑器，容易编辑程度比csdn的要好。后来因为csdn人气的关系，又转到csdn，不过csdn编辑确实比较虐心，再后来就在服务器上搭一个WordPress的博客，不过用起来不是很爽。几个月前支持Markdown编辑，这无疑给用github pages的用户一个福音，因为可以直接上传到自己的github博客上。","text":"#一、综述 最开始的博客用的是oschina, 自我感觉小清新，支持比较多风格的编辑器，容易编辑程度比csdn的要好。后来因为csdn人气的关系，又转到csdn，不过csdn编辑确实比较虐心，再后来就在服务器上搭一个WordPress的博客，不过用起来不是很爽。几个月前支持Markdown编辑，这无疑给用github pages的用户一个福音，因为可以直接上传到自己的github博客上。用github pages生成静态的页面，目前比较流行有Jekyll,Octopress,Hexo等，其他的我没怎么调查，jekyll是github推荐的，上面有他们给的官方框架、主题，支持网页书写Markdown文章。后两者需要编译后再上传，github上呈现出来的是html文件。总体而言，后两者的界面更好，用起来更加舒适。我用的是Hexo，命令简洁，不过要注意备份整个工程，不然后果不堪设想。 #二、迁移CSDN原文档到github Pages ###1、前文虽然目前CSDN支持markdown，但以前的文章都是用xeditor编辑器写的，不能导出，所以调研了方法。总体而言有两种方法，但好像没多少人采用，因为格式可能不好。1.爬取页面，导出html，然后在放在hexo中，目录为/source/_post/，直接放html文件，然后设置layout：false，那么hexo会忽略对html的编译，在浏览时直接超链接到html文件2.将html文件再用程序转换为markdown3.直接用代码爬取页面然后生成markdown文件 第1种方法可能会遇到html文件中格式不支持的情况，没得到解决；第2种方法发现在线转换效果也不好，就寻求代码解决，github上有一段程序，作者说可行，不过我环境没搭成功，不懂php，更何况要装curl（这个之前做android时NDK开发时也很难配置），后来用第3种方法就直接写python爬虫程序，参考github的一段程序，不过程序有些问题，也有些不符合如今CSDN的布局，所以我大改了一下，转为markdown的那一部分程序脉络是差不多的，这个也是最关键的部分，直接影响到markdown的显示，不过我也做得不太好。 ###2、程序所需安装库：BeautifulSoup根据版本不同可能要改动相应的代码，一般不用改。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253#! /usr/bin/env python#coding=utf-8import urllib2from bs4 import BeautifulSoupimport loggingimport reimport threadingimport tracebackimport timeimport datetimeimport sysreload(sys)sys.setdefaultencoding('gb18030')# global variableblog = \"http://blog.csdn.net\"url = \"http://blog.csdn.net/gugugujiawei?viewmode=contents\"outputDir = 'F:\\\\linux\\\\Share\\\\github\\\\article\\\\'gRetryCount = 4def decodeHtmlSpecialCharacter(htmlStr): specChars = &#123;\"&amp;ensp;\" : \"\", \\ \"&amp;emsp;\" : \"\", \\ \"&amp;nbsp;\" : \"\", \\ \"&amp;lt;\" : \"&lt;\", \\ \"&amp;gt\" : \"&gt;\", \\ \"&amp;amp;\" : \"&amp;\", \\ \"&amp;quot;\" : \"\\\"\", \\ \"&amp;copy;\" : \"®\", \\ \"&amp;times;\" : \"×\", \\ \"&amp;divide;\" : \"÷\", \\ &#125; for key in specChars.keys(): htmlStr = htmlStr.replace(key, specChars[key]) return htmlStrdef repalceInvalidCharInFilename(filename): specChars = &#123;\"\\\\\" : \"\", \\ \"/\" : \"\", \\ \":\" : \"\", \\ \"*\" : \"\", \\ \"?\" : \"\", \\ \"\\\"\" : \"\", \\ \"&lt;\" : \"小于\", \\ \"&gt;\" : \"大于\", \\ \"|\" : \" and \", \\ \"&amp;\" :\" or \", \\ &#125; for key in specChars.keys(): filename = filename.replace(key, specChars[key]) return filenamedef getPageUrlList(url): global blog #获取所有的页面的 url user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' header = &#123; 'User-Agent' : user_agent &#125; request = urllib2.Request(url, None, header) response = urllib2.urlopen(request) data = response.read() #print data soup = BeautifulSoup(data) pageListDocs = soup.find_all(id=\"article_list\") # artclie----&#123;url:title&#125; articleUrlTitle = &#123;&#125; #print len(pageListDocs) for pageList in pageListDocs: h1List = pageList.find_all('h1') for articleList in h1List: hrefDocs = articleList.find_all(\"a\") if len(hrefDocs) &gt; 0: articleHrefDoc = hrefDocs[0] #print \"hello\",articleHrefDoc articleUrl = blog + articleHrefDoc[\"href\"] articleTitle = articleHrefDoc.text articleUrlTitle[articleUrl] = articleTitle print 'the count of articles is',len(articleUrlTitle) ''' for s in articleUrlTitle: print s,'--',articleUrlTitle[s] ''' return articleUrlTitledef download(url, title): # 下载文章，并保存为 markdown 格式 logging.info(\" &gt;&gt; download: \" + url) print 'downloading the article',title data = None title = '\"' + title + '\"' categories = \"\" content = \"\" #postDate = datetime.datetime.now() global gRetryCount count = 0 while True: if count &gt;= gRetryCount: break count = count + 1 try: time.sleep(2.0) #访问太快会不响应 user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' header = &#123; 'User-Agent' : user_agent &#125; request = urllib2.Request(url, None, header) response = urllib2.urlopen(request) data = response.read() break except Exception,e: exstr = traceback.format_exc() logging.info(\" &gt;&gt; failed to download \" + url + \", retry: \" + str(count) + \", error:\" + exstr) pass if data == None: logging.info(\" &gt;&gt; failed to download \" + url) return #print data soup = BeautifulSoup(data) #date=link_postdate manageDocs = soup.find_all(\"div\", \"article_manage\") for managerDoc in manageDocs: categoryDoc = managerDoc.find_all(\"span\", \"link_categories\") if len(categoryDoc) &gt; 0: categories = categoryDoc[0].a.get_text().encode('UTF-8').strip() categories = categories.decode('utf-8').encode('gb2312') postDateDoc = managerDoc.find_all(\"span\", \"link_postdate\") if len(postDateDoc) &gt; 0: postDateStr = postDateDoc[0].string.encode('UTF-8').strip() postDate = datetime.datetime.strptime(postDateStr, '%Y-%m-%d %H:%M') print 'date',postDate contentDocs = soup.find_all(id=\"article_content\") for contentDoc in contentDocs: htmlContent = contentDoc.prettify().encode('UTF-8') #print htmlContent #file = open('F:\\\\linux\\\\Share\\\\github\\\\out2.txt','a+') #file.write(htmlContent) content = htmlContent2String(htmlContent) exportToMarkdown(outputDir, postDate, categories, title, content)# htmlContent2String 是整个程序的关键，用于将html转换为markdown格式def htmlContent2String(contentStr): # 因为格式中可能会有点乱，换行符乱入，所以用[\\s\\S]匹配任何字符，包括换行符，注意其中的？是为了去除贪婪匹配 # &lt;img src=\"http://img.blog.csdn.net/20150118194525562\" align=\"middle\" width=\"400 height=\"300\" alt=\"\"&gt; # 图片链接 patternImg = re.compile(r'(&lt;img[\\s\\S]+?src=\")([\\s\\S]+?)(\"[\\s\\S]+?&gt;)') # &lt;a target=\"_blank\" href=\"http://blog.csdn.net/gugugujiawei/article/details/42558411\"&gt;博文&lt;/a&gt; # 文字链接 patternHref = re.compile(r'(&lt;a[\\s\\S]+?href=\")([\\s\\S]*?)(\"[\\s\\S]*?&gt;)([\\s\\S]+?)(&lt;/a&gt;)') # 去除html各种标签，这里的？则是指匹配0次或1次 patternRemoveHtml = re.compile(r'&lt;/?[^&gt;]+&gt;') resultContent = patternImg.sub(r'![image_mark](\\2)', contentStr) resultContent = patternHref.sub(r'[\\4](\\2)', resultContent) resultContent = re.sub(patternRemoveHtml, r'', resultContent) resultContent = decodeHtmlSpecialCharacter(resultContent) #file = open('F:\\\\linux\\\\Share\\\\github\\\\out3.txt','a+') #file.write(resultContent) return resultContentdef exportToMarkdown(exportDir, postdate, categories, title, content): titleDate = postdate.strftime('%Y-%m') contentDate = postdate.strftime('%Y-%m-%d %H:%M:%S %z') filename = title filename = repalceInvalidCharInFilename(filename) filepath = exportDir + filename + '.txt' #newFile = open(unicode(filepath, \"utf8\"), 'w') newFile = open(filepath,'a+') # 根据自己需要选择去留注释,这里categores和tag用了一样的 # newFile.write('---' + '\\n') # newFile.write('layout: post' + '\\n') newFile.write('title: ' + title + '\\n') newFile.write('date: ' + contentDate + '\\n') # newFile.write('comments: true' + '\\n') newFile.write('categories: [' + categories + ']' + '\\n') newFile.write('tags: [' + categories + ']' + '\\n') #newFile.write('description:' + title + '\\n') # newFile.write('keywords: ' + categories + '\\n') newFile.write('---' + '\\n\\n') content = content.decode('utf-8').encode('gb18030') #print content newFile.write(content) newFile.write('\\n') newFile.close()if __name__ == \"__main__\": global url articleUrlTitle = getPageUrlList(url) ''' for s in articleUrlTitle: print s,'--',articleUrlTitle[s] ''' #multithread download threads = [] for url in articleUrlTitle: patternTitle = re.compile('\\r\\n *(.+) *\\r\\n') title = patternTitle.sub(r'\\1',articleUrlTitle[url]) # print 'title',title t = threading.Thread(target = download,args = (url,title)) t.start() threads.append(t) for i in threads: i.join() print \"success\" 注意一下，我是直接在windows下运行生成的txt文件，所以文件时dos文件，当在linux下编辑的话会出现各种因格式问题导致的奇怪的问题，这需要自己改一下编码。","categories":[{"name":"Markdown","slug":"Markdown","permalink":"http://enjoyhot.github.io/categories/Markdown/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"http://enjoyhot.github.io/tags/Markdown/"},{"name":"CSDN","slug":"CSDN","permalink":"http://enjoyhot.github.io/tags/CSDN/"}]},{"title":"Album","slug":"Album","date":"2015-03-26T15:45:21.000Z","updated":"2017-04-16T14:48:30.610Z","comments":true,"path":"2015/03/26/Album/","link":"","permalink":"http://enjoyhot.github.io/2015/03/26/Album/","excerpt":"","text":"You just say something I don’t know.","categories":[{"name":"Photo","slug":"Photo","permalink":"http://enjoyhot.github.io/categories/Photo/"}],"tags":[{"name":"Photo","slug":"Photo","permalink":"http://enjoyhot.github.io/tags/Photo/"}]},{"title":"HPC集群平台搭建概念","slug":"HPC-cluster","date":"2015-03-24T12:23:50.000Z","updated":"2017-04-16T14:48:30.638Z","comments":true,"path":"2015/03/24/HPC-cluster/","link":"","permalink":"http://enjoyhot.github.io/2015/03/24/HPC-cluster/","excerpt":"在本文之前，我建议先看完这篇文章：Linux高性能计算集群 – Beowulf集群 #一、搭建集群中可能会遇到的问题 1.集群设计/布局的一大难点就是网络，各家有各家的策略，一般围绕Beowulf变型。2.软件的部署缺乏实操调试，也不能断定这样部署是否成功以及是否优越。3.操作系统的选型以及安装系统、所需软件（包括集群管理工具、驱动软件、能耗监控软件等）的整个流程的手动/自动化部署。","text":"在本文之前，我建议先看完这篇文章：Linux高性能计算集群 – Beowulf集群 #一、搭建集群中可能会遇到的问题 1.集群设计/布局的一大难点就是网络，各家有各家的策略，一般围绕Beowulf变型。2.软件的部署缺乏实操调试，也不能断定这样部署是否成功以及是否优越。3.操作系统的选型以及安装系统、所需软件（包括集群管理工具、驱动软件、能耗监控软件等）的整个流程的手动/自动化部署。4.每个提供硬件的对应公司几乎都推出他们自己的集群管理软件，浪潮好像没有，在HPC方面浪潮好像没有相应的开发社区，DELL和IBM这些大公司都有。5.能耗的衡量，具体的优化方向和策略。 #二、HPC概览 ##①总体构成 Outside Network: 外部网络 Master Node: 主节点 Compute Nodes: 计算节点 Storage: 存储器 Computational Network: 计算网络 Management Network: 管理网络 ##②大多数 HPCC 系统配有两个网络 基于TCP的管理网络 计算网络，可以是基于 TCP 或其它协议的，通常是 InfiniBand 或 Myrinet 10G 之 类的高速网络##③拓扑图网上找的普遍认可的： 这个是自己根据实际情况“臆想”的： ##④所需软件组件（按安装顺序）： ###1、对于安装系统： 集群中的每个节点（HPCC 节点文章链接）、主节点、登录节点和计算节点都需要有操作系统。操作系统可以安装在节点的硬盘驱动器上，甚至可以安装在ramdisk 上，这有时被称为“无盘”或“无状态”节点。一般说来，主节点创建所谓的“映像”，然后将其发送到计算节点上进行安装（硬盘驱动器或 ramdisk 上）。 系统装在内存会更快，但断电后要采取将系统拷贝到硬盘的方法重新存储起来，比较麻烦。 几种安装工具的特点： 但是，比较有名有Rocks（比xCat安装简单）、xCat。 xCAT:a、支持自己系统独立，可以自己选择基于RHEL的最新版本系统；b、命令行安装，需要人工编辑配置文件，安装一台之后再利用脚本进行网络安装裸机节点；c、xCat在安装起来像一个个独立软件，如其他并行计算需要的相关软件可以通过xCat命令安装； Rocks:a、ROCKS 基于 Red Hat 发行版，这对于大多数人是合适的，但是对于使用 SUSE 或者希望使用在 RH 6.2 发行版上创建的映像的人就不合适了，里面的软件可以选择性安装。另外，ROCKS 不是克隆解决方案；b、之前需刻录CD，GUI界面部署，之后人工进行节点命名和节点ip的配置，安装时一台一台进行；c、Rocks则像一个集成的包，包含工具与软件；Rocks = CentOS + RollsRoll软件包包括： base：基本的Rocks Cluster管理工具 SGE：SUN Grid Engine，集群下作业调度 HPC：为集群上的并行应用提供运行环境（MPI，PVM） area51：分析集群上文件和内核的完整性 Ganglia：集群监控软件（主流hpc cluster基本都有这个） Bio：集群的生物信息学工具 一般而言，这两种方法只有适用性与便捷性的差异。有一个视频课程 http://edu.51cto.com/course/course_id-507.html 对于操作系统的选择，调研了很多，一般用Red Hat，Centos居多，一方面是社区活跃，一方面是源码开发，资源兼容较多。（MIC开发上基本采用Red Hat，Centos，SUSE，windows也开始有了） ###2、安装驱动和开发工具。包括IB驱动、编译器、编辑器、调试器、库等。 并行计算开发环境： 安装英特尔众核平台软件堆栈(MPSS)，包含各种驱动。https://software.intel.com/en-us/articles/intel-manycore-platform-software-stack-mpss#lx34rel 假如用intel的编译器的话（我们应该是用intel的）：下载安装intel parallel studio xe 2015，包含有性能分析工具、编译器、高性能库、并行编程工具等，对Xeon Phi进行相应的优化和升级。 Cilk Plus，OpenMP，TBB多线程编程技术和向量化技术在Xeon &amp; Xeon Phi上都已实现了，软件开发者无需额外的移植成本。（试用30天）https://software.intel.com/en-us/intel-parallel-studio-xe/try-buy xeon phi与第三方工具是否available，参考Intel FAQ：https://software.intel.com/en-us/articles/intel-and-third-party-tools-and-libraries-available-with-support-for-intelr-xeon-phitm ###3、配置节点信息存储系统安装NFS、PVFS、Lustre 、Luster、GPFS、SNFS等，一般大型的HPC集群用Lustre能获得更好的性能，但不太适合小集群，小集群可以考虑用NFS和PVFS，但NFS并不是面向并行计算的，推荐用PVFS好一点。 关于Lustre： 一个Lustre文件系统主要包括以下四个组件：管理服务器Management Server(mgs), 元数据服务器Meta Data Target(mdt), 对象存储服务器Object Storge Target(ost) ,客户端Lustre clients(lc)。 它主要包括三个部分：元数据服务器MDS (Metadata Server)、对象存储服务器OSS (Object Storage Server)和客户端Client。 正常的启动顺序是：OST -&gt; MDS -&gt; CLIENT 甲骨文产品管理负责人Bob Thome表示：“云文件系统并不是甲骨文首个基于集群文件系统的产品。甲骨文管理着Lustre项目，Lustre更适合于拥有上千台服务器的大规模HPC（高性能计算）部署。云文件系统则更适合于25个节点数左右的小规模部署，尽管Lustre已经通过了多达100个节点的测试。Lustre也可以实现很多相同的功能，但使用门槛较高，安装和配置较为繁琐，并不适合于小规模部署。” 关于Lustre的博文：http://www.cnblogs.com/jpa2/category/384788.html PVFS存在以下不足：1）单一管理节点。上面说到过PVFS中只有一个管理节点来管理元数据，当集群系统达到一定的规模之后，管理节点将可能出现过度繁忙的情况，这时管理节点将成为系统瓶颈。2）对数据的存储缺乏容错机制。当某一I/O节点无法工作时，上面的数据将出现不可用的情况。3）静态配置。对PVFS的配置只能在启动前进行，一旦系统运行则不可再更改原先的配置。 ###4、集群管理工具（考虑是否集成了一些组件） 集群管理工具 (CMT)，它的职能是管理集群。它有多个功能，有的是可选功能。而必须具备的功能包括: 维护计算节点清单（即集群中包括的节点）。只需通过简单如 /etc/hosts 的，就能复制或通过本地 DNS 发送至每个计算节点 创建、管理映像或安装在计算节点上的数据包集 发送映像或数据包到计算节点（一般通过 PXE ） 执行对计算节点的基本监控（例如，节点工作情况？什么节点发生起落？） 计算节点电源控制（不是硬性要求，但是强烈推荐）。即远程开启/关闭节点，此功能可以通过各种方法实现，有的方法需要使用增加其他硬件。 虽然这个功能清单对于有集群经验的人来说显得太简短，但清单所载功能是 CMT真正的核心。具备其他功能也不错，但对集群来说并不是必不可少的。 CMT 包括 Platform OCS、Clustercorp ROCKS+、Microsoft Windows CCS 和平台管理器 (Platform Manager) 、Mon等。 ###5、可选组件： 集群所需的工具并不多，但有了这些就能实现集群的基本运行。不过，它只能满足 1 个用户或 2 至 3 个用户的需要，此外，要实现全面控制和掌握集群的运行情况。要安装一些可选组件，从技术上虽然是可选项，但是没有这些工具，集群就不具备生产能力。有一些组件可以添加到 CMT 或 CMI 上层。一个有数年管理多个集群经验的人说的，强烈建议您郑重考虑使用以下附加组件： 更加广泛的监控工具，包括集群状态图形视图，例如Ganglia（链接- http://ganglia.info/）、Cacti（链接 - http://www.cacti.net/）和 Nagios（链接 -http://www.nagios.org/) 报告工具，允许您创建关于集群运行情况的报告 用户帐户管理工具（允许您在整个集群上创建用户帐号、允许用户设置密码，然后将其传播到集群的所有节点上，允许无密码登录节点，这对于运行 MPI 应用程序是必需的） 另一个理论上可选，但值得强烈推荐的组件——任务调度器（也被称为资源管理器）任务调度器是一个允许用户提交执行任务、但不参与任务运行的排队系统。任务调度器把提交的任务排成队列，等到资源（即节点）可用时，就开始运行。任务调度器包括：Platform LSF、PBS-Pro 和 MOAB 等。 ###6、测试略 #三、参考链接：http://www.ibm.com/developerworks/cn/linux/l-cluster1/http://zh.community.dell.com/techcenter/w/techcenter_wiki/50http://www.hpcblog.com.cn/ 附上一张高清MIC图：","categories":[{"name":"HPC","slug":"HPC","permalink":"http://enjoyhot.github.io/categories/HPC/"}],"tags":[{"name":"HPC","slug":"HPC","permalink":"http://enjoyhot.github.io/tags/HPC/"}]},{"title":"HPC性能测试","slug":"HPC-benchmark","date":"2015-03-24T06:43:00.000Z","updated":"2017-04-16T14:48:30.638Z","comments":true,"path":"2015/03/24/HPC-benchmark/","link":"","permalink":"http://enjoyhot.github.io/2015/03/24/HPC-benchmark/","excerpt":"#一、介绍 说到高性能计算，一般都是利用搭建集群配合加速卡做并行计算实现。但用相同的硬件实现更快的方法，那就是做测试了。比较有名的就是Linpack测试了，Linpack现在在国际上已经成为最流行的用于测试高性能计算机系统浮点性能的benchmark，在高性能领域，就是利用其中的HPL测试进行测试。下面摘一段百科上的解释:","text":"#一、介绍 说到高性能计算，一般都是利用搭建集群配合加速卡做并行计算实现。但用相同的硬件实现更快的方法，那就是做测试了。比较有名的就是Linpack测试了，Linpack现在在国际上已经成为最流行的用于测试高性能计算机系统浮点性能的benchmark，在高性能领域，就是利用其中的HPL测试进行测试。下面摘一段百科上的解释: Linpack现在在国际上已经成为最流行的用于测试高性能计算机系统浮点性能的benchmark。通过利用高性能计算机，用高斯消元法求解N元一次稠密线性代数方程组的测试，评价高性能计算机的浮点性能。 Linpack测试包括三类，Linpack100、Linpack1000和HPL。Linpack100求解规模为100阶的稠密线性代数方程组，它只允许采用编译优化选项进行优化，不得更改代码，甚至代码中的注释也不得修改。Linpack1000要求求解规模为1000阶的线性代数方程组，达到指定的精度要求，可以在不改变计算量的前提下做算法和代码上做优化。HPL即High Performance Linpack，也叫高度并行计算基准测试，它对数组大小N没有限制，求解问题的规模可以改变，除基本算法（计算量）不可改变外，可以采用其它任何优化方法。前两种测试运行规模较小，已不是很适合现代计算机的发展，因此现在使用较多的测试标准为HPL，而且阶次N也是linpack测试必须指明的参数。 HPL是针对现代并行计算机提出的测试方式。用户在不修改任意测试程序的基础上，可以调节问题规模大小N(矩阵大小)、使用到的CPU数目、使用各种优化方法等来执行该测试程序，以获取最佳的性能。HPL采用高斯消元法求解线性方程组。当求解问题规模为N时，浮点运算次数为(2/3 N^3－2N^2)。因此，只要给出问题规模N，测得系统计算时间T，峰值=计算量(2/3 N^3－2N^2)/计算时间T，测试结果以浮点运算每秒（Flops）给出。 计算峰值： 随着产品硬件的不断的升级，整个的计算能力也以数量级的速度提升。衡量计算机性能的一个重要指标就是计算峰值，例如浮点计算峰值，它是指计算机每秒钟能完成的浮点计算最大次数。包括理论浮点峰值和实测浮点峰值： 理论浮点峰值是该计算机理论上能达到的每秒钟能完成浮点计算最大次数，它主要是由CPU的主频决定的，理论浮点峰值=CPU主频×CPU每个时钟周期执行浮点运算的次数×系统中CPU核心数目 实测浮点峰值是指Linpack测试值，也就是说在这台机器上运行Linpack测试程序，通过各种调优方法得到的最优的测试结果。实际上在实际程序运行过程中，几乎不可能达到实测浮点峰值，更不用说达到理论浮点峰值了。这两个值只是作为衡量机器性能的一个指标，用来表明机器处理能力的一个标尺和潜能的度量。 此次参加一个高性能的比赛，虽然做的是平台搭建部分，但对此也有些了解。也了解了一下HPCC测试和NAMD测试，下面主要分这两部分来说一下。 #二、正文 1、HPCC HPCC 全称 HPC Chanllenge Benchmark Benchmark 是指一组用来评估硬件或者软件相关性能的基准测试程序 HPC Chanllenge Benchmark 由一组benchmark组成，共计7个，分别测试了系统7个方面的性能，这7个分别为： HPL stream Random Access PTRANS, Latency/Bandwidth FFT DGEMM stream：测试内存带宽内存看作是内存控制器与CPU之间的桥梁与仓库。内存的容量决定“仓库”的大小，而内存的带宽决定“桥梁”的宽窄Random Access：测试内存刷新的速率（随机存储中电容器需要刷新）PTRANS：通过多处理器结构中两两之间的通信，来衡量在整个网络的通信能力Latency/Bandwidth：测试延时与带宽 Latency：8 byte 数据从一个节点到另一个节点所需时间 Bandwidth：节点间网络通信的带宽 HPL：测试系统在解线性方程组时进行浮点运算的性能 浮点运算：超级计算机应用的场景主要是在科学计算，经常涉及矩阵运算、各种数值模拟等。 超级计算机经常以FLOPS（每秒浮点运算次数）去衡量计算能力 HPCC里有3种benchmark是测试浮点计算性能的 FFT：涉及到双精度一维离散傅里叶变换时，浮点运算的速度DGEMM：在做双精度矩阵乘法时，浮点运算的速度 HPCC的下载地址见：链接 这个软件13年8月后就没更新了，网上查阅了很多资料，发现使用的人好像也不多，对于高性能计算，大多都是HPL的文章和测试。要想使用好这个软件，需要研读其源代码。 其中的安装使用方法可参照网站的FAQ以及软件中的README.html 编译之前，需要下载好相关的库，包括MPI（mpich，mpich2，mvapich，openmpi，platform_mpi，hpmpi……）、MKL/BLAS/GotoBLAS2……、C语言编译器等，前两者集群一般需要自己去找对应的软件，详细配置请自行上网搜一下，和Linpack基本一样。 ###2、NAMD测试 NAMD是一个与分子动力学有关，用来高仿真大型生物分子系统的程序。即生物分子建模程序。 现提供两个工作负载apoa1, f1atpase 它们可以从 http://www.ks.uiuc.edu/Research/namd/utilities/f1atpase.tar.gz和http://www.ks.uiuc.edu/Research/namd/utilities/apoa1.tar.gz下载。 至于性能的话，且看图： 图片来源：链接 NAMD 2.8 在天津超算天河-1A上的测试结果： 表1 每节点不同进程数以及使用不同节点数的NAMD性能（days/ns）比较 结论：为了在天河上达到性能最优，最佳的方案是使用GPU，并且每个节点开6个进程。参照：链接","categories":[{"name":"HPC","slug":"HPC","permalink":"http://enjoyhot.github.io/categories/HPC/"}],"tags":[{"name":"HPC","slug":"HPC","permalink":"http://enjoyhot.github.io/tags/HPC/"},{"name":"benchmark","slug":"benchmark","permalink":"http://enjoyhot.github.io/tags/benchmark/"}]},{"title":"学习感悟","slug":"study-thinking-20150319","date":"2015-03-19T05:42:00.000Z","updated":"2017-04-16T14:48:30.682Z","comments":true,"path":"2015/03/19/study-thinking-20150319/","link":"","permalink":"http://enjoyhot.github.io/2015/03/19/study-thinking-20150319/","excerpt":"社会篇 走过2014，看得很多是互联网如何在颠覆各个行业。以下说的都是感悟，可能有些无序，多多包含。 无论是做科研也好，投入社会工作也罢，在做以互联网技术为核心的技术学习时，都要有一个目标，做出为社会认可的有价值的“产品”，并间接体现为金钱财富。","text":"社会篇 走过2014，看得很多是互联网如何在颠覆各个行业。以下说的都是感悟，可能有些无序，多多包含。 无论是做科研也好，投入社会工作也罢，在做以互联网技术为核心的技术学习时，都要有一个目标，做出为社会认可的有价值的“产品”，并间接体现为金钱财富。 其实，互联网从访问互联网的用户中赚钱，流量即是渠道。曾听过一句话——“有流量就能赚钱”。确实，在各个行业环环相扣的时候，利用“代理”的作用能把流量转化为金钱，也由于这个原因，才使那么多网民孜孜不倦地以合法或不合法的手段游走于因特网各个角落。 然而，怎样最高效率地利用流量，叫要看个人的素养，假如我们将单纯上网的网民比作现实中普通的消费者，那么单纯利用流量赚钱可以比作打工者，而管理某个访问点的可以比作房东，那么房地产，设计师又如何？那需要有强大编程功底的各个阶层的创作者来担任了。从做服务，到做标准，互联网是另一个世界，虚拟而却又像现实生活中那样分工明确，生活着。 在这里，我相信都想往高处走，做创作者。但这里我有个发现，能做到标准的少之又少，多数成功者，走的却是先往高处走，当走到一定规模后，市场足够大，再往低处走，做稳定发展的打算。之后有余力再在高处不断试探，不断阶梯式上升，建造更高的高峰。 14年相比13年又可以称为互联网的爆炸阶段，不管是从从业的数量来看还是从创业的数量来看。一切的发展归根到底都是为了服务人类生活，往更便捷更智能更廉价发展。 因此，才涌现了更方便的产品，更智能的算法，更高速的计算。 #热点篇 更方便的产品，更智能的算法，更高速的计算，既是相对分开，却又是一脉相承，任何一个领域都可以颠覆另一个领域，当然，最终体现在产品上， 因而产品领域是最活跃的，门槛相对低的情况下又与金钱结合得最为紧密。而超算方面一直以来都比较冷门，毕竟门槛高，又与硬件运算最为相关 说到产品，苹果、谷歌、Facebook，产品之王|搜索之王|社交之王。 说到算法，目前人工智能领域的机器学习，深度学习，还有相关的大数据，云计算等。 说到计算，那就是超算，高性能计算，单台配上GPU和MIC可以提升计算能力，或者直接在集群上并行。 这三种内容在以后的博客应该都会多多少少体现。 #学习篇 慢慢啃，从实验到产品，是一个漫长的过程，但所有的收获，都是从test开始的，或许从产品着手不需要多高的理论积累，在这个风口也完全有可能飞得老高，所以才一家家公司冒出来，或者一些资深程序员直接跳出来创业。说实在，最后能将厚积理论勃发在产品的，互联网大公司才做得最好，毕竟资金人力都全了。小公司或者突然强势冒出来，大有盖过大公司风头之势，但大多昙花一现、干不过大公司，最后被收购也不是少例。虽说这样足够混迹互联网圈，但能主导一个潮流，不算是人生的一大乐事吗？","categories":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/categories/Life/"}],"tags":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/tags/Life/"}]},{"title":"【machine learning】KNN算法        ","slug":"machinelearning-KNN","date":"2015-02-25T08:04:00.000Z","updated":"2017-04-16T14:48:30.654Z","comments":true,"path":"2015/02/25/machinelearning-KNN/","link":"","permalink":"http://enjoyhot.github.io/2015/02/25/machinelearning-KNN/","excerpt":"适逢学习机器学习基础知识，就将书中内容读读记记，本博文代码参考书本Machine Learning in Action（《机器学习实战》）。 一、概述kNN算法又称为k近邻分类(k-nearest neighbor classification)算法。 kNN算法则是从训练集中找到和新数据最接近的k条记录，然后根据他们的主要分类来决定新数据的类别。该算法涉及3个主要因素：训练集、距离或相似的衡量、k的大小。","text":"适逢学习机器学习基础知识，就将书中内容读读记记，本博文代码参考书本Machine Learning in Action（《机器学习实战》）。 一、概述kNN算法又称为k近邻分类(k-nearest neighbor classification)算法。 kNN算法则是从训练集中找到和新数据最接近的k条记录，然后根据他们的主要分类来决定新数据的类别。该算法涉及3个主要因素：训练集、距离或相似的衡量、k的大小。 二、算法要点1、指导思想kNN算法的指导思想是“近朱者赤，近墨者黑”，由你的邻居来推断出你的类别。 计算步骤如下：1）算距离：给定测试对象，计算它与训练集中的每个对象的距离2）找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻3）做分类：根据这k个近邻归属的主要类别，来对测试对象分类 2、距离或相似度的衡量什么是合适的距离衡量？距离越近应该意味着这两个点属于一个分类的可能性越大。距离衡量包括欧式距离、夹角余弦等。对于文本分类来说，使用余弦(cosine)来计算相似度就比欧式(Euclidean)距离更合适。 3、类别的判定投票决定：少数服从多数，近邻中哪个类别的点最多就分为该类，属于以频率为标准。加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大（权重为距离平方的倒数），属于以量化为标准。 三、优缺点1、优点简单，易于理解，易于实现，无需估计参数，无需训练适合对稀有事件进行分类（例如当流失率很低时，比如低于0.5%，构造流失预测模型）特别适合于多分类问题(multi-modal,对象具有多个类别标签)，例如根据基因特征来判断其功能分类，kNN比SVM的表现要好 2、缺点懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢可解释性较差，无法给出决策树那样的规则。 四、利用KNN进行手写识别假如存在训练数据，都是二值得灰度图，来源于手写面板的采集图像数据。如下表示数字‘0’，所在文件夹下包括表示0~9的文件，文件夹命名A_B.txt,A表示真实数字，B表示该数字的第B个样本（一般数据越多有有利于接近预测值） 在另一个文件夹中，也存在同样命名的数据文件，用于检验有监督学习下的准确率，我们称为测试数据。在代码中，我们需要三个函数 def classify0(inX, dataSet, labels, k)——用于对输入单个样本inX进行分类，dataSet为训练数据，labels为训练数据的类别，K为近邻范围def img2vector(filename)——将文件filename中的数据规格由32X32转换为1X1024的向量def handwritingClassTest()——利用测试数据进行测试，得出错误率我这里只用了0~9分别20个训练数据而已，提高速度。需要源代码可以到机器学习实战的配套代码中取 http://vdisk.weibo.com/s/uEZesAafcjQgx?sudaref=www.baidu.com 代码中用到了numpy库，numpy库用在数据量大的计算较高效 numpy用法小抄：123456&gt;&gt;&gt; tile([0, 0], (1, 2))array([[0, 0, 0, 0]])&gt;&gt;&gt; tile([0, 0], (2, 1))array([[0, 0], [0, 0]])` 第一个是矩阵A第二个参数是要 只有一个数字时，表示 对 A中元素重复的次数两个参数时（x， y） y表示对A中元素重复的次数， x表示 对前面的操作执行x次。 12345678910111213&gt;&gt;&gt; b= np.arange(12).reshape(3,4)&gt;&gt;&gt; barray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; b.sum(axis=0) # 计算每一列的和，注意理解轴的含义，参考数组的第一篇文章array([12, 15, 18, 21])&gt;&gt;&gt; b.min(axis=1) # 获取每一行的最小值array([0, 4, 8])&gt;&gt;&gt; b.cumsum(axis=1) # 计算每一行的累积和array([[ 0, 1, 3, 6], [ 4, 9, 15, 22], [ 8, 17, 27, 38]]) 需要用到再另外写博客进行补充。KNN.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#! /usr/bin/env python#coding=utf-8from numpy import *import operatorfrom os import listdirdef classify0(inX, dataSet, labels, k): #inX------[x,x,x,x] #dataSet------array([[x,x,x,x],[x,x,x,x]]) #labels------[x,x] #k------n dataSetSize = dataSet.shape[0] diffMat = tile(inX, (dataSetSize,1)) - dataSet sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 sortedDistIndicies = distances.argsort() classCount=&#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 #将字典按value值大小降序排序,结果为二维列表 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0]def img2vector(filename): returnVect = zeros((1,1024)) fr = open(filename,'r') for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int(lineStr[j]) return returnVecttrainFile = 'F:\\\\python\\\\pyproject\\\\ML\\\\codes\\\\machinelearninginaction\\\\Ch02\\\\training20\\\\'testFile = 'F:\\\\python\\\\pyproject\\\\ML\\\\codes\\\\machinelearninginaction\\\\Ch02\\\\testDigits\\\\'def handwritingClassTest(): hwLabels = [] trainingFileList = listdir(trainFile) #load the training set m = len(trainingFileList) trainingMat = zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] #take off .txt classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) path = trainFile + '%s' trainingMat[i,:] = img2vector(path % fileNameStr) testFileList = listdir(testFile) #iterate through the test set errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split('.')[0] #take off .txt classNumStr = int(fileStr.split('_')[0]) path = testFile + '%s' vectorUnderTest = img2vector(path % fileNameStr) classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) print \"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, classNumStr) if (classifierResult != classNumStr): errorCount += 1.0 print \"\\nthe total number of errors is: %d\" % errorCount print \"\\nthe total error rate is: %f\" % (errorCount/float(mTest)) 再在test.py中调用KNN.handwritingClassTest()，则程序开始运行 test.py 12345#! /usr/bin/env python#coding=utf-8import KNNKNN.handwritingClassTest() 可看到错误率10.68%，挺高的，增加训练数据量就应该会减小一些。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://enjoyhot.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://enjoyhot.github.io/tags/Machine-Learning/"},{"name":"Classifier","slug":"Classifier","permalink":"http://enjoyhot.github.io/tags/Classifier/"}]},{"title":"网络爬虫框架scrapy介绍及应用——抓取新浪新闻的标题内容评论        ","slug":"python-scrapy","date":"2015-01-21T07:12:00.000Z","updated":"2017-04-16T14:48:30.674Z","comments":true,"path":"2015/01/21/python-scrapy/","link":"","permalink":"http://enjoyhot.github.io/2015/01/21/python-scrapy/","excerpt":"一、综述 开始这篇博文之前，调研了相关的爬虫方法，简单罗列冰山一角。 综述： http://www.crifan.com/summary_about_flow_process_of_fetch_webpage_simulate_login_website_and_some_notice/","text":"一、综述 开始这篇博文之前，调研了相关的爬虫方法，简单罗列冰山一角。 综述： http://www.crifan.com/summary_about_flow_process_of_fetch_webpage_simulate_login_website_and_some_notice/ 手动编写爬虫，httpclient是常用工具。常见的请求方式有httpget和httppost http://blog.csdn.net/mr_tank_/article/details/17454315 http://blog.csdn.net/chszs/article/details/16854747 http://www.yeetrack.com/?p=779 这个教程很全面。供参考和备查 htmlunit httpclient 对js 的支持比较差，有时候需要使用htmlunit 或者selenium。 http://www.360doc.com/content/13/1229/14/14875906_340995211.shtml http://blog.csdn.net/strawbingo/article/details/5768421 http://www.cnblogs.com/microsoftmvp/p/3716750.html 抽取相关当爬取了html 后，需要去除噪声广告，抽取有用的信息。jsoup 和tika 是非常强大的工具 http://jsoup.org/cookbook/ http://summerbell.iteye.com/blog/565922 github开源爬虫库 https://github.com/CrawlScript/WebCollector https://github.com/zhuoran/crawler4j 开源爬虫框架nutch http://www.cnblogs.com/xuekyo/archive/2013/04/18/3028559.html http://ahei.info/nutch-tutorial.htm http://lc87624.iteye.com/blog/1625677 由于要学习python语言，就关注了python爬虫的方法，scrapy框架是个成熟的开源爬虫框架，因此选择其作为学习内容。Scrapy是一个基于Twisted，纯Python实现的爬虫框架，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容、图片、视频等，非常方便。 二、scrapy框架1、整体架构如下： 绿线是数据流向，首先从初始URL 开始，Scheduler 会将其交给 Downloader 进行下载，下载之后会交给 Spider 进行分析，Spider分析出来的结果有两种：一种是需要进一步抓取的链接， 例如之前分析的“下一页”的链接，这些东西会被传回 Scheduler ；另一种是需要保存的数据，它们则被送到Item Pipeline 那里，那是对数据进行后期处理（详细分析、过滤、存储等）的 地方。另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。参考 博客 2、工程文件介绍假设你已经配置好环境了，进入某个文件夹pythonproject，在命令行中输入scrapy startproject mypro即可在pythonporoject文件夹下找到mypro的工程文件夹，结构如下： ├── mypro│ ├── mypro│ │ ├── init.py│ │ ├── items.py│ │ ├── pipelines.py│ │ ├── settings.py│ │ └── spiders│ │ └── init.py│ └── scrapy.cfg scrapy.cfg: 项目配置文件items.py: 需要提取的数据结构定义文件pipelines.py:管道定义，用来对items里面提取的数据做进一步处理，如保存等settings.py: 爬虫配置文件 Items是将要装载抓取的数据的容器，它工作方式像python里面的字典，但它提供更多的保护，比如对未定义的字段填充以防止拼写错误。它通过创建一个scrapy.item.Item类来声明，定义它的属性为scrpiy.item.Field对象，就像是一个对象关系映射(ORM)，我们通过将需要的item模型化，来控制从dmoz.org获得的站点数据。虽然这次的实现并没有用到items.py和 pipelines.py，但大规模的爬虫还是需要注意一下解耦。举个例子： 12345from scrapy.item import Item, Fieldclass DmozItem(Item): title = Field() link = Field() desc = Field() 在修改初始化代码时，首先需要在pythonproject//mypro//mypro//spiders下新建一个python文件，原则上所有的实现可以在这个文件里完成，当然耦合度就高了。在这个文件中，你需要新 建一个类，这个类需要添加以下属性：1、该类继承于某个spider类，根据自己的需求，有很多可以选，如crawSpider，BaseSpider，Spider，XMLFeedSpider，CSVFeedSpider，SitemapSpider等等2、name：爬虫的识别名，它必须是唯一的，在不同的爬虫中你必须定义不同的名字，例如下文的”yourname”3、start_urls：爬虫开始爬的一个URL列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些URLS开始。其他子URL将会从这些起始URL中继承性生成。4、parse()：爬虫的方法，调用时候传入从每一个URL传回的Response对象作为参数，response将会是parse方法的唯一的一个参数，这个方法负责解析返回的数据、匹配抓取的数据(解析为 item)并跟踪更多的URL。返回前可以巧妙地运用yield方法递归调用网址，此关键词的作用是返回某个对象后继续执行。如果不用该关键字，则直接会在函数中返回。 一般而言，运用scrapy的步骤是这样的：1、在pythonproject//mypro//mypro//spiders下新建一个python文件2、导入该导入的库文件，新建一个类满足以上要求。3、根据继承的类的要求和功能，定义爬取规则。4、在def parse(self, response)函数中对response对象解析，将需要的内容存入item对象并返回，在这里对数据不返回而是进行进一步处理也是可以的，耦合度高。5、PipeLine用来对Spider返回的Item列表进行保存操作，可以写入到文件、或者数据库等。PipeLine只有一个需要实现的方法：process_item。万事具备之后，通过命令行进入pythonproject//mypro文件夹中，敲下命令行开始爬虫scrapy crawl “yourname”scrapy命令罗列几个，要更多请参看doc scrapy startproject xxx 新建一个xxx的project scrapy crawl xxx 开始爬取，必须在project中 scrapy shell url 在scrapy的shell中打开url，非常实用 scrapy runspider &lt;spider_file.py&gt; 可以在没有project的情况下运行爬虫 三、新浪新闻爬虫众所周知，评论一般是隐藏起来的，或者显示部分，需要手动点击加载去获取更多评论。有两种方法可以解决这种方法，一种是利用js动态解析，工作量大，也比较难实现，二是直接定位到其查询数据库的url，直接抽取。下文就是讲第二种方法。新浪页面导航为我们简单分好类了http://news.sina.com.cn/guide/，而且每个类别中都可以找到相应的滚动新闻（url冠以roll），因而没必要用到crawSpider这个类，这个类功能很强大，不仅可以自动去重，还可以定义更多的爬取规则。例如这个链接http://roll.finance.sina.com.cn/finance/zq1/index_1.shtml，通过修改数字可以实现不断爬取对于新闻的url，当然没有这么“好”的url也是可以找到新闻的url。例如：http://sports.sina.com.cn/nba/可以调用的浏览器的开发工具查找对应的js代码，查看数据库的url，之后在查看评论的时候也是这样的方法（点击刷新即可） 访问这个链接可以查看url 因此，访问这个链接的内容，爬取新闻url,访问新闻并爬取标题、内容、评论。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222#! /usr/bin/env python#coding=utf-8from scrapy.selector import Selectorfrom scrapy.http import Requestimport re,osfrom bs4 import BeautifulSoupfrom scrapy.spider import Spiderimport urllib2,thread#处理编码问题import sysreload(sys)sys.setdefaultencoding('gb18030')#flag的作用是保证第一次爬取的时候不进行单个新闻页面内容的爬取flag=1projectpath='F:\\\\Python27\\\\pythonproject\\\\fuck\\\\'def loop(*response): sel = Selector(response[0]) #get title title = sel.xpath('//h1/text()').extract() #get pages pages=sel.xpath('//div[@id=\"artibody\"]//p/text()').extract() #get chanel_id &amp; comment_id s=sel.xpath('//meta[@name=\"comment\"]').extract() #comment_id = channel[index+3:index+15] index2=len(response[0].url) news_id=response[0].url[index2-14:index2-6] comment_id='31-1-'+news_id #评论内容都在这个list中 cmntlist=[] page=1 #含有新闻url,标题,内容,评论的文件 file2=None #该变量的作用是当某新闻下存在非手机用户评论时置为False is_all_tel=True while((page==1) or (cmntlist != [])): tel_count=0 #each page tel_user_count #提取到的评论url url=\"http://comment5.news.sina.com.cn/page/info?version=1&amp;format=js&amp;channel=cj&amp;newsid=\"+str(comment_id)+\"&amp;group=0&amp;compress=1&amp;ie=gbk&amp;oe=gbk&amp;page=\"+str(page)+\"&amp;page_size=100\" url_contain=urllib2.urlopen(url).read() b='=&#123;' after = url_contain[url_contain.index(b)+len(b)-1:] #字符串中的None对应python中的null，不然执行eval时会出错 after=after.replace('null','None') #转换为字典变量text text=eval(after) if 'cmntlist' in text['result']: cmntlist=text['result']['cmntlist'] else: cmntlist=[] if cmntlist != [] and (page==1): filename=str(comment_id)+'.txt' path=projectpath+'stock\\\\' +filename file2=open(path,'a+') news_content=str('') for p in pages: news_content=news_content+p+'\\n' item=\"&lt;url&gt;\"+response[0].url+\"&lt;/url&gt;\"+'\\n\\n'+\"&lt;title&gt;\"+str(title[0])+\"&lt;/title&gt;\\n\\n\"+\"&lt;content&gt;\\n\"+str(news_content)+\"&lt;/content&gt;\\n\\n&lt;comment&gt;\\n\" file2.write(item) if cmntlist != []: content='' for status_dic in cmntlist: if status_dic['uid']!='0': is_all_tel=False #这一句视编码情况而定，在这里去掉decode和encode也行 s=status_dic['content'].decode('UTF-8').encode('GBK') #见另一篇博客“三张图” s=s.replace(\"'\"\"'\",'\"') s=s.replace(\"\\n\",'') s1=\"u'\"+s+\"'\" try: ss=eval(s1) except: try: s1='u\"'+s+'\"' ss=eval(s1) except: return content=content+status_dic['time']+'\\t'+status_dic['uid']+'\\t'+ss+'\\n' #当属于手机用户时 else: tel_count=tel_count+1 #当一个page下不都是手机用户时，这里也可以用is_all_tel进行判断，一种是用开关的方式，一种是统计的方式 #算了不改了 if tel_count!=len(cmntlist): file2.write(content) page=page+1 #while loop end here if file2!=None: #当都是手机用户时，移除文件，否则写入\"&lt;/comment&gt;\"到文件尾 if is_all_tel: file2.close() try: os.remove(file2.name) except WindowsError: pass else: file2.write(\"&lt;/comment&gt;\") file2.close()class DmozSpider(Spider): name = \"stock\" allowed_domains = [\"sina.com.cn\"] #在本程序中，start_urls并不重要，因为并没有解析 start_urls = [ \"http://news.sina.com.cn/\" ] global projectpath if os.path.exists(projectpath+'stock'): pass else: os.mkdir(projectpath+'stock') def parse(self, response): #这个scrapy.selector.Selector是个不错的处理字符串的类，python对编码很严格，它却处理得很好 #在做这个爬虫的时候，碰到很多奇奇怪怪的编码问题，主要是中文，试过很多既有的类，BeautifulSoup处理得也不是很好 sel = Selector(response) global flag if(flag==1): flag=2 page=1 while page&lt;260: url=\"http://roll.finance.sina.com.cn/finance/zq1/index_\" url=url+str(page)+\".shtml\" #伪装为浏览器 user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers = &#123; 'User-Agent' : user_agent &#125; req = urllib2.Request(url, headers=headers) response = urllib2.urlopen(req) url_contain = response.read() #利用BeautifulSoup进行文档解析 soup = BeautifulSoup(url_contain) params = soup.findAll('div',&#123;'class':'listBlk'&#125;) if os.path.exists(projectpath+'stock\\\\'+'link'): pass else: os.mkdir(projectpath+'stock\\\\'+'link') filename='link.txt' path=projectpath+'stock\\\\link\\\\' + filename filelink=open(path,'a+') for params_item in params: persons = params_item.findAll('li') for item in persons: href=item.find('a') mil_link= href.get('href') filelink.write(str(mil_link)+'\\n') #递归调用parse,传入新的爬取url yield Request(mil_link, callback=self.parse) page=page+1 #对单个新闻页面新建线程进行爬取 if flag!=1: if (response.status != 404) and (response.status != 502): thread.start_new_thread(loop,(response,)) 爬取结果： 在爬取的过程中要注意三点：1.爬取不要过于频繁，不然可能会被封ip，可以减小爬取的速度，sleep一下，或者更改设置文件，我的在F:\\Python27\\python\\Lib\\site-packages\\Scrapy-0.24.4-py2.7.egg\\scrapy\\settings\\default_settings.py2.文件夹的文件上限为21845，超过后注意再新建一个文件夹爬取3.线程不能开得太多，不然也可能达到上限，可以考虑用代码现在所开线程的多少或者利用分布式系统","categories":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/tags/Python/"},{"name":"Spider","slug":"Spider","permalink":"http://enjoyhot.github.io/tags/Spider/"},{"name":"Scrapy","slug":"Scrapy","permalink":"http://enjoyhot.github.io/tags/Scrapy/"}]},{"title":"【machine learning】regularization        ","slug":"machinelearning-linear-regularization","date":"2015-01-21T06:57:00.000Z","updated":"2017-04-16T14:48:30.658Z","comments":true,"path":"2015/01/21/machinelearning-linear-regularization/","link":"","permalink":"http://enjoyhot.github.io/2015/01/21/machinelearning-linear-regularization/","excerpt":"一、机器学习范式1、按数据类型划分(带标签与否)","text":"一、机器学习范式1、按数据类型划分(带标签与否) 这是从样本的数据进行划分，现实中大部分属于半监督学习，并且大部分数据是没分类好的。 监督学习： 例子： 分类 e.g. 文本分类 垃圾邮件过滤 搜索结果回归分析 e.g. 房价预测 股价预测序列标注 e.g. 词性标注 输入:“我中了一张彩票” 输出:“我/r 中/v 了/y /一/m /张/q /彩票/n 无监督学习： 例子：聚类 e.g. 热点话题发现 社团发现密度函数估计(probability density estimation ) e.g. pdf估计异常点检测(outlier detection) e.g. one-class SVM, 去噪 半监督学习： 核心思想考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题 例子：分类 e.g. 垃圾邮件过滤，半监督SVM回归分析聚类e.g. GMM 2、按学习过程划分主动学习、转导学习、强化学习 主动学习(Active Learning) 有少量标注的数据以及丰富的未标注数据 ，标注数据的成本很高，学习算法主动提出一些标注请求，将筛选过的数据交给专家进行标注，然后将标注的数据加入到训练集中，再进行训练。 核心问题：怎么样筛选数据才能使得请求标注的次数尽量少而最终的结果又尽量好 与半监督学习的区别：半监督学习算法不需要人工干预，基于自身对未标记数据加以利用，微博的用户推荐用户就是充当专家的角色。问题可形式化为： 转导学习(Transductive Learning)一种通过观察特定的训练样本，进而预测特定的测试样本的方法在不同的测试集上会产生相互不一致的预测 特点： 1.建立一个更适用于问题域的模型，而非一个更通用的模型 2.利用无标注的测试样本的信息发现聚簇，进而更有效地分类 3.模型近似 与半监督学习的区别：半监督学习不知道测试案例是什么，转导学习知道测试案 例是什么 半监督学习本质上是从特殊到一般(train)，一般到特殊(predict)的推理方法 转导学习本质上是直接从特殊到特殊的推理方法，自动修正模型。强化学习(Reinforcement Learning)从环境状态到行为映射的学习，以使系统行为从环境中获得的累积奖赏值最大。该方法不同与监督学习技术那样通过正例、反例来告知采取何种行为，而是通过试错（trial-and-error）的方法来发现最优行为策略 ）的方法来发现最优行为策略 适用情况：适用于序列决策或者控制问题，很难有这么规则的样本。 e.g. 象棋AI程序解决思路:我们设计一个回报函数（reward function），如果learning agent（象棋AI程序）在决定一步后，获得了较好的结果，那么我们给agent一些回报（比如回报函数结果为正），得到较差的结果，那么回报函数为负。如果我们能够对每一步进行评价，得到相应的回报函数，那么就好办了，我们只需要找到一条回报值最大的路径（每步的回报之和最大），就认为是最佳的路径。备注:一个强化学习的比赛 http://ijcai-15.org/index.php/angry-birds-competition 二、正则化1、模型选择： a.若采用多项式拟合 欠拟合(underfit,also high bias)特征集过小，模型过于简单，会导致训练集的误差明显增大的现象。 过拟合(overfit,also high variance)非常多的特征，那么所学的Hypothesis有可能对训练集拟合的非常好,但是对测试集效果很差，即训练误差少，测试集误差大。 b.避免过拟合的方法——约束高阶多项式的系数 以下是不同阶数的多项式相对应的系数 定义损失函数： 绿色曲线为最佳拟合，红色曲线为实际拟合结果。罚项系数选择 1.交叉验证 2.超参数学习，解决最优化问题 2、模型选择策略 a.代价函数(Cost function)b.风险函数或期望风险(risk function)① 定义为损失函数的期望② 理论上模型f(x)关于联合分布P(X, Y)的平均意义下的损失 ① 学习的本质目标是选择期望风险最小的模型，由于联合分布P(X,Y)是未知的，风险函数Rexp(f)不能直接计算。c.经验风险(empirical risk minimizatiion, ERM)①模型f(x)关于训练数据集的平均损失称为经验风险，对期望风险的近似 其实，最大似然估计等价于最小化经验风险。②经验风险最小化(ERM)的策略认为，经验风险最小的模型是最优模型③当样本容量是够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛应用④当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合（over-fitting）”现象，如多项式阶数很大，出现过拟合。 d.结构风险(structural risk minimization, SRM)①在经验风险上加上表示模型复杂度的正则化项或罚项②防止过拟合 其中J(f)为模型的复杂度，是定义在假设空间 F 上的泛函数。模型 f 越复杂，复杂度J(f)就越大；反之，模型 f 越简单，复杂度J(f)就越小 ③决定了用以权衡经验风险和模型复杂度④结构风险小需要经验风险与模型复杂度同时小，结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测，正则化的本质是控制模型的复杂度。 3、贝叶斯公式 贝叶斯公式反映人们推理的方式,即人做实验时，先需要作出一定的假设(利用先验知识)，在假设的指导上去做实验，得到观察数据，最后利用实验数据修正对假设的理解，也就得到后验分布。 最小二乘问题的最大后验估计(MAP)令 最大后验估计等价于最小化正则化的平方损失函数，最大后验估计等价于最小化结构风险。代价函数可改成： 回顾下不加正则化的正规方程 加正则化的正规方程，相当于对矩阵所有特征值同时加了，新矩阵基本上是可逆的(除非原矩阵存在负的特征)，即新的特征方程有唯一解。因而利用贝叶斯的正则化是分类算法中常用的方法。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://enjoyhot.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://enjoyhot.github.io/tags/Machine-Learning/"}]},{"title":"我唱自己的歌","slug":"songsong","date":"2015-01-20T13:30:26.000Z","updated":"2017-04-16T14:48:30.678Z","comments":true,"path":"2015/01/20/songsong/","link":"","permalink":"http://enjoyhot.github.io/2015/01/20/songsong/","excerpt":"我唱自己的歌在布满车前草的道路上在灌木的集市上在雪松和白桦树的舞会上在那山野的原始欢乐上我唱自己的歌","text":"我唱自己的歌在布满车前草的道路上在灌木的集市上在雪松和白桦树的舞会上在那山野的原始欢乐上我唱自己的歌我唱自己的歌在热电厂恐怖的烟云中在变速箱复杂的组织中在砂轮的亲吻中在那社会文明的运行中我唱自己的歌 我唱自己的歌即不陌生又不熟练我是练习曲的孩子愿意加入所有歌队为了不让规范的人们知道我唱自己的歌 我唱歌，唱自己的歌直到世界恢复了史前的寂寞细长的月亮从海边向我走来轻轻地问：为什么？你唱自己的歌 ——顾城 在满是荆棘的历程中，我可能被泡沫掩埋，请让我还来得及向世界挑战。 插入图的两种方法： 内链（空间）： 外链（图床）：","categories":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/categories/Life/"}],"tags":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/tags/Life/"}]},{"title":"【machine learning】linear regression        ","slug":"machinelearning-linear-regression","date":"2015-01-18T11:43:00.000Z","updated":"2017-04-16T14:48:30.654Z","comments":true,"path":"2015/01/18/machinelearning-linear-regression/","link":"","permalink":"http://enjoyhot.github.io/2015/01/18/machinelearning-linear-regression/","excerpt":"一、曲线拟合 1、问题引入 ①假设现在有一份关于某城市的住房面积与相应房价的数据集","text":"一、曲线拟合 1、问题引入 ①假设现在有一份关于某城市的住房面积与相应房价的数据集 表1 居住面积与房价关系 图1 居住面积与房价关系 那么给定这样一个数据集，我们怎么学习出一个以住房面积大小为自变量的用于预测该城市房价的函数？ 问题可形式化为给定大小为m的训练样本集 我们希望学习的目标函数为 房价预测本质上是回归问题 a、回归分析挖掘自变量与因变量之间的关系 b、有监督的学习问题，所有的样本点都带有目标变量 c、输出变量为连续值，可取任意实数 ②假设现在我们有份更详尽的数据集，它还记录了卧室的数量 输入，X=(x1,x2) 假设每个自变量都与因变量Y存在线性相关 目标是学习出假设函数 2、怎样建模 ①基本概念 Relationshipl Linear correlated?l Nonlinear correlated?Mining relationl Correlation coefficient = 1时，称X,Y完全相关，X,Y之间具有线性函数关系 Special Casee.g. 猜想Y与X存在指数关系，观察lnY与X的线性相关性 General—Polynomial Curve Fit（多项式曲线拟合）找到合适的阶数k，使等式成立，譬如logistic regression。 ②多元变量线性回归 上文提到假设函数： 参数或权重(反映每个自变量对输出的影响)，使线性函数空间参数化(h形式已知,用参数来刻画)为了表示方便，令x0(对应截距项)，则上式可写成 注：k与自变量的个数有关，此处k=2 3、怎样获取参数合理的选择策略:对于该数据集的每一个样本，选定的参数使得尽可能接近y。在实际中，尽可能接近用代价函数来表示。Cost Function(代价函数)描述预测值与真实值之间的差距，从而优化目标函数参数，可以利用0-1损失，绝对值损失，平方损失，对数损失。对于线性回归问题，我们采用的目标函数为 这是普通最小二乘回归模型(statistics)，可以利用概率论知识解释为什么可以，如下。 二、概率解释1、选择最小二乘(平方损失)代价函数的理由：我们做出如下假设： E(i)：误差项(没有model出的效应，e.g.遗漏了某些因素的影响)或随机噪声进一步假设： 即 注意，下面式子与这条式子等价。 等价于 之后我们就可以利用似然函数解释最小二乘代价函数： 定义:给定随机变量X与参数，我们观察到结果Y的可能性 由E(i)之间的独立性假设，得 简单解释一下，我们的目标，是实现在给定的情况下，对于m个样本的输入，能输出的m个y的概率的总乘积最大，那构建的模型就越准确了，即最大似然估计。 定义：最大似然估计(maximum likelihood estimation) 当给定似然函数(关联y与x的概率模型)时，一种合理的参数估计方法是尽可 能选择使数据出现的概率最大，即最大化似然函数。 实际上，常用的是对数似然： 因而，最大似然估计等价于最小化平方损失函数，得证。 三、模型求解 1、梯度下降法 (steepest??gradient??descent) 负梯度方向是函数值下降的方向，利用负梯度方向来决定每次迭代的新的搜索方向，使得每次迭代能使待优化的目标函数逐步减小。 a：学习率 其中（关键）： LMS 更新法则： 注意：每次参数更新只用到一个训练样本，样本维数等于维数。 2、批量梯度下降(batch gradient descent) 每次参数更新，需要依赖训练集的所有样本。 对于线性回归问题，代价函数是凸二次规划函数，有全局最优解 图1 梯度下降法迭代过程 3、随机梯度下降 特点:1.每次随机选取一个样本点 立即更新参数2.单个样本点的代价函数值下降近似于总体的代价函数值下降3.对步长选择敏感 可能会出现overshoot the minimum 3、方法比较 1．梯度下降法是批量更新算法，随机梯度是在线算法 2．梯度法优化的是经验风险，随机梯度法优化的是泛化风险 3．梯度法可能陷入局部最优，随机梯度可能找到全局最优 4．梯度法对步长不敏感，随机梯度对步长选择敏感 5．梯度法对初始点(参数)选择敏感 4、输入预处理 a.归一化 输入特征归一化，确保特征在相似的尺度里，但不一定所有的数据都需要归一化。 理由：梯度下降法可能会存执之字形地下降，影响算法的收敛速度。 一般做法： 其中均值，最大值与最小值之差或标准差。 b.步长的选择 对于梯度下降法： 注意两个问题： 1、“调试”：如何确保梯度下降算法正确的执行； 2、如何选择正确的步长(learning rate): α 如何选择α-经验的方法：…, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1… 特别对于随机梯度下降法，步长的选择需满足两点：①保证算法收敛性②保证有机会搜索到全局最优解 5、正规方程假设函数作用于每个样本： 则： 代价函数可改成： 此问题等价于： 即两个向量之间的欧氏距离： 几何意义： 需保证可逆(可逆的充分条件:矩阵X各列线性无关) 回顾一下，上面我们的方法是利用迭代的方式求出，从而使代价函数值最小，并没有求出代价函数。也就是说，所谓的最优解能否求得，不管是通过迭代的方式或是其它方式也好，符合上面的条件才行。 但现实中的数据不是那么理想的。若不可逆，如何求解？1、求伪逆(statistics的解决方案)2、去掉冗余的特征（线性相关）3、去掉过多的特征，例如m &lt;= n (m为样本数, n为特征数) 四、小结1、梯度下降法需要选择合适的learning rate α;需要很多轮迭代即使n很大的时候效果也很好（n为特征数，即维度）2、正规方程不需要选择α不需要迭代，一次搞定 需要计算，其时间复杂度是 n很大，就非常慢，可以考虑降维","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://enjoyhot.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://enjoyhot.github.io/tags/Machine-Learning/"}]},{"title":"Python进行文档抽取与解析的简单实现        ","slug":"python-analysis","date":"2015-01-18T05:36:00.000Z","updated":"2017-04-16T14:48:30.670Z","comments":true,"path":"2015/01/18/python-analysis/","link":"","permalink":"http://enjoyhot.github.io/2015/01/18/python-analysis/","excerpt":"一、前文 之前被叫去做网络爬虫，爬取新浪新闻的url，标题，内容和评论，不过在需求上有点改变，主要是评论的间隔被要求有‘\\t’的分割，比如将 1234&lt;comment&gt;2014-12-10 18:53:20 1004400533 遗弃亲生骨肉猪狗不如，难道就不怕受到良心谴责？你能睡得安稳？2014-12-10 17:17:07 3294923134 这父亲是人吗？&lt;/comment&gt;","text":"一、前文 之前被叫去做网络爬虫，爬取新浪新闻的url，标题，内容和评论，不过在需求上有点改变，主要是评论的间隔被要求有‘\\t’的分割，比如将 1234&lt;comment&gt;2014-12-10 18:53:20 1004400533 遗弃亲生骨肉猪狗不如，难道就不怕受到良心谴责？你能睡得安稳？2014-12-10 17:17:07 3294923134 这父亲是人吗？&lt;/comment&gt; 改为123456&lt;comment&gt;2014-12-10 18:53:20 1004400533遗弃亲生骨肉猪狗不如，难道就不怕受到良心谴责？你能睡得安稳？2014-12-10 17:17:07 3294923134这父亲是人吗？&lt;/comment&gt; 试过重新爬取，不过新浪在2015年后，做了些改变，一是评论api下的内容有了些修改，从原本的中文字符变为了’\\u’，解决办法可以参考另一篇博文 三张图告诉你python爬虫时转换\\u中文字符的“坑”，二是在爬虫上需要做好浏览器模拟的工作，不然很容易被封杀，而且，在做大规模爬取的时候，需要利用多线程或者分布式的手段进行，多线程稍不注意，到了系统线程的上限，这是要注意的。 二、抽取和解析 因为我想爬取这个页面（social）一年的内容，已经超过2万了（只算那些有评论的新闻），所以经常第二天回来发现中途error。分布式计算的东西现在还没去学习，所以还是复习一下python。 在这个简短的程序中，用到了SGMLParser类进行解析，这个类封装了可以实现对具有标签的字符串的处理函数，当然没有标签也是可以用的，不需要冠以html文本种类的要求，网上很多例子可能会拿HTMLParser与SGMLParser进行说明，通过解析X.html进行说明。这里我只采用单线程的方法。 SGMLParser: 1、通过利用开关变量的方法进行标签间内容的抽取和解析，在此之前，需要进行变量的初始化，可以利用init()或者reset()，一般用reset()。 2、start_x(),end_x(),handle_data()，这三个函数，第一个是当读取到;时，执行的函数，第一个是当读取到;时，执行的函数,第三个是当遇到标签内的内容，就会调用这个函数，更详细一点可以参考 这篇博文 变量说明： rootdir：原始文件目录 rootdir2：需要存放转换好的文件目录 方法说明： 1、通过SGMLParser读取;;中的内容，调用solve()函数，利用正则匹配的方法将原来不符合规范的空格替换为’\\t’，返回需要替换的内容到comment变量 2、利用正则匹配找到定位到;;，替换为Parse类对象的comment值 （正则表达式的方法参考以下注释） 完整代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#!/usr/bin/env python# -*- coding: utf-8 -*-from sgmllib import SGMLParserimport re,osrootdir = 'F:\\\\Python27\\\\pythonproject\\\\fuck\\\\file1\\\\'rootdir2 = 'F:\\\\Python27\\\\pythonproject\\\\fuck\\\\file2\\\\'class Parse(SGMLParser): def __init__(self,filename): self.filename = filename self.comment = '' SGMLParser.__init__(self) def reset(self): self.found_comment = False SGMLParser.reset(self) def start_comment(self, attrs): self.found_comment = True def end_comment(self): self.found_comment = False def handle_data(self, text): if self.found_comment == True: self.comment=solve(self.filename,text)def solve(filename,text): #找到3个或3个以上空格的地方，替换为'\\t' strinfo = re.compile('(\\s\\s\\s*)') str_result = strinfo.sub('\\t',text) return str_resultif __name__ == '__main__': global rootdir,rootdir2 for parent,dirnames,filenames in os.walk(rootdir): #三个参数：分别返回1.父目录 2.所有文件夹名字（不含路径） 3.所有文件名字 for filename in filenames: #print filename filesource = open(rootdir+filename, 'a+') s = filesource.read() filesource.close() #初始化实例p,feed()函数为添加处理字符串 p = Parse(filename) p.feed(s) filedes = open(rootdir2+filename, 'a+') #因为原来原数据中存在不以&lt;/comment&gt;;结尾的内容，因此需要先做处理 pattern1='&lt;/comment&gt;;' if re.findall(pattern1,s) == []: s=s+'\\n&lt;/comment&gt;;'&lt;span style=\"white-space:pre\"&gt;; &lt;/span&gt;; #定位到comment标签位置，([\\s\\S]*)能够匹配所有字符，包括换行符，然后替换 strinfo = re.compile('&lt;comment&gt;;([\\s\\S]*)&lt;/comment&gt;;') str_result = strinfo.sub('&lt;comment&gt;;'+p.comment+'&lt;/comment&gt;;',s) filedes.write(str_result) filedes.close() 结果： 注意：win7系统一般文件夹下的文件总数（包括文件夹）不能超过21845个，不然会提示错误。","categories":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/tags/Python/"},{"name":"Spider","slug":"Spider","permalink":"http://enjoyhot.github.io/tags/Spider/"}]},{"title":"【python web学习】python web窥探","slug":"python-web","date":"2015-01-15T07:51:00.000Z","updated":"2017-04-16T14:48:30.674Z","comments":true,"path":"2015/01/15/python-web/","link":"","permalink":"http://enjoyhot.github.io/2015/01/15/python-web/","excerpt":"本文为参考网上一些博客翻译以及想法，自己写的一篇总结博文，可能有重复的地方，纯粹总结只用。阅读之前可参考: 1、How to write a web framework in Python（作者anandology是","text":"本文为参考网上一些博客翻译以及想法，自己写的一篇总结博文，可能有重复的地方，纯粹总结只用。阅读之前可参考: 1、How to write a web framework in Python（作者anandology是web.py代码的两位维护者之一，另一位则是大名鼎鼎却英年早逝的AaronSwartz ） 2、Why so many Python web frameworks? 也是一篇很好的文章，也许它会让您对Python中Web框架的敬畏之心荡然 无存:-) 如果你打算用python进行网络开发的话，自己写的框架可以说是一种不受支持的想法，可能使用一个现成的Web框架（如Djang、Tornado 、web.py 、Pylons等）会是更合适的选择，毕竟都是大师级的作品。 一、一次最简单的web之旅1234567891011#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\"myweb.py\"\"\"from wsgiref.simple_server import make_server, demo_apphttpd = make_server('', 8086, demo_app)sa = httpd.socket.getsockname()print 'http://&#123;0&#125;:&#123;1&#125;/'.format(*sa)# Respond to requests until process is killedhttpd.serve_forever() 在命令运行之后 打开浏览器：http://0.0.0.0:8086/ 一行”Hello world!” 和 众多环境变量值。 定位到simple_server.py文件，我们看到make_server函数和WSGIServer类 1234567891011121314151617181920212223242526272829303132333435 def make_server( host, port, app, server_class=WSGIServer, handler_class=WSGIRequestHandler): \"\"\"Create a new WSGI server listening on `host` and `port` for `app`\"\"\" server = server_class((host, port), handler_class) server.set_app(app) return serverclass WSGIServer(HTTPServer): \"\"\"BaseHTTPServer that implements the Python WSGI protocol\"\"\" application = None def server_bind(self): \"\"\"Override server_bind to store the server name.\"\"\" HTTPServer.server_bind(self) self.setup_environ() def setup_environ(self): # Set up base environment env = self.base_environ = &#123;&#125; env['SERVER_NAME'] = self.server_name env['GATEWAY_INTERFACE'] = 'CGI/1.1' env['SERVER_PORT'] = str(self.server_port) env['REMOTE_HOST']='' env['CONTENT_LENGTH']='' env['SCRIPT_NAME'] = '' def get_app(self): return self.application def set_app(self,application): self.application = application 可以看到，我们运行python文件后启动的是WSGIServer类对象（继承于HTTPServer，子类有run函数，后文会详细讲一下），而demo_app是一个拥有特定格式:接受两个参数，一个列表return 对象的函数，抑或是类、类对象（见下文）。很多时候，要简单写一个web框架，主要需要改动传入的app以及server。 二、app的修改其中，可调用对象 包括 函数、方法、类 或者 具有call方法的 实例；environ 是一个字典对象，包括CGI风格的环境变量（CGI-style environment variables）和 WSGI必需的变 量（WSGI-required variables）；start_response 是一个可调用对象，它接受两个常规参数（status，response_headers）和 一个 默认参数（exc_info）；字符串迭代对象 可以是 字符 串列表、生成器函数 或者 具有iter方法的可迭代实例。更多细节参考Specification Details。 The Application/Framework Side中给出了一个典型的application实现： 1234567891011#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\"application.py\"\"\"def simple_app(environ, start_response): \"\"\"Simplest possible application object\"\"\" status = '200 OK' response_headers = [('Content-type', 'text/plain')] start_response(status, response_headers) return ['Hello world!\\n'] 替换原来自带的demo_app，重新运行之 123456789101112#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\"code.py\"\"\"from wsgiref.simple_server import make_serverfrom application import my_app as appif __name__ == '__main__': httpd = make_server('', 8086, app) sa = httpd.socket.getsockname() print 'http://&#123;0&#125;:&#123;1&#125;/'.format(*sa) #Respond to requests until process is killed httpd.serve_forever() 这时就输出hello world!而没有环境变量。因为demo_app.py是这样的： 12345678910def demo_app(environ,start_response): from StringIO import StringIO stdout = StringIO() print &gt;;&gt;;stdout, \"Hello world!\" print &gt;;&gt;;stdout h = environ.items(); h.sort() for k,v in h: print &gt;;&gt;;stdout, k,'=', repr(v) start_response(\"200 OK\", [('Content-Type','text/plain')]) return [stdout.getvalue()] 三、URL调度修改之前的访问server都是基于host+port的形式，那要怎样实现url的分发呢。这需要对app进行修改才行。说到这里，就先将app从一个函数改为一个类吧，再做url区分处理。 123456789101112131415#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\"application.py\"\"\"class my_app: def __init__(self, environ, start_response): self.environ = environ self.start = start_response def __iter__(self): status = '200 OK' response_headers = [('Content-type', 'text/plain')] self.start(status, response_headers) yield \"Hello world!\\n\" 复习一下python类的语法，说说为什么可以这样写。开始的app可以这样用 1list = simple_app(a,b) 现在也可以这样用 1list = my_app(a,b) 注：其中参数来自init()，返回值来自iter()的return值（yield返回的就是一个可迭代对象），也许你会问，如果是传类对象的话呢？且看下下文。 再在return的函数即iter()中修改根据不同的path进行不同返回。 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\"application.py\"\"\"class my_app: def __init__(self, environ, start_response): self.environ = environ self.start = start_response def __iter__(self): path = self.environ['PATH_INFO'] #environ的作用看到了吧 if path == \"/\": return self.GET_index() elif path == \"/hello\": return self.GET_hello() else: return self.notfound() def GET_index(self): status = '200 OK' response_headers = [('Content-type', 'text/plain')] self.start(status, response_headers) yield \"Welcome!\\n\" def GET_hello(self): status = '200 OK' response_headers = [('Content-type', 'text/plain')] self.start(status, response_headers) yield \"Hello world!\\n\" def notfound(self): status = '404 Not Found' response_headers = [('Content-type', 'text/plain')] self.start(status, response_headers) yield \"Not Found\\n\" 这时用浏览器就可以访问/,/hello,其他访问为Not Found。 四、重构1、正则匹配URL消除URL硬编码，增加URL调度的灵活性： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\"application.py\"\"\"import re ##########修改点class my_app: urls = ( (\"/\", \"index\"), (\"/hello/(.*)\", \"hello\"), ) ##########修改点，Django工程中url.py即视感 def __init__(self, environ, start_response): self.environ = environ self.start = start_response def __iter__(self): ##########修改点 path = self.environ['PATH_INFO'] method = self.environ['REQUEST_METHOD'] for pattern, name in self.urls: m = re.match('^' + pattern + '$', path) #注意这里，url匹配函数名 if m: #pass the matched groups as arguments to the function args = m.groups() funcname = method.upper() + '_' + name if hasattr(self, funcname): func = getattr(self, funcname) return func(*args) return self.notfound() def GET_index(self): status = '200 OK' response_headers = [('Content-type', 'text/plain')] self.start(status, response_headers) #遵循调用start_response后再return iterObject yield \"Welcome!\\n\" def GET_hello(self, name): ##########修改点 status = '200 OK' response_headers = [('Content-type', 'text/plain')] self.start(status, response_headers) yield \"Hello %s!\\n\" % name def notfound(self): status = '404 Not Found' response_headers = [('Content-type', 'text/plain')] self.start(status, response_headers) yield \"Not Found\\n\" 2、消除GET_*方法中的重复代码，并且允许它们返回字符串：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\"application.py\"\"\"import reclass my_app: urls = ( (\"/\", \"index\"), (\"/hello/(.*)\", \"hello\"), ) def __init__(self, environ, start_response): ##########修改点 self.environ = environ self.start = start_response self.status = '200 OK' self._headers = [] def __iter__(self): ##########修改点 result = self.delegate() #利用这个函数先进行返回结果的收集 self.start(self.status, self._headers) #start_response # 将返回值result（字符串 或者 字符串列表）转换为迭代对象 if isinstance(result, basestring): return iter([result]) else: return iter(result) def delegate(self): ##########修改点 path = self.environ['PATH_INFO'] method = self.environ['REQUEST_METHOD'] for pattern, name in self.urls: m = re.match('^' + pattern + '$', path) if m: # pass the matched groups as arguments to the function args = m.groups() funcname = method.upper() + '_' + name if hasattr(self, funcname): func = getattr(self, funcname) return func(*args) return self.notfound() def header(self, name, value): ##########修改点 self._headers.append((name, value)) def GET_index(self): ##########修改点 self.header('Content-type', 'text/plain') return \"Welcome!\\n\" def GET_hello(self, name): ##########修改点 self.header('Content-type', 'text/plain') return \"Hello %s!\\n\" % name def notfound(self): ##########修改点 self.status = '404 Not Found' self.header('Content-type', 'text/plain') return \"Not Found\\n\" 3、抽象出框架为了将类my_app抽象成一个独立的框架，需要作出以下修改： 1、剥离出其中的具体处理细节：urls配置 和 GET_*方法（改成在多个类中实现相应的GET方法） 2、把方法header实现为类方法（classmethod），以方便外部作为功能函数调用 3、改用 具有call方法的 实例 来实现application（上文提到） 修改后的application.py（最终版本）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\"application.py\"\"\"import reclass my_app: \"\"\"my simple web framework\"\"\" headers = [] def __init__(self, urls=(), fvars=&#123;&#125;): self._urls = urls self._fvars = fvars def __call__(self, environ, start_response): self._status = '200 OK' # 默认状态OK del self.headers[:] # 清空上一次的headers result = self._delegate(environ) start_response(self._status, self.headers) # 将返回值result（字符串 或者 字符串列表）转换为迭代对象 if isinstance(result, basestring): return iter([result]) else: return iter(result) def _delegate(self, environ): path = environ['PATH_INFO'] method = environ['REQUEST_METHOD'] for pattern, name in self._urls: m = re.match('^' + pattern + '$', path) if m: # pass the matched groups as arguments to the function args = m.groups() funcname = method.upper() # 方法名大写（如GET、POST） klass = self._fvars.get(name) # 根据字符串名称查找类对象 if hasattr(klass, funcname): func = getattr(klass, funcname) return func(klass(), *args) return self._notfound() def _notfound(self): self._status = '404 Not Found' self.header('Content-type', 'text/plain') return \"Not Found\\n\" @classmethod def header(cls, name, value): cls.headers.append((name, value)) 到这里，基本上就算是小功告成了，但只是了解了怎么用那些子类，大篇幅还是讲怎么设计。窥探一下wsgiref.simple_server。 五、wsgiref原理介绍1、概述a.什么是WSGI, WSGI application, WSGI server, WSGI middleware. WSGI是关于Python脚本与Web服务器交互的协议，wsgi将 web 组件分为三类： web服务器,web中间件,web应用程序。 b.WSGI Server有哪些 比如 Django、CherryPy 都自带 WSGI server,主要是测试用途, 发布时则使用生产环境的 WSGI server，例如Apache，nginx等，而有些 WSGI 下的框架比如 pylons、bfg 等, 自己不实现 WSGI server。 wsgiref就是python自带的WSGI server。上面提到的app需要传入的两个参数application(environ, start_response)，其实就是一个接口两个参数的集合体。一篇博文这样说明： wsgi server 基本工作流程： 1、服务器创建socket，监听端口，等待客户端连接。 2、当有请求来时，服务器解析客户端信息放到环境变量environ中，并调用绑定的handler来处理请求。 3、handler解析这个http请求，将请求信息例如method，path等放到environ中。 4、wsgi handler再将一些服务器端信息也放到environ中，最后服务器信息，客户端信息，本次请求信息全部都保存到了环境变量environ中。 5、wsgi handler 调用注册的wsgi app，并将environ和回调函数传给wsgi app 6、wsgi app 将reponse header/status/body 回传给wsgi handler 7、最终handler还是通过socket将response信息塞回给客户端。 2、组成（python2.7.8） simple_server 这一模块实现了一个简单的 HTTP 服务器，并给出了一个简单的 demo，运行： python simple_server.py 会启动这个demo，运行一次请求，并把这次请求中涉及到的环境变量在浏览器中显示出来。 handlers simple_server模块将HTTP服务器分成了 Server 部分和Handler部分，前者负责接收请求，后者负责具体的处理， 其中Handler部分主要在handlers中实现。 headers 这一模块主要是为HTTP协议中header部分建立数据结构。 util 这一模块包含了一些工具函数，主要用于对环境变量，URL的处理。 validate 这一模块提供了一个验证工具，可以用于验证你的实现是否符合WSGI标准。 simple_server 模块主要有两部分内容，上面一到四的内容可以总结。 应用程序 函数demo_app是应用程序部分 服务器程序 服务器程序主要分成Server 和 Handler两部分，另外还有一个函数 make_server 用来生成一个服务器实例 各种继承关系： 12345678910111213141516171819# M:# +------------+# | BaseServer |# +------------+# |# V# +------------+# | TCPServer |# +------------+# |# V# +------------+# | HTTPServer |# +------------+# |# V# +------------+# | WSGIServer |# +------------+ 12345678910111213141516171819 # M:# +--------------------+# | BaseRequestHandler |# +--------------------+# |# V# +-----------------------+# | StreamRequestHandler |# +-----------------------+# |# V# +------------------------+# | BaseHTTPRequestHandler |# +------------------------+# |# V# +--------------------+# | WSGIRequestHandler |# +--------------------+ 1234567891011121314# M:# +-------------+# | BaseHandler |# +-------------+# |# V# +----------------+# | SimpleHandler |# +----------------+# |# V# +---------------+# | ServerHandler |# +---------------+ 在调用make_server的时候，都发生了什么 再这里，就不细讲handler的处理过程了，很多时候网络handler的研究需要看源码才能真正消化。 以下完全引用on_1y的一篇博文，该博文讲得很细，不过需要研究源码才能真正消化。可以先看doc:https://docs.python.org/2/library/wsgiref.html再看源码:https://pypi.python.org/pypi/wsgiref headers 这个模块是对HTTP 响应部分的头部设立的数据结构，实现了一个类似Python 中 dict的数据结构。可以看出，它实现了一些函数来支持一些运算符，例如 len, setitem, getitem, delitem, str, 另外，还实现了 dict 操作中的get, keys, values函数 util 这个模块主要就是一些有用的函数，用于处理URL, 环境变量。 validate 这个模块主要是检查你对WSGI的实现，是否满足标准，包含三个部分： validator Wrapper Check validator 调用后面两个部分来完成验证工作，可以看出Check部分对WSGI中规定的各个部分进行了检查。","categories":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/tags/Python/"},{"name":"Python web","slug":"Python-web","permalink":"http://enjoyhot.github.io/tags/Python-web/"}]},{"title":"Python spider error occurs to Chinese words","slug":"python-chinese","date":"2015-01-09T11:51:00.000Z","updated":"2017-04-16T14:48:30.670Z","comments":true,"path":"2015/01/09/python-chinese/","link":"","permalink":"http://enjoyhot.github.io/2015/01/09/python-chinese/","excerpt":"平台为win7系统","text":"平台为win7系统 一劳永逸的方法： 1234567s=s.replace(\"'\",'\"')s=s.replace(\"\\n\",'')s1=\"u'\"+s+\"'\"try: ss=eval(s1)except: return 将’转换为“，将\\n去掉。利用eval转化即可。","categories":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/categories/Python/"}],"tags":[{"name":"Spider","slug":"Spider","permalink":"http://enjoyhot.github.io/tags/Spider/"}]},{"title":"今天真是窘迫的一天","slug":"poor-day","date":"2014-12-28T12:23:50.000Z","updated":"2017-04-16T14:48:30.666Z","comments":true,"path":"2014/12/28/poor-day/","link":"","permalink":"http://enjoyhot.github.io/2014/12/28/poor-day/","excerpt":"今天是一个窘迫的一天，昨天上午去实验室，发现前天跑的爬虫还是没爬完，郁闷，有的爬的数据比它多用几个小时就跑完了，然后又开了一个cmd窗口继续跑另一个爬虫，后来司机，鼠标也动不了，强制关机后不进行硬盘检查（在笔记本经常这样做，省时），不过在台式机还是第一次这么干。今天终于尝到苦头。","text":"今天是一个窘迫的一天，昨天上午去实验室，发现前天跑的爬虫还是没爬完，郁闷，有的爬的数据比它多用几个小时就跑完了，然后又开了一个cmd窗口继续跑另一个爬虫，后来司机，鼠标也动不了，强制关机后不进行硬盘检查（在笔记本经常这样做，省时），不过在台式机还是第一次这么干。今天终于尝到苦头。 今天依旧跑的是昨天强制关机后的那个程序（注意特指），不过下午才到的实验室，居然比昨天还慢，一半都没有。索性不理它，继续搞，再开窗口，到了四点多的时候，厄运来了，自动关机。自动重启，硬盘检查，不过启动失败直接进入系统修复，失败。想到的是四种方法，系统修复，安全模式，pe系统的修复，重装系统，后来真的只能重装系统了，pe系统修复那步也许做得不太好，不过以防万一，就将c盘中的虚拟机拷到f盘。 也许是我频繁得对硬盘io进行操作，导致D盘受损，开始以为是系统问题，想继续做个系统，边做边上网搜方法，用了一下diskGenius，但最终还是得将d盘格式化，毕竟里面的东西我只放了个Dreamweaver而已，大不了再下一个。 看上去好像都搞定了的样子，其实最重要的事情能不能成功都是还是挺担心的，虽说备份了虚拟机在c盘的东西，不过还是第一次要想这么做。尼玛看来我没有好好调研清楚怎么迁移virtualbox的系统就开始弄，确实有点草率，映射的硬盘我都不记得在哪里，拷过来的东西只有几兆，显然错了。近乎崩溃……工程还没有完全拷过来（这已经够崩溃了） 后来逐渐深入才知道，要重新搭一个环境也是挺久的，django,mongodb以及webvirtmgr附带的东西，数据库记得当时调试的时候是手打出来的假数据，现在不知新建一个空的会不会报错。 明天又是一个挑战的一天，挺浪费时间的，又得搭环境，又得写服务器信息栏目后台，还好前端一直在win7下写的，之前后天边调编写用了接近3天，希望在之前的经验下，能在明天内既搭好环境又一气呵成写好后台，老天保佑，我不想做太多无谓时间牺牲，虽说反驳者觉得可以练练手，写重复的东西你说呢！？","categories":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/categories/Life/"}],"tags":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/tags/Life/"},{"name":"Spider","slug":"Spider","permalink":"http://enjoyhot.github.io/tags/Spider/"}]},{"title":"考研日","slug":"postgraduate-exam","date":"2014-12-27T12:45:09.000Z","updated":"2017-04-16T14:48:30.666Z","comments":true,"path":"2014/12/27/postgraduate-exam/","link":"","permalink":"http://enjoyhot.github.io/2014/12/27/postgraduate-exam/","excerpt":"微信发一条真诚祝福，也是对自己的祝福勉励，不忘自己是怎么走到这一步，运气背，但也阴差阳错。 今天是考研的第一天，曾经很担心这一天的到来，直到9月中旬。依稀记得，8月刚过20，内心甚是纠结，最后还是面对现实，做两手准备，虽是不甘，也无可奈何。","text":"微信发一条真诚祝福，也是对自己的祝福勉励，不忘自己是怎么走到这一步，运气背，但也阴差阳错。 今天是考研的第一天，曾经很担心这一天的到来，直到9月中旬。依稀记得，8月刚过20，内心甚是纠结，最后还是面对现实，做两手准备，虽是不甘，也无可奈何。 今天起得不算太晚，9点出头，不想去实验室，只想到图书馆坐坐，看看书，一到那里，人出奇得多，不过转念一想，考试月，理所当然，自从大四后关于自习室的念想都是研究生层面啊。 研究生阶段的方向，如无意外，无其它偏好，应该就是数据挖掘，喜好程度，中偏上而已。做做工程也好，搞搞理论也罢，出路两条，创业或工作，不止说说而已，咳咳。 慢慢接近自己想得到的，不忘初衷，成长当如此。","categories":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/categories/Life/"}],"tags":[{"name":"Life","slug":"Life","permalink":"http://enjoyhot.github.io/tags/Life/"}]},{"title":"html与javascript之网络篇","slug":"html-javascript-1","date":"2014-12-26T06:09:00.000Z","updated":"2017-04-16T14:48:30.642Z","comments":true,"path":"2014/12/26/html-javascript-1/","link":"","permalink":"http://enjoyhot.github.io/2014/12/26/html-javascript-1/","excerpt":"此篇依然与前一篇动态篇环境一样，基于jquery #一、表单提交（post的同步加载） 12345678 &lt;form method=\"post\" onSubmit=\"return check_recover()\"&gt;; &lt;a href=\"javascript:void(0)\" id=\"allSelect2\" &gt;;全部&lt;/a&gt;; &lt;a &gt;;-&lt;/a&gt;; &lt;a href=\"javascript:void(0)\" id=\"noneSelect2\" &gt;;无&lt;/a&gt;; &lt;a &gt;;-&lt;/a&gt;; &lt;input type=\"submit\" class=\"btn btn-primary\" name=\"recover\" value=\"恢复所选\"&gt;;&lt;/input&gt;; &lt;button type=\"button\" class=\"btn btn-primary\"&gt;;删除所选&lt;/button&gt;;&lt;/form&gt;;","text":"此篇依然与前一篇动态篇环境一样，基于jquery #一、表单提交（post的同步加载） 12345678 &lt;form method=\"post\" onSubmit=\"return check_recover()\"&gt;; &lt;a href=\"javascript:void(0)\" id=\"allSelect2\" &gt;;全部&lt;/a&gt;; &lt;a &gt;;-&lt;/a&gt;; &lt;a href=\"javascript:void(0)\" id=\"noneSelect2\" &gt;;无&lt;/a&gt;; &lt;a &gt;;-&lt;/a&gt;; &lt;input type=\"submit\" class=\"btn btn-primary\" name=\"recover\" value=\"恢复所选\"&gt;;&lt;/input&gt;; &lt;button type=\"button\" class=\"btn btn-primary\"&gt;;删除所选&lt;/button&gt;;&lt;/form&gt;; 解释：当点击submit对应的按钮时，会直接提交到服务器，当此前为调用check_recover()函数，在这个函数中可以判断提交的内容是否合法，如果是则return true，则完成表单提交，实现同步加载。 二、ajax异步加载123456789101112131415function post_message1()&#123;//alert('success')data=&#123;'type':\"2\",'ip':'127.0.0.1'&#125;;$.post($(this).attr('action'), data, function(data,status,xhr)&#123;if(status==\"success\")&#123;change();&#125;&#125;);&#125;function post_message2()&#123;$.getJSON(\"yoururl\",function(data)&#123;dosomething();&#125;);&#125; 解释： 1.当调用post_message()函数时，发送data数据到服务器，当成功返回时，再调用change()函数。 2.当调用post_message()函数时，访问url为yoururl的链接，返回含有data数据的response，然后调用dosomething()函数。 三、刷新当前页面1window.location.reload(); 四、定时执行某些特定操作，譬如更新图 123 function () &#123; window.setInterval('hostusage()', 2000);&#125; 解释：两秒调用hostusage()函数一次","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://enjoyhot.github.io/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://enjoyhot.github.io/tags/JavaScript/"}]},{"title":"html与javascript之动态篇        ","slug":"html-javascript-2","date":"2014-12-26T05:26:00.000Z","updated":"2017-04-16T14:48:30.642Z","comments":true,"path":"2014/12/26/html-javascript-2/","link":"","permalink":"http://enjoyhot.github.io/2014/12/26/html-javascript-2/","excerpt":"以下所讲内容采用bootstrap模板，jquery库，对于原生html大致也同样适用 一、侧边栏分页","text":"以下所讲内容采用bootstrap模板，jquery库，对于原生html大致也同样适用 一、侧边栏分页 12345678910111213141516171819202122232425262728293031323334353637 &lt;div class=\"navbar-default sidebar\" role=\"navigation\"&gt; &lt;div class=\"sidebar-nav navbar-collapse\"&gt; &lt;ul class=\"nav\" id=\"side-menu\"&gt; &lt;li class=\"sidebar-search\"&gt; &lt;div class=\"input-group custom-search-form\"&gt; &lt;input type=\"text\" class=\"form-control\" placeholder=\"Search...\"&gt; &lt;span class=\"input-group-btn\"&gt; &lt;button class=\"btn btn-default\" type=\"button\"&gt; &lt;i class=\"fa fa-search\"&gt;&lt;/i&gt; &lt;/button&gt; &lt;/span&gt; &lt;/div&gt; &lt;!-- /input-group --&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=\"#\"&gt;&lt;i class=\"fa fa-wrench fa-fw\"&gt;&lt;/i&gt; 审核业务&lt;span class=\"fa arrow\"&gt;&lt;/span&gt;&lt;/a&gt; &lt;ul class=\"nav nav-second-level\"&gt; &lt;li&gt; &lt;a href=\"&#123;% url 'user-review' %&#125;\"&gt;用户审核业务&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=\"&#123;% url 'device-review' %&#125;\"&gt;设备审核业务&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=\"&#123;% url 'middleware-review' %&#125;\"&gt;中间件审核业务&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;!-- /.nav-second-level --&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;!-- /.sidebar-collapse --&gt;&lt;/div&gt;&lt;!-- /.navbar-static-side --&gt; 当点击某一项跳转到另外页面时，可以在相应的html页面中改为 123456789101112131415 &lt;li class=\"active\"&gt; &lt;a href=\"#\"&gt;&lt;i class=\"fa fa-wrench fa-fw\"&gt;&lt;/i&gt; 审核业务&lt;span class=\"fa arrow\"&gt;&lt;/span&gt;&lt;/a&gt; &lt;ul class=\"nav nav-second-level\"&gt; &lt;li&gt; &lt;a class=\"active\" href=\"&#123;% url 'user-review' %&#125;\"&gt;用户审核业务&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=\"&#123;% url 'device-review' %&#125;\"&gt;设备审核业务&lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=\"&#123;% url 'middleware-review' %&#125;\"&gt;中间件审核业务&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;!-- /.nav-second-level --&gt;&lt;/li&gt; 二、单击按钮弹出modal对话框 1&lt;button type=\"button\" class=\"btn btn-primary\" data-toggle=\"modal\" data-target=\"#myModal\"&gt;新增用户&lt;/button&gt; modal为嵌入在html div中的html隐藏文本 123456789101112131415161718192021222324252627&lt;div aria-hidden=\"true\" aria-labelledby=\"myModalLabel\" class=\"modal fade\" id=\"myModal\" role=\"dialog\" tabindex=\"-1\"&gt;&lt;div class=\"modal-dialog\"&gt;&lt;div class=\"modal-content\"&gt;&lt;div class=\"modal-header\" style=\"background:#d9edf7\"&gt;&lt;button aria-hidden=\"true\" class=\"close\" data-dismiss=\"modal\" type=\"button\"&gt;×&lt;/button&gt;&lt;h4 class=\"modal-title\" id=\"myModalLabel\"&gt;新增用户信息&lt;/h4&gt;&lt;/div&gt; &lt;div class=\"modal-body form-horizontal\"&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"username\" class=\"col-sm-2 control-label\"&gt;用户名&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"text\" class=\"form-control\" id=\"username\" placeholder=\"用户名\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"modal-footer\"&gt; &lt;input type=\"hidden\" name=\"add-name\" id=\"add-id\"&gt;&lt;/input&gt; &lt;button class=\"btn btn-default\" data-dismiss=\"modal\" type=\"button\"&gt;关闭&lt;/button&gt; &lt;input class=\"btn btn-primary\" type=\"submit\" name=\"addButton\" value=\"确定\"&gt;&lt;/input&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt; 三、按钮触发事件 12345678910 &lt;input type=\"submit\" class=\"btn btn-primary\" name=\"pass\" value=\"通过所选\"&gt;&lt;/input&gt;&lt;script type=\"text/javascript\"&gt;$(document).ready(function()&#123;$(\"input[name='pass']\").click(function()&#123;check(\"hidden-value2\")&#125;)&#125;) &lt;/script&gt; 监听name为pass的按钮点击事件，当成也可以在button中添加onclick方法。 四、获取table的行列的文本内容，或是值 1var rowsize = document.getElementById('user-history-body').getElementsByTagName('tr').length 解释：获取tbody—，得到其下tr标签的个数。 1var td = document .getElementById (\"dataTables-user-history\").rows [i+1].cells[8].innerHTML 解释：获取 1&lt;table class=\"table table-striped table-bordered table-hover\" id=\"dataTables-user-history\"&gt; 下tobody的第i+1行第8列的文本 五、checkbox的全选和全不选操作 假设监听的对象的id=”chk_all2”，另外需置所有的checkbox的name=”transPro2” 1234567891011 //&lt;!--全选与全不选的操作（点击checkbox）--&gt;$(document).on(\"click\",\"#chk_all2\",function()&#123; var ischecked= $(\"#chk_all2:checked\").length var $checkboxs =$(\"input[name='transPro2']\") if(ischecked)&#123; $checkboxs.prop(\"checked\",true) &#125;else&#123; $checkboxs.prop(\"checked\",false) &#125;&#125;)//&lt;!--end--&gt; 1.可以类似QQ邮箱那样用checkbox控制checkbox 12345&lt;input type=\"checkbox\" name=\"transPro2\" id=\"chk_all2\"/&gt;2.用点击文本的方式控制则为:```html&lt;a href=\"javascript:void(0)\" id=\"allSelect2\" &gt;全部&lt;/a&gt; 而javascript改为 1234567 &lt;script type=\"text/javascript\"&gt;$(document).on(\"click\",\"#allSelect2\",function()&#123;var $checkboxs =$(\"input[name='transPro2']\")$checkboxs.prop(\"checked\",true)&#125;) &lt;/script&gt; 注意有些人喜欢将 1href=\"javascript:void(0)\" 改为href=”#”，但这样的话点击之后会使页面回到页首部，太过突兀。","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://enjoyhot.github.io/categories/JavaScript/"}],"tags":[{"name":"JavaScript","slug":"JavaScript","permalink":"http://enjoyhot.github.io/tags/JavaScript/"}]},{"title":"冲茶之道","slug":"drink-tea","date":"2014-12-13T09:02:00.000Z","updated":"2017-04-16T14:48:30.630Z","comments":true,"path":"2014/12/13/drink-tea/","link":"","permalink":"http://enjoyhot.github.io/2014/12/13/drink-tea/","excerpt":"沏茶的艺术 1.白鹤沐浴 （洗 杯）：用开水洗净茶具 2.乌龙入宫 （落 茶）：把铁观音茶放入茶具，放茶量约占茶具容量的五分；","text":"沏茶的艺术 1.白鹤沐浴 （洗 杯）：用开水洗净茶具 2.乌龙入宫 （落 茶）：把铁观音茶放入茶具，放茶量约占茶具容量的五分； 3.悬壶高冲 （冲 茶）：把滚开的水提高冲入茶壶或盖瓯，使茶叶转动； 4.春风拂面 （刮泡沫）：用壶盖或瓯盖轻轻刮去漂浮的白泡沫，使其清新洁净； 5.关公巡城 （倒茶）：把泡一，二分钟后的茶水依次巡回注入并列的茶杯里； 6.韩信点兵 （点 茶）：茶水倒到少许时要一点一点均匀地滴到各茶杯里； 7.鉴尝汤色 （看 茶）：观尝杯中茶水的颜色 8.品啜甘霖 （喝 茶）：乘热细缀，先闻其香，后尝其味，边啜边闻，浅斟细饮。饮量虽不多，但能齿颊留香，喉底回甘，心旷神怡，别有情趣。","categories":[{"name":"tea","slug":"tea","permalink":"http://enjoyhot.github.io/categories/tea/"}],"tags":[{"name":"tea","slug":"tea","permalink":"http://enjoyhot.github.io/tags/tea/"}]},{"title":"Python2.x学习小记","slug":"python2.x-study","date":"2014-12-11T12:14:00.000Z","updated":"2017-04-16T14:48:30.678Z","comments":true,"path":"2014/12/11/python2.x-study/","link":"","permalink":"http://enjoyhot.github.io/2014/12/11/python2.x-study/","excerpt":"不定时更新，不一定适合3.X，但一定适合2.7。一、Python中的装饰器","text":"不定时更新，不一定适合3.X，但一定适合2.7。一、Python中的装饰器 1234567891011121314151617def main(): def decorator_with_params(arg_of_decorator): print arg_of_decorator print '1' def newDecorator(func): print '3' print func.__name__ return func(1,2) print '2' return newDecorator @decorator_with_params(\"deco_args\") def foo(a,b): print 'foo('+str(a)+','+ str(b)+') is called'if __name__ == '__main__': main() 输出结果： 123456deco_args123foofoo(1,2) is called","categories":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://enjoyhot.github.io/tags/Python/"}]},{"title":"mongodb在python中的应用——利用pymongo和mongoengine","slug":"pymongo-vs-mongoengine","date":"2014-12-10T04:56:00.000Z","updated":"2017-04-16T14:48:30.670Z","comments":true,"path":"2014/12/10/pymongo-vs-mongoengine/","link":"","permalink":"http://enjoyhot.github.io/2014/12/10/pymongo-vs-mongoengine/","excerpt":"一、前文Django 是一种 Python Web 框架，由对象关系映射器 (ORM)、后端控制器和模板系统组成。MongoDB 是一种面向文档的数据库（也称为 NoSQL 数据库），能有效地进行扩展并提供高性能。在本文中，我们将学习如何从 Python 调用 MongoDB（使用 MongoEngine），以及如何将它集成到 Django 项目中以代替内置的 ORM。","text":"一、前文Django 是一种 Python Web 框架，由对象关系映射器 (ORM)、后端控制器和模板系统组成。MongoDB 是一种面向文档的数据库（也称为 NoSQL 数据库），能有效地进行扩展并提供高性能。在本文中，我们将学习如何从 Python 调用 MongoDB（使用 MongoEngine），以及如何将它集成到 Django 项目中以代替内置的 ORM。NoSQL 数据库是 “下一代数据库，主要具有以下几个要点：非关系型、分布式、开放源码和可水平伸缩”。面向文档的数据库 MongoDB 就是这种类型的数据库。要添加对 MongoDB 的支持非常容易，但要以失去自动管理面板为代价。因此，您必须根据您的需要进行权衡。MongoDB 是一种无模式数据库，与关系型数据库完全相反。无模式数据库没有使用表格，而是使用由文档组成的集合。这些文档是使用对象字面量语法创建的。 图片不可见可直接看原文可参考： http://www.zhihu.com/question/20059632 http://www.csdn.net/article/2014-03-06/2818652-when-use-mongodb-rather-mysql 二、工程需求 Django 通过mongoengine库调用mongoDB，由于mongoengine底层使用的是pymongo库，所以安装mongoengine的时候一定要安装与其版本配套的pymongo版本。几条命令，分别用于安装pymongo,mongodb,关/开mongodb服务，若easy_install安装不成功，就要安装easy_install工具： 123easy_install pymongoapt-get install mongodbservice mongodb stop/start 第一部分：pymongo 12345import pymongocon=pymongo.Connection('127.0.0.1',27017) #连接到数据库db=con.cimcc #获取数据库db.authenticate('username','password')collection=db.cimcc_user #获得表 举例子应用： 1.查找 1collection.find(&#123;'level':2,'userid':int(float(item))&#125;) 返回list，下面2有示例 2.降序排序，获取键值 12for i in collection.find().sort([(\"userid\",-1)]): userid=i['userid']+userid 3.插入数据项 1collection.insert(&#123;'userid':1&#125;) 4.更新 1collection.update(&#123;'userid':int(float(item))&#125;,&#123;'$set':&#123;'status':1&#125;&#125;,upsert=False,multi=True) 有兴趣可以研究一下update的四个参数，第一个为匹配，第二个为条件（条件上可以大作文章），第三个为没找到是否插入，第四个为是否支持多操作 pymongo够直接，但脱离了python web设计的本意，django也是mvc设计的典型例子，然后利用pymongo就不能很好地体现这点，耦合度较高。 第二部分：mongoengine 其实开始我是打算用django-nonrel，因为利用其可以再内存中模拟ORM，继承models.Model,管理者admin就有一个可视化管理数据库的界面（自带），也可以利用Model的函数映射到mongodb，但想想就好，后来没能成功装上，估计被墙了。后来发现知乎有人说：django的ORM并不支持NoSQL，但是有一个叫django-nonrel的分支，扩展了django的ORM，支持部分NoSQL数据库，其中包括mongodb 从我最后一次关心django-nonrel时它的可用度来看，实际生产环境使用还是很困难。比如说： mongodb类NoSQL天然没有join操作，所以在django-nonrel中使用mongodb时没有多表继承功能，只能从abstract类继承 没有join的操作也意味着很多atomic的SQL查询需要转换成非atomic的nosql查询 很多原ORM的特性在django-nonrel中使用会抛出NotImplemented的异常。 所以正常情况下请勿使用django-nonrel提供的nosql支持（除非你有兴趣给它贡献代码） http://www.zhihu.com/question/19818326 于是我还是用了mongoengine，继承Document而不是models.Model，效果也还行。 123456789101112from mongoengine import *connect('database',host='127.0.0.1',username='username',password='password')# Create your models here.class cimc_message(Document): msg_from=StringField(required=True) to=IntField(required=True) msg_type=StringField(required=True) status=IntField(required=True) result=IntField(required=True) def insert(self): self.save() 1.get_or_create 1cimc_message.objects.get_or_create(msg_from='admin',to='xiaoming',msg_type='type1',status=0,defaults=msg_dic) 找到匹配条件就返回该条数据，否则就插入defaults，defaults为字典 2.delete 1cimc_message.objects(msg_from='admin',to='xiaoming',msg_type='type1',status=0).delete() 其他的举一反三，边用边找","categories":[{"name":"Database","slug":"Database","permalink":"http://enjoyhot.github.io/categories/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://enjoyhot.github.io/tags/Database/"},{"name":"Pymongo","slug":"Pymongo","permalink":"http://enjoyhot.github.io/tags/Pymongo/"},{"name":"Mongoengine","slug":"Mongoengine","permalink":"http://enjoyhot.github.io/tags/Mongoengine/"}]},{"title":"linux命令操作学习（摘自鸟哥私房菜基础篇）","slug":"linux-common-command","date":"2014-11-24T11:37:00.000Z","updated":"2017-04-16T14:48:30.650Z","comments":true,"path":"2014/11/24/linux-common-command/","link":"","permalink":"http://enjoyhot.github.io/2014/11/24/linux-common-command/","excerpt":"一、关于电源reboot 重启logout 注销root 注销后登录","text":"一、关于电源reboot 重启logout 注销root 注销后登录 二、vi管理器编译java程序1.输入vi hello.java2.输入i，进入insert模式3.输入 esc，进入命令模式4.输入 :wq，保存并退出 :q!，退出不保存5.编译 javac hello.java6.运行 java hello简单了解vi自带编辑器后，可以用更强大的编辑器vim，ubuntu中自带，用起来更顺手 三、简单命令1、ls 查看当前目录有什么，ls -l以列的方式查看明细，ls -al列出隐藏的.开头系统文件2、cal 10 2014 查看日期3、man date 查看date 的用法4、在root（不用）权限下改变文件的拥有者，群组 1、chgrp user install.log 改变群组为user，前提为 /etc/group里有user 2、chown root:root install.log 同时改变拥有者和群组为root，前提为 /etc/passwd里有root 主要用在 cp install.log install.logcp 复制操作后，install.logcp的权限什么的都没改 3、chmod 777 install.log 改变权限 r=4 w=2 x=1或者 chmod u=rwx,go=rx install.log 4、chmod a+w install.log 表示使所有都增加写的权限 5、su gugugugjiawei 切换身份6、mkdir testing 建立新目录 mkdir -p tmp1/tmp2 建立空目录tmp1,再建立空目录tmp2于tmp1下 mkdir -m 711 tmp11 指定目录下新建的tmp11的权限为711 rmdir tmp 删除当前目录下的空目录tmp,如果空目录下有tmp1，可以使用rmkir tmp/tmp1，再使用rmdir tmp，或者使用rmdir rm -r tmp 不管是不是空目录，删除掉！ rmdir -p tmp1/tmp2/tmp3 如果tmp3是空目录，先删除tmp3，如果tmp2是空目录，继续删，如果不是，提示并停止 rmdir tmp1/tmp2/tmp3 就只删tmp37、cd ../回到上级目录 8、pwd 显示当前目录 pwd -P 显示正确的路径，即连接档会被连接到什么地方去的完整路径9、nano mytext.txt10、当打开a.txt中文出现乱码可以使用命令 iconv -f gb2312 -t utf-8 a.txt&gt;ab.txt 传输件：PSCP.EXE 传输文件目录 用户名@主机地址：接受文件目录11、mv重命名或移动，cp为复制 mv newtmp movetmp 重命名为movetmp12、bc 计算器13、df 看看是否挂载14、压缩与解压缩 zcat是读取解压后的文件的内容并显示在屏幕上，但没有解压（因为cat可以读取纯文本）gzip newtxt.txt&gt;newtxt.txt.gz 表示newtxt.txt压缩为newtxt.txt.gz，然后再输入n即可保留两个然而，bzip2更优秀，用法类似，不过要压缩的话是bzip2 -z filename 注意：gzip 和bzip2不能压缩文件夹，即不能打包此时就用到了tar 不同点：要自定义档名；小写-p保留权限，建议加上去。 大写-P会加入完整路径，解压后回到原来的地方，可能会导致覆盖原来的文件造成后果。压缩后保留原来文件以后这样写 压缩：tar -jpcv -f filename.tar.bz2 filename 解压：tar -jxv -f filename.tar.bz215.vi编辑器详解 一般编辑模式：（注意a的妙用） L(H) 下(上)一个字符 P换行 u复原上一次动作 . 重复上次操作 3j表示向下跳三行 yy是复制一行 p是粘贴到下一行 要是不小心换行了解决方法：切换为一般模式（esc）+回到上一行（k）+合并下一行（大写J） + delete+end + a 16、关于字体编码装换（P351）","categories":[{"name":"Linux","slug":"Linux","permalink":"http://enjoyhot.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://enjoyhot.github.io/tags/Linux/"}]},{"title":"浅谈Virtual Box下，ubuntu系统能否被外网访问的问题","slug":"ubuntu-internet","date":"2014-11-21T10:41:00.000Z","updated":"2017-04-16T14:48:30.686Z","comments":true,"path":"2014/11/21/ubuntu-internet/","link":"","permalink":"http://enjoyhot.github.io/2014/11/21/ubuntu-internet/","excerpt":"写写记记，不然有时会寻寻觅觅，纠结很久~ 1、Host-Only模式下外网访问不了（因为没有端口映射），虚拟机能上网，主机能利用局域网分配给虚拟机的ip进行访问（ping）","text":"写写记记，不然有时会寻寻觅觅，纠结很久~ 1、Host-Only模式下外网访问不了（因为没有端口映射），虚拟机能上网，主机能利用局域网分配给虚拟机的ip进行访问（ping） 注意：启动服务器记得加上0.0.0.0:port，有时候访问不了服务器，点击右上角的设置查看ip和ifconfig的ip，如果不一样，弄到一样为止，重启或者其他reset操作什么的。 2、网络地址转换(NAT)通过端口映射可以使局域网内的所有计算机可以访问虚拟机中的服务器，手机连接wifi亲测可用（前提是主机ip设置为空），不过不知道非局域网怎样，不过坑爹的校园网是不行的。。。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://enjoyhot.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://enjoyhot.github.io/tags/Linux/"},{"name":"virtual box","slug":"virtual-box","permalink":"http://enjoyhot.github.io/tags/virtual-box/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://enjoyhot.github.io/tags/Ubuntu/"}]},{"title":"windows下eclipseNDK开发兼调试环境配置        ","slug":"ndk-setup","date":"2014-11-21T10:35:00.000Z","updated":"2017-04-16T14:48:30.662Z","comments":true,"path":"2014/11/21/ndk-setup/","link":"","permalink":"http://enjoyhot.github.io/2014/11/21/ndk-setup/","excerpt":"windows下eclipseNDK开发兼调试环境配置 NDK-r7版本及之后windows下开发NDK开发更加方便。 PS：该环境并没有配置CDT、cygwin、Mingw，因为现在的adt已经有c/c++开发的相关配置了，而ndk又集成了linux的编译环境，装太多东西不保证能成功。","text":"windows下eclipseNDK开发兼调试环境配置 NDK-r7版本及之后windows下开发NDK开发更加方便。 PS：该环境并没有配置CDT、cygwin、Mingw，因为现在的adt已经有c/c++开发的相关配置了，而ndk又集成了linux的编译环境，装太多东西不保证能成功。 原文链接：http://my.oschina.net/u/262922/blog/301513 配置流程如下： 新建一个android工程，等下也不要转为c/c++工程 以下根据具体情况去设置下Application.mk: 以下根据具体情况去设置下 Application.mk: 1234567891011APP_STL := gnustl_staticAPP_CFLAGS += -fexceptionsAPP_CPPFLAGS +=-std=c++11APP_CPPFLAGS +=-fpermissiveAPP_OPTIM := debugAPP_PLATFORM := android-8 Android.mk:1234567891011121314151617181920212223242526272829303132LOCAL_PATH := $(call my-dir)include $(CLEAR_VARS)#APP_STL := gnustl_staticLOCAL_MODULE := WebServiceLOCAL_SRC_FILES := com_http_WebService.cpp \\ MessageProxy.cpp \\ MsgBase.cpp \\ NetQueue.cpp \\ SendOperation.cpp \\ ThreadControl.cppLOCAL_STATIC_LIBRARIES := stdc++LOCAL_C_INCLUDES := $(LOCAL_PATH)/includeLOCAL_LDLIBS := -L$(LOCAL_PATH)/lib -lcurlLOCAL_LDLIBS += -lloginclude $(BUILD_SHARED_LIBRARY) Manifest.xml中application可能要加入才能调试： android:debuggable=”true” 如果要从Android调试到C/C++代码，需要再MainActivity.java的onCreate中加入 android.os.Debug.waitForDebugger(); 然后右键—&gt;Debug As—&gt;Android Native Application开始调试","categories":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/tags/Android/"},{"name":"NDK","slug":"NDK","permalink":"http://enjoyhot.github.io/tags/NDK/"}]},{"title":"android混淆与反射","slug":"android-reflect","date":"2014-11-21T10:32:00.000Z","updated":"2017-04-16T14:48:30.618Z","comments":true,"path":"2014/11/21/android-reflect/","link":"","permalink":"http://enjoyhot.github.io/2014/11/21/android-reflect/","excerpt":"android混淆与反射：Android包4.0以后混淆更加简单 一、配置1、在project.properties.txt将以下内容注释去掉proguard.config=${sdk.dir}/tools/proguard/proguard-android.txt:proguard-project.txt","text":"android混淆与反射：Android包4.0以后混淆更加简单 一、配置1、在project.properties.txt将以下内容注释去掉proguard.config=${sdk.dir}/tools/proguard/proguard-android.txt:proguard-project.txt 2、在proguard-project中加入取消混淆的声明：需要混淆的jar包所依赖的jar包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148-libraryjars 'F:\\eclipse\\adt-bundle-windows-x86-20130917\\sdk\\platforms\\android-19\\android.jar'-optimizationpasses 5# 混淆时不会产生形形色色的类名-dontusemixedcaseclassnames# 指定不去忽略非公共的库类-dontskipnonpubliclibraryclasses# 不预校验-dontpreverify-verbose# 优化-optimizations !code/simplification/arithmetic,!field/*,!class/merging/*# 保留了继承自Activity、Application、Service、BroadcastReceiver、ContentProvider、BackupAgentHelper、Preference和ILicensingService 的子类。因为这些子类，都是可能被外部调用的。-keep public class * extends android.app.Activity-keep public class * extends android.app.Application-keep public class * extends android.app.Service-keep public class * extends android.content.BroadcastReceiver-keep public class * extends android.content.ContentProvider-keep public class * extends android.app.backup.BackupAgentHelper-keep public class * extends android.preference.Preference-keep public class com.android.vending.licensing.ILicensingService-keep public class * extends android.support.v4.app.FragmentActivity-keep public class * extends android.support.v4.app.Fragmen# 保留了含有native方法的类-keepclasseswithmembernames class * &#123; native &lt;methods&gt;;&#125;# actionbarSherlock 这里要注意了-keep class android.support.v4.app.** &#123; *; &#125;-keep interface android.support.v4.app.** &#123; *; &#125;-keep class com.actionbarsherlock.** &#123; *; &#125;-keep interface com.actionbarsherlock.** &#123; *; &#125;-keepattributes *Annotation*# 保留构造函数从xml构造的类（一般为View的子类）-keepclasseswithmembernames class * &#123; public &lt;init&gt;(android.content.Context, android.util.AttributeSet);&#125;# 保留构造函数从xml构造的类（一般为View的子类）-keepclasseswithmembernames class * &#123; public &lt;init&gt;(android.content.Context, android.util.AttributeSet, int);&#125;# 保护指定类的成员，如果此类受到保护他们会保护的更好-keepclassmembers class * extends android.app.Activity &#123; public void *(android.view.View);&#125;# 保留枚举类型中的values和valueOf静态方法-keepclassmembers enum * &#123; public static **[] values(); public static ** valueOf(java.lang.String);&#125;# 保留继承Parcelable的跨进程数据类-keep class * implements android.os.Parcelable &#123; public static final android.os.Parcelable$Creator *;&#125;# 保留反射中用到的类和方法，到时根据具体情况再改# 反编译测试的时候有效，运行时也与未混淆情况一样-keepclassmembers class 包名.** &#123; public *; protected *; private *;&#125; 3、工程中含有第三方jar包，或者.so文件 混淆处理。 -libraryjars libs/apns_1.0.6.jar -libraryjars libs/armeabi/libBaiduMapSDK_v2_3_1.so -libraryjars libs/armeabi/liblocSDK4.so -libraryjars libs/baidumapapi_v2_3_1.jar -libraryjars libs/core.jar -libraryjars libs/gesture-imageview.jar -libraryjars libs/gson-2.0.jar 4、 其它反射调用的java类与方法使用反射时一定要注意proguard可能会认为那些方法未被调用过, 会在代码优化过程中将它们改名或除去. 在使用反射的地方一定要在proguard-project.txt中配置, 不优化反射调用过的类和方法 下面开始打包（不打包你就算设置了其实也没有混淆，不信可以解混淆试试，能成功解混淆） 二、在jdk的bin目录下生成.keystore证书，输入相关资料，记住口令。这里有图，看不到而已，到很久之前的博客可以看到。 三、右键—&gt;Android Tools—&gt;Export signed Application package这里有图，看不到而已，到很久之前的博客可以看到。","categories":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/tags/Android/"},{"name":"Reflect","slug":"Reflect","permalink":"http://enjoyhot.github.io/tags/Reflect/"}]},{"title":"AndroidNDK开发之JNI学习        ","slug":"ndk-study","date":"2014-11-21T10:30:00.000Z","updated":"2017-04-16T14:48:30.662Z","comments":true,"path":"2014/11/21/ndk-study/","link":"","permalink":"http://enjoyhot.github.io/2014/11/21/ndk-study/","excerpt":"androidNDK开发之JNI学习 1、在工程目录下编译1javah -classpath F:\\eclipse\\adt-bundle-windows-x86-201309017\\sdk\\platforms\\android-19\\android.jar;bin/classes com.example.jnithread.MainActivity","text":"androidNDK开发之JNI学习 1、在工程目录下编译1javah -classpath F:\\eclipse\\adt-bundle-windows-x86-201309017\\sdk\\platforms\\android-19\\android.jar;bin/classes com.example.jnithread.MainActivity 2、增加log信息的方法 在.c文件中添加 12345#define LOG_TAG \"System.out.cpp\"#define LOGD(...) __android_log_print(ANDROID_LOG_DEBUG,LOG_TAG,__VA_ARGS__)#define LOGI(...) __android_log_print(ANDROID_LOG_INFO,LOG_TAG,__VA_ARGS__) 在配置mk文件增加 123#liblog.so libGLESv2.soLOCAL_LDLIBS += -llog e.g 1LOGD(\"x=%d\",x); 3、类型转换 4、对象传递机制 假如类为c/c++类，并开始在c/c++程序建立： 返回对象以jlong 类型强制转换对象的指针值，从而将c/c++的对象返回到java中以long变量保存起来，以便后续工作回调c/c++中的对象。 e.g123456789dataInCpp * data1=new dataIntCpp();return (jlong)data1;////////////////////////////////////////jlong data1;dataInCpp * data2 = (dataInCpp *)data1; 假如类为java类，通过c/c++程序创建 A、在c++中创建并返回给java程序12345678910111213141516171819202122232425Jclass clazz = env-&gt;FindClass(“com/example/jnithread/myData”);If(!clazz)&#123;return NULL;&#125;jmethodID constr = env-&gt;GetMethodID(clazz,”&lt;init&gt;”,”()V”);if(!constr)&#123;return;&#125;jobject newObj = env-&gt;NewObject(clazz,constr);if(!newObj)&#123;return;&#125; return newObj; 在java程序中直接用类型myData接收即可，无需用Object B、将c++中创建的java对象再传回给c++方法1jclass clazz = env-&gt;GetObjectClass(dataObject); 接下来和调用java类中函数一样如下所述 5、简单说明一下c++调用java函数：非静态方法调用12345678910111213141516171819202122232425262728293031jclass dpclazz = env-&gt;FindClass(”com/example/jnithreads/MainActivity”);if(dpclazz==0)&#123;LOGI(“find class error”);return;&#125;//去得到函数的信息 javap –s com.example.jnithreads.MainActivityjmethod method1 = env-&gt;GetMethodID(dpclazz,”getNum”,(参数)V);//获取非静态方法的ID//V代表返回类型为void,也可(II)I,表示接收两个int方法返回int类型//如果是String类型的参数写为(Ljava/lang/String;),如果返回类型为byte数组则写为[B If(method1==0)&#123;LOGI(“find method1 error”);Return;&#125;env-&gt;CallVoidMethod(obj,method1);//Void表示返回类型为void//int result = env-&gt;CallVoidMethod(obj,method1,3,5); 后面为参数 注： 1/返回类型定义 2/如果要调用类似【”hello”.getBytes(“gb2312”);】则需要1env-&gt;CallVoidMethod(obj,”hello”,method1,”gb2312”);//”~”要先转换为jstring 3/ byteArray-&gt;char *123456789jsize len = env-&gt;GetArrayLength(byteArray);jbyte* ba = env-&gt;GetByteArrayElements(byteArray,JNI_FALSE);if(len&gt;0)&#123; char * rtn = (char *)malloc(len+1); memcpy(rtn,ba,len); rth[len]=0;&#125;env-&gt;ReleaseByteArrayElements(byteArray,ba,0);//释放掉内存空间return rtn; 4/获取java对象的ID为:12jfieldID field_a=env-&gt;GetFieldID(dpclazz,”a”,”I”);int ma=(int)env-&gt;GetObjectField(obj,field_a);//调用以获得值 静态方法调用： 获取非静态方法的ID：1jmethod methodID = env-&gt;GetStaticMethodID(dpclazz,”getNum”,(参数)V); 调用静态方法：1env-&gt;CallStaticVoidMethod(dpclazz,mthodID,参数) 熟记：get 参数 clazz call参数 obj,methodID 如果是static的话，call前面obj改为clazz 6、全局变量 Jobject的子类别包括jclass、jstring,、jarray(这些类称为局部性的对象参考)，他们就算定义成static，也要利用函数：1gObj = env-&gt;NewGlobalRef(obj);将其赋值从而保持全局性 7、JNI线程模式的调用8、利用C++回调来更新UI(Handler) 同步锁：(java调用c++) 1、定义的时候：1private native synchronized String execute(Object oSync); 进入函数资源调用时：1env-&gt;MonitorEnter(syncObj); 释放时：1env-&gt;MonitorExit(syncObj); 上面讲的是不同线程情况下将对象传入C++程序，所以还是与线程安全有关 如果是在c++程序中的多线程，也一样。 多线程注意：12345if (0 == gVm-&gt;AttachCurrentThread(&amp;env, NULL))&#123;gVm-&gt;DetachCurrentThread();……&#125;","categories":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/tags/Android/"},{"name":"NDK","slug":"NDK","permalink":"http://enjoyhot.github.io/tags/NDK/"}]},{"title":"关于django的几篇文章","slug":"django-csdn","date":"2014-11-21T10:28:00.000Z","updated":"2017-04-16T14:48:30.626Z","comments":true,"path":"2014/11/21/django-csdn/","link":"","permalink":"http://enjoyhot.github.io/2014/11/21/django-csdn/","excerpt":"摘自CSDN，不小心把源文件删了，就不直接贴了","text":"摘自CSDN，不小心把源文件删了，就不直接贴了 django分页技术Paginator（进阶篇）【python web学习】python web窥探django分页技术django-pagination和Paginator（基础篇）ubuntu下发布Django Web的两种方法django之数据库(sqlite3, mongoDB)应用Django Web开发环境的搭建（二）Django Web开发环境的搭建（一）","categories":[{"name":"Django","slug":"Django","permalink":"http://enjoyhot.github.io/categories/Django/"}],"tags":[{"name":"CSDN","slug":"CSDN","permalink":"http://enjoyhot.github.io/tags/CSDN/"},{"name":"Django","slug":"Django","permalink":"http://enjoyhot.github.io/tags/Django/"}]},{"title":"android利用ListView滑动刷新        ","slug":"android-listview","date":"2014-11-21T10:28:00.000Z","updated":"2017-04-16T14:48:30.614Z","comments":true,"path":"2014/11/21/android-listview/","link":"","permalink":"http://enjoyhot.github.io/2014/11/21/android-listview/","excerpt":"滑动刷新","text":"滑动刷新 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package com.laohuai.appdemo.customui;import java.util.ArrayList;import java.util.List;import com.laohuai.appdemo.customui.ui.MyListView;import com.laohuai.appdemo.customui.ui.MyListView.OnRefreshListener;import android.app.Activity;import android.os.AsyncTask;import android.os.Bundle;import android.view.View;import android.view.ViewGroup;import android.widget.BaseAdapter;import android.widget.TextView;public class MainActivity extends Activity &#123; private List&lt;String&gt; data; private BaseAdapter adapter; public void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.main); data = new ArrayList&lt;String&gt;(); data.add(\"a\"); data.add(\"b\"); data.add(\"c\"); data.add(\"a\"); data.add(\"b\"); final MyListView listView = (MyListView) findViewById(R.id.listView); adapter = new BaseAdapter() &#123; public View getView(int position, View convertView, ViewGroup parent) &#123; TextView tv = new TextView(getApplicationContext()); tv.setText(data.get(position)); return tv; &#125; public long getItemId(int position) &#123; return 0; &#125; public Object getItem(int position) &#123; return null; &#125; public int getCount() &#123; return data.size(); &#125; &#125;; listView.setAdapter(adapter); listView.setonRefreshListener(new OnRefreshListener() &#123; public void onRefresh() &#123; new AsyncTask&lt;Void, Void, Void&gt;() &#123; protected Void doInBackground(Void... params) &#123; try &#123; Thread.sleep(3000);//刷新3秒 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; data.add(\"刷新后添加的内容\"); return null; &#125; @Override protected void onPostExecute(Void result) &#123; adapter.notifyDataSetChanged(); listView.onRefreshComplete(); &#125; &#125;.execute(null,null,null); &#125; &#125;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404package com.laohuai.appdemo.customui.ui;import java.util.Date;import com.laohuai.appdemo.customui.R;import android.content.Context;import android.util.AttributeSet;import android.util.Log;import android.view.LayoutInflater;import android.view.MotionEvent;import android.view.View;import android.view.ViewGroup;import android.view.animation.LinearInterpolator;import android.view.animation.RotateAnimation;import android.widget.AbsListView;import android.widget.BaseAdapter;import android.widget.ImageView;import android.widget.LinearLayout;import android.widget.ListView;import android.widget.AbsListView.OnScrollListener;import android.widget.ProgressBar;import android.widget.TextView;public class MyListView extends ListView implements OnScrollListener &#123; private static final String TAG = \"listview\"; private final static int RELEASE_To_REFRESH = 0; private final static int PULL_To_REFRESH = 1; private final static int REFRESHING = 2; private final static int DONE = 3; private final static int LOADING = 4; // 实际的padding的距离与界面上偏移距离的比例 private final static int RATIO = 3; private LayoutInflater inflater; //一个线性布局类对象，设置一个可选布局区域 private LinearLayout headView; private TextView tipsTextview; private TextView lastUpdatedTextView; private ImageView arrowImageView; private ProgressBar progressBar; private RotateAnimation animation; private RotateAnimation reverseAnimation; // 用于保证startY的值在一个完整的touch事件中只被记录一次 private boolean isRecored; private int headContentWidth; private int headContentHeight; private int startY; private int firstItemIndex; private int state; private boolean isBack; private OnRefreshListener refreshListener; private boolean isRefreshable; //新建时自动调用构造函数 public MyListView(Context context) &#123; super(context); init(context); &#125; //新建时自动调用构造函数 public MyListView(Context context, AttributeSet attrs) &#123; super(context, attrs); init(context); &#125; private void init(Context context) &#123; //拖动时背景黑色设置 setCacheColorHint(context.getResources().getColor(R.color.transparent)); //---------设置项目前面的显示--------- inflater = LayoutInflater.from(context); headView = (LinearLayout) inflater.inflate(R.layout.head, null); //获取箭头对象 arrowImageView = (ImageView) headView .findViewById(R.id.head_arrowImageView); arrowImageView.setMinimumWidth(70); arrowImageView.setMinimumHeight(50); //进度条 progressBar = (ProgressBar) headView .findViewById(R.id.head_progressBar); //下拉刷新、上拉刷新 tipsTextview = (TextView) headView.findViewById(R.id.head_tipsTextView); //最近更新 lastUpdatedTextView = (TextView) headView .findViewById(R.id.head_lastUpdatedTextView); measureView(headView); headContentHeight = headView.getMeasuredHeight(); headContentWidth = headView.getMeasuredWidth(); //和父类比较，（上下左右）的view之间的距离 headView.setPadding(0, -1 * headContentHeight, 0, 0); headView.invalidate(); Log.v(\"size\", \"width:\" + headContentWidth + \" height:\" + headContentHeight); //下面两句极其重要，一方面将listView和headView绑定，一方面监听操作 addHeaderView(headView, null, false); setOnScrollListener(this); //下面没看懂，待查 animation = new RotateAnimation(0, -180, RotateAnimation.RELATIVE_TO_SELF, 0.5f, RotateAnimation.RELATIVE_TO_SELF, 0.5f); animation.setInterpolator(new LinearInterpolator()); animation.setDuration(250); animation.setFillAfter(true); reverseAnimation = new RotateAnimation(-180, 0, RotateAnimation.RELATIVE_TO_SELF, 0.5f, RotateAnimation.RELATIVE_TO_SELF, 0.5f); reverseAnimation.setInterpolator(new LinearInterpolator()); reverseAnimation.setDuration(200); reverseAnimation.setFillAfter(true); state = DONE; isRefreshable = false; &#125; public void onScroll(AbsListView arg0, int firstVisiableItem, int arg2, int arg3) &#123; //滚动时一直回调，直到停止滚动时才停止回调。单击时不回调。刷新 //onTouchEvent调用后总会回调一次 //firstVisibleItem：当前能看见的第一个列表项ID（从0开始），总是0 //arg2：当前能看见的列表项个数（小半个也算）listView个数 //arg3：列表项共数,listView个数 //Log.d(TAG,\"firstItemIndex:\"+firstItemIndex); firstItemIndex = firstVisiableItem; &#125; //点击或滑动都会触发 public void onScrollStateChanged(AbsListView arg0, int arg1) &#123; &#125; public boolean onTouchEvent(MotionEvent event) &#123; if (isRefreshable) &#123; switch (event.getAction()) &#123; case MotionEvent.ACTION_DOWN: if (firstItemIndex == 0 &amp;&amp; !isRecored) &#123; isRecored = true; startY = (int) event.getY(); Log.v(TAG, \"在down时候记录当前位置‘\"); &#125; break; case MotionEvent.ACTION_UP: if (state != REFRESHING &amp;&amp; state != LOADING) &#123; if (state == DONE) &#123; // 什么都不做 &#125; if (state == PULL_To_REFRESH) &#123; state = DONE; changeHeaderViewByState(); //有下拉但没刷新 Log.v(TAG, \"由下拉刷新状态，到done状态\"); &#125; if (state == RELEASE_To_REFRESH) &#123; state = REFRESHING; //因为异步执行，所以下面两句一起进行，由onRefresh决定什么时候改变状态， //因为会调用onRefreshComplete() changeHeaderViewByState(); onRefresh(); //有下拉并且刷新 Log.v(TAG, \"由松开刷新状态，到done状态\"); &#125; &#125; isRecored = false; isBack = false; break; //当触摸时移动操作时调用 case MotionEvent.ACTION_MOVE: int tempY = (int) event.getY(); if (!isRecored &amp;&amp; firstItemIndex == 0) &#123; Log.v(TAG, \"在move时候记录下位置\"); isRecored = true; startY = tempY; &#125; if (state != REFRESHING &amp;&amp; isRecored &amp;&amp; state != LOADING) &#123; // 保证在设置padding的过程中，当前的位置一直是在head，否则如果当列表超出屏幕的话，当在上推的时候，列表会同时进行滚动 // 可以松手去刷新了 if (state == RELEASE_To_REFRESH) &#123; setSelection(0); // 往上推了，推到了屏幕足够掩盖head的程度，但是还没有推到全部掩盖的地步 if (((tempY - startY) / RATIO &lt; headContentHeight) &amp;&amp; (tempY - startY) &gt; 0) &#123; state = PULL_To_REFRESH; changeHeaderViewByState(); Log.v(TAG, \"由松开刷新状态转变到下拉刷新状态\"); &#125; // 一下子推到顶了 else if (tempY - startY &lt;= 0) &#123; state = DONE; changeHeaderViewByState(); Log.v(TAG, \"由松开刷新状态转变到done状态\"); &#125; // 往下拉了，或者还没有上推到屏幕顶部掩盖head的地步 else &#123; // 不用进行特别的操作，只用更新paddingTop的值就行了 &#125; &#125; // 还没有到达显示松开刷新的时候,DONE或者是PULL_To_REFRESH状态 if (state == PULL_To_REFRESH) &#123; setSelection(0); // 下拉到可以进入RELEASE_TO_REFRESH的状态 if ((tempY - startY) / RATIO &gt;= headContentHeight) &#123; state = RELEASE_To_REFRESH; isBack = true; changeHeaderViewByState(); Log.v(TAG, \"由done或者下拉刷新状态转变到松开刷新\"); &#125; // 上推到顶了 else if (tempY - startY &lt;= 0) &#123; state = DONE; changeHeaderViewByState(); Log.v(TAG, \"由DOne或者下拉刷新状态转变到done状态\"); &#125; &#125; // done状态下 if (state == DONE) &#123; if (tempY - startY &gt; 0) &#123; state = PULL_To_REFRESH; changeHeaderViewByState(); &#125; &#125; // 更新headView的size if (state == PULL_To_REFRESH) &#123; headView.setPadding(0, -1 * headContentHeight + (tempY - startY) / RATIO, 0, 0); &#125; // 更新headView的paddingTop if (state == RELEASE_To_REFRESH) &#123; headView.setPadding(0, (tempY - startY) / RATIO - headContentHeight, 0, 0); &#125; &#125; break; &#125; &#125; return super.onTouchEvent(event); &#125; // 当状态改变时候，调用该方法，以更新界面 private void changeHeaderViewByState() &#123; switch (state) &#123; case RELEASE_To_REFRESH: arrowImageView.setVisibility(View.VISIBLE); progressBar.setVisibility(View.GONE); tipsTextview.setVisibility(View.VISIBLE); lastUpdatedTextView.setVisibility(View.VISIBLE); arrowImageView.clearAnimation(); //方向 arrowImageView.startAnimation(animation); tipsTextview.setText(\"松开刷新\"); Log.v(TAG, \"当前状态，松开刷新\"); break; case PULL_To_REFRESH: progressBar.setVisibility(View.GONE); tipsTextview.setVisibility(View.VISIBLE); lastUpdatedTextView.setVisibility(View.VISIBLE); arrowImageView.clearAnimation(); arrowImageView.setVisibility(View.VISIBLE); // 是由RELEASE_To_REFRESH状态转变来的 if (isBack) &#123; isBack = false; arrowImageView.clearAnimation(); //方向 arrowImageView.startAnimation(reverseAnimation); tipsTextview.setText(\"下拉刷新\"); &#125; else &#123; tipsTextview.setText(\"下拉刷新\"); &#125; Log.v(TAG, \"当前状态，下拉刷新\"); break; case REFRESHING: headView.setPadding(0, 0, 0, 0); progressBar.setVisibility(View.VISIBLE); arrowImageView.clearAnimation(); arrowImageView.setVisibility(View.GONE); tipsTextview.setText(\"正在刷新...\"); lastUpdatedTextView.setVisibility(View.VISIBLE); Log.v(TAG, \"当前状态,正在刷新...\"); break; case DONE: headView.setPadding(0, -1 * headContentHeight, 0, 0); progressBar.setVisibility(View.GONE); arrowImageView.clearAnimation(); arrowImageView.setImageResource(R.drawable.arrow); tipsTextview.setText(\"下拉刷新\"); lastUpdatedTextView.setVisibility(View.VISIBLE); Log.v(TAG, \"当前状态，done\"); break; &#125; &#125; public void setonRefreshListener(OnRefreshListener refreshListener) &#123; this.refreshListener = refreshListener; isRefreshable = true; &#125; public interface OnRefreshListener &#123; public void onRefresh(); &#125; public void onRefreshComplete() &#123; state = DONE; lastUpdatedTextView.setText(\"最近更新:\" + new Date().toLocaleString()); changeHeaderViewByState(); &#125; private void onRefresh() &#123; if (refreshListener != null) &#123; refreshListener.onRefresh(); &#125; &#125; // 此方法直接照搬自网络上的一个下拉刷新的demo，此处是“估计”headView的width以及height //1.static int getMode(int measureSpec):根据提供的测量值(格式)提取模式(上述三个模式之一) //2.static int getSize(int measureSpec):根据提供的测量值(格式)提取大小值(这个大小也就是我们通常所说的大小) //3.static int makeMeasureSpec(int size,int mode):根据提供的大小值和模式创建一个测量值(格式) private void measureView(View child) &#123; ViewGroup.LayoutParams p = child.getLayoutParams(); if (p == null) &#123; p = new ViewGroup.LayoutParams(ViewGroup.LayoutParams.FILL_PARENT, ViewGroup.LayoutParams.WRAP_CONTENT); &#125; int childWidthSpec = ViewGroup.getChildMeasureSpec(0, 0 + 0, p.width); int lpHeight = p.height; //高度格式 // 一个MeasureSpec封装了父布局传递给子布局的布局要求 int childHeightSpec; if (lpHeight &gt; 0) &#123; //EXACTLY(完全)，父元素决定子元素的确切大小，子元素将被限定在给定的边界里而忽略它本身大小 childHeightSpec = MeasureSpec.makeMeasureSpec(lpHeight, MeasureSpec.EXACTLY); Log.d(TAG,\"lpHeight:\"+ lpHeight); &#125; else &#123; //UNSPECIFIED(未指定),在高度上父元素不对子元素施加任何束缚 childHeightSpec = MeasureSpec.makeMeasureSpec(0, MeasureSpec.UNSPECIFIED); //测试的时候发现是-2 Log.d(TAG,\"lpHeight:\"+ lpHeight); &#125; //宽度保持不变 //测试的时候childWidthSpec, childHeightSpec都是0 child.measure(childWidthSpec, childHeightSpec); &#125; public void setAdapter(BaseAdapter adapter) &#123; lastUpdatedTextView.setText(\"最近更新:\" + new Date().toLocaleString()); super.setAdapter(adapter); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;!-- ListView的头部 --&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" &gt; &lt;!-- 内容 --&gt; &lt;RelativeLayout android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:id=\"@+id/head_contentLayout\" android:paddingLeft=\"30dp\" &gt; &lt;!-- 箭头图像、进度条 --&gt; &lt;FrameLayout android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_alignParentLeft=\"true\" android:layout_centerVertical=\"true\" &gt; &lt;!-- 箭头 --&gt; &lt;ImageView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_gravity=\"center\" android:src=\"@drawable/arrow\" android:id=\"@+id/head_arrowImageView\" /&gt; &lt;!-- 进度条 --&gt; &lt;ProgressBar android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" style=\"?android:attr/progressBarStyleSmall\" android:layout_gravity=\"center\" android:id=\"@+id/head_progressBar\" android:visibility=\"gone\" /&gt; &lt;/FrameLayout&gt; &lt;!-- 提示、最近更新 --&gt; &lt;LinearLayout android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_centerHorizontal=\"true\" android:orientation=\"vertical\" android:gravity=\"center_horizontal\" &gt; &lt;!-- 提示 --&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:text=\"下拉刷新\" android:textColor=\"@color/white\" android:textSize=\"20sp\" android:id=\"@+id/head_tipsTextView\" /&gt; &lt;!-- 最近更新 --&gt; &lt;TextView android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:id=\"@+id/head_lastUpdatedTextView\" android:text=\"最近更新\" android:textColor=\"@color/gold\" android:textSize=\"10sp\" /&gt; &lt;/LinearLayout&gt; &lt;/RelativeLayout&gt;&lt;/LinearLayout&gt; 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"fill_parent\" android:layout_height=\"fill_parent\" android:background=\"#000066\" android:orientation=\"vertical\" &gt; &lt;com.laohuai.appdemo.customui.ui.MyListView android:layout_width=\"fill_parent\" android:layout_height=\"fill_parent\" android:id=\"@+id/listView\" /&gt;&lt;/LinearLayout&gt;","categories":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/tags/Android/"},{"name":"Listview","slug":"Listview","permalink":"http://enjoyhot.github.io/tags/Listview/"}]},{"title":"利用简易Tomcat服务器结合MysqL实现Android手机注册与登录(服务器部分)        ","slug":"tomcat-server","date":"2014-11-21T10:27:00.000Z","updated":"2017-04-16T14:48:30.686Z","comments":true,"path":"2014/11/21/tomcat-server/","link":"","permalink":"http://enjoyhot.github.io/2014/11/21/tomcat-server/","excerpt":"一、概述1、服务器在本地搭建，利用Tomcat运行servlet容器2、继承于httpServlet类的自定义实现与客户端的交互，结合MysqL，jdbc知识3、客户端实现简单的注册和登录验证","text":"一、概述1、服务器在本地搭建，利用Tomcat运行servlet容器2、继承于httpServlet类的自定义实现与客户端的交互，结合MysqL，jdbc知识3、客户端实现简单的注册和登录验证注意点：1、web.xml里入口定义要正确，否则登录客户端连接不到2、当在Tomcat中运行时，需要在WEB_INF文件夹中手动添加jdbc库文件，偶尔会在运行过后消失，要注意。（或者直接脱离Eclipse就不会有这个问题）3、sql语句要多测试，否则容易写错但执行不会报错。4、注意事先需启动MysqL。 二、代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package com;import java.io.IOException;import java.io.PrintWriter;import java.sql.*;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;&lt;!--more--&gt;public class JavaRegisterMysql extends HttpServlet&#123; private static final long serialVersionUID = 1L; private static final int NAME_CODE_RIGHT = 0; // private static final int CODE_WRONG = 1; // private static final int NAME_WRONG = 2; // String notename = null; String notepassword = null; String notephone = null; public JavaRegisterMysql()&#123; &#125; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; // TODO Auto-generated method stub if(req == null)&#123; return; &#125; resp.setContentType(\"text/html;charset=utf-8\"); req.setCharacterEncoding(\"utf-8\"); resp.setCharacterEncoding(\"utf-8\"); PrintWriter out = resp.getWriter(); String name = req.getParameter(\"NAME\"); String code = req.getParameter(\"CODE\"); String phone = req.getParameter(\"PHONE\"); String chose=req.getParameter(\"CHOSE\"); //手机客户端访问 int ret=-1; try &#123; ret = checkSubmit(name, code, phone, chose); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; out.print(ret); out.flush(); out.close(); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; // TODO Auto-generated method stub if(req == null)&#123; return; &#125; resp.setContentType(\"text/html;charset=utf-8\"); req.setCharacterEncoding(\"utf-8\"); resp.setCharacterEncoding(\"utf-8\"); PrintWriter out = resp.getWriter(); String name = req.getParameter(\"NAME\"); String code = req.getParameter(\"CODE\"); String phone = req.getParameter(\"PHONE\"); String chose=req.getParameter(\"CHOSE\"); int ret=-2; try &#123; ret = checkSubmit(name, code, phone,chose); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; out.print(ret); out.flush(); out.close(); &#125; /** * 判断登录名和密码 * @param name * @param code * @return * @throws Exception */ private int checkSubmit(String name, String code, String phone,String chose) throws Exception&#123; int ret = -1; //UserDAOProxy ab=new UserDAOProxy(); User user=new User(name,code,phone); ret=new UserDAOProxy().findLogin(user, chose); return ret; &#125;&#125; 主接收类 1234567891011121314151617181920212223242526272829package com;import java.sql.*;public class DatabaseConnection &#123; public static final String DRIVER = \"com.mysql.jdbc.Driver\"; public static final String URL = \"jdbc:mysql://localhost:3306/scut\"; public static final String USER = \"root\"; public static final String PASS = \"123456\"; private Connection con; public DatabaseConnection()&#123; try&#123; Class.forName(DRIVER); con = DriverManager.getConnection(URL,USER,PASS); &#125; catch(Exception e)&#123;&#125; &#125; public Connection getConnection()&#123; return con; &#125; public void close()&#123; try&#123; if(con!=null)&#123; con.close(); &#125; &#125; catch(Exception e)&#123;&#125; &#125;&#125; JDBC接口类 123456789101112131415package com;public class UserDAOProxy &#123; private DatabaseConnection dbc; private UserDAOImpl idao; public UserDAOProxy()&#123; dbc = new DatabaseConnection(); idao = new UserDAOImpl(dbc.getConnection()); &#125; public int findLogin(User user,String chose)throws Exception&#123; int flag = idao.findLogin(user,chose); dbc.close(); return flag; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com;import java.sql.*;public class UserDAOImpl &#123; private Connection con; private Statement stat; public UserDAOImpl(Connection con)&#123; this.con = con; &#125; public int findLogin(User user,String chose)throws Exception&#123; int flag = 0; //String sql = \"SELECT * FROM noteinfo WHERE name=\"+\"'\"+user.getName()+\"'\"; //String sql = \"SELECT * FROM noteinfo WHERE id=1\"; //String cd=user.getName(); //String sql = \"SELECT * FROM noteinfo WHERE name=\"+ \"'\"+user.getName()+\"'\" + // \" and password=\"+ \"'\"+user.getPassword()+\"'\"; try &#123; this.stat = con.createStatement(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; // 结果集 if(chose.equals(\"1\")) &#123; flag=1; String sql = \"SELECT * FROM noteinfo WHERE name=\"+ \"'\"+user.getName()+\"'\" + \" and password=\"+ \"'\"+user.getPassword()+\"'\"; //String sql2=\"insert into noteinfo values(null,'ggg','111','13412323234')\"; ResultSet rs = stat.executeQuery(sql); if(rs.next())&#123; user.setName(rs.getString(\"name\")); user.setPassword(rs.getString(\"password\")); flag = 2; &#125; //stat.execute(sql2); &#125; if(chose.equals(\"2\")) &#123; flag=3; String sql=\"insert into noteinfo values(null,\"+ \"'\"+user.getName()+ \"'\"+\",\"+ \"'\"+user.getPassword()+ \"'\"+\",\" + \"'\"+user.getPhone()+ \"')\"; stat.execute(sql); flag = 4; &#125; return flag; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435package com;public class User &#123; public User()&#123; this.name=\" \"; this.password=\" \"; this.phone=\" \"; &#125; public User(String name,String code,String phone)&#123; this.name=name; this.password=code; this.phone=phone; &#125; private String phone; private String name; private String password; public String getPhone()&#123; return phone; &#125; public void setPhone(String phone)&#123; this.phone = phone; &#125; public String getName()&#123; return name; &#125; public void setName(String name)&#123; this.name = name; &#125; public void setPassword(String password)&#123; this.password = password; &#125; public String getPassword()&#123; return password; &#125;&#125; 以上为相应调用类 12345678&lt;servlet&gt; &lt;servlet-name&gt;gjwServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.JavaRegisterMysql&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;gjwServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/login&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; web.xml需添加以上部分代码","categories":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/tags/Android/"},{"name":"tomcat","slug":"tomcat","permalink":"http://enjoyhot.github.io/tags/tomcat/"},{"name":"servlet","slug":"servlet","permalink":"http://enjoyhot.github.io/tags/servlet/"}]},{"title":"利用简易Tomcat服务器结合MysqL实现Android手机注册与登录(客户端部分)","slug":"tomcat-client","date":"2014-11-21T10:22:00.000Z","updated":"2017-04-16T14:48:30.686Z","comments":true,"path":"2014/11/21/tomcat-client/","link":"","permalink":"http://enjoyhot.github.io/2014/11/21/tomcat-client/","excerpt":"一、概述1、服务器在本地搭建，利用Tomcat运行servlet容器2、继承于httpServlet类的自定义实现与客户端的交互，结合MysqL，jdbc知识3、客户端实现简单的注册和登录验证","text":"一、概述1、服务器在本地搭建，利用Tomcat运行servlet容器2、继承于httpServlet类的自定义实现与客户端的交互，结合MysqL，jdbc知识3、客户端实现简单的注册和登录验证注意点：1、web.xml里入口定义要正确，否则登录客户端连接不到2、当在Tomcat中运行时，需要在WEB_INF文件夹中手动添加jdbc库文件，偶尔会在运行过后消失，要注意。（或者直接脱离Eclipse就不会有这个问题）3、sql语句要多测试，否则容易写错但执行不会报错。4、注意事先需启动MysqL。 二、代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117package com.example.java4androidmysql;import java.io.IOException;import java.io.UnsupportedEncodingException;import java.util.ArrayList;import java.util.List;import org.apache.http.HttpResponse;import org.apache.http.HttpStatus;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.HttpClient;import org.apache.http.client.entity.UrlEncodedFormEntity;import org.apache.http.client.methods.HttpPost;import org.apache.http.conn.params.ConnManagerParams;import org.apache.http.impl.client.DefaultHttpClient;import org.apache.http.message.BasicNameValuePair;import org.apache.http.params.BasicHttpParams;import org.apache.http.params.HttpConnectionParams;import org.apache.http.params.HttpParams;import org.apache.http.protocol.HTTP;import org.apache.http.util.EntityUtils;import android.os.AsyncTask;import android.os.Bundle;import android.app.Activity;import android.content.Intent;import android.view.Menu;import android.view.View;import android.view.Window;import android.widget.Button;import android.widget.EditText;import android.widget.Toast;public class MainActivity extends Activity &#123; public static final String URL = \"http://192.168.2.1:8080/JavaRegisterMysql/login\"; public static String res=\"\"; public static String name = \"\"; public static String code = \"\"; public static String phone =\" \"; public static String chose=\"0\"; Button connectButton=null; Button registerButton=null; EditText noteUser=null; EditText notePassword=null; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); requestWindowFeature(Window.FEATURE_NO_TITLE); setContentView(R.layout.activity_main); connectButton=(Button)findViewById(R.id.btnConnect); registerButton=(Button)findViewById(R.id.btnRegister); noteUser=(EditText)findViewById(R.id.editUserName); notePassword=(EditText)findViewById(R.id.editPassword); //////注意类Class DriverManager Statement ResultSet connectButton.setOnClickListener(new View.OnClickListener() &#123; public void onClick(View v) &#123; Toast.makeText(MainActivity.this, \"正在登录\", Toast.LENGTH_SHORT).show(); name = noteUser.getText().toString().trim(); code = notePassword.getText().toString().trim(); chose=\"1\"; new SubmitAsyncTask().execute(URL); Toast.makeText(MainActivity.this, \"res = \"+res, Toast.LENGTH_SHORT).show(); if(res.equals(\"2\"))&#123; Toast.makeText(MainActivity.this, \"登录成功\", Toast.LENGTH_SHORT).show(); //Intent intent=new Intent(); //intent.setClass(MainActivity.this, WriteNote.class); //MainActivity.this.startActivity(intent); &#125;else if(res.equals(\"1\"))&#123; Toast.makeText(MainActivity.this, \"用户名或密码错误\", Toast.LENGTH_SHORT).show(); &#125;else if(res.equals(\"3\"))&#123; Toast.makeText(MainActivity.this, \"注册失败\", Toast.LENGTH_SHORT).show(); &#125;else if(res.equals(\"4\"))&#123; Toast.makeText(MainActivity.this, \"注册成功\", Toast.LENGTH_SHORT).show(); &#125;else if(res.equals(\"-1\"))&#123; Toast.makeText(MainActivity.this, \"网络异常\", Toast.LENGTH_SHORT).show(); &#125; &#125; &#125;); noteUser.setText(\"ab\"); notePassword.setText(\"123456\"); registerButton.setOnClickListener(new View.OnClickListener() &#123; public void onClick(View v) &#123; Intent intent=new Intent(); intent.setClass(MainActivity.this, Register.class); MainActivity.this.startActivity(intent); &#125; &#125;); &#125; @Override public boolean onCreateOptionsMenu(Menu menu) &#123; // Inflate the menu; this adds items to the action bar if it is present. getMenuInflater().inflate(R.menu.main, menu); return true; &#125;&#125; 以上为手机主界面函数调用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.example.java4androidmysql;import android.app.Activity;import android.os.Bundle;import android.view.View;import android.view.Window;import android.widget.Button;import android.widget.EditText;import android.widget.Toast;public class Register extends Activity&#123; public static final String URL = \"http://192.168.2.1:8080/JavaRegisterMysql/login\"; Button backButton=null; Button registerButton=null; EditText user=null; EditText password=null; EditText phone=null; @Override protected void onCreate(Bundle savedInstanceState) &#123; // TODO Auto-generated method stub super.onCreate(savedInstanceState); requestWindowFeature(Window.FEATURE_NO_TITLE); setContentView(R.layout.register); backButton=(Button)findViewById(R.id.btnBack); registerButton=(Button)findViewById(R.id.btnRegister); user=(EditText)findViewById(R.id.editUserName); password=(EditText)findViewById(R.id.editPassword); phone=(EditText)findViewById(R.id.editPhone); //////注意类Class DriverManager Statement ResultSet registerButton.setOnClickListener(new View.OnClickListener() &#123; public void onClick(View v) &#123; MainActivity.name = user.getText().toString().trim(); MainActivity.code = password.getText().toString().trim(); MainActivity.phone = phone.getText().toString().trim(); MainActivity.chose=\"2\"; new SubmitAsyncTask().execute(URL); Toast.makeText(Register.this, \"res = \"+MainActivity.res, Toast.LENGTH_SHORT).show(); if(MainActivity.res.equals(\"2\"))&#123; Toast.makeText(Register.this, \"登录成功\", Toast.LENGTH_SHORT).show(); //Intent intent=new Intent(); //intent.setClass(MainActivity.this, WriteNote.class); //MainActivity.this.startActivity(intent); &#125;else if(MainActivity.res.equals(\"1\"))&#123; Toast.makeText(Register.this, \"用户名或密码错误\", Toast.LENGTH_SHORT).show(); &#125;else if(MainActivity.res.equals(\"3\"))&#123; Toast.makeText(Register.this, \"注册失败\", Toast.LENGTH_SHORT).show(); &#125;else if(MainActivity.res.equals(\"4\"))&#123; Toast.makeText(Register.this, \"注册成功\", Toast.LENGTH_SHORT).show(); &#125;else if(MainActivity.res.equals(\"-1\"))&#123; Toast.makeText(Register.this, \"网络异常\", Toast.LENGTH_SHORT).show(); &#125; &#125; &#125;); user.setText(\"gjw\"); password.setText(\"123\"); phone.setText(\"13570236302\"); &#125;&#125; 以上为注册界面 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package com.example.java4androidmysql;import java.io.IOException;import java.io.UnsupportedEncodingException;import java.util.ArrayList;import java.util.List;import org.apache.http.HttpResponse;import org.apache.http.HttpStatus;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.HttpClient;import org.apache.http.client.entity.UrlEncodedFormEntity;import org.apache.http.client.methods.HttpPost;import org.apache.http.conn.params.ConnManagerParams;import org.apache.http.impl.client.DefaultHttpClient;import org.apache.http.message.BasicNameValuePair;import org.apache.http.params.BasicHttpParams;import org.apache.http.params.HttpConnectionParams;import org.apache.http.params.HttpParams;import org.apache.http.protocol.HTTP;import org.apache.http.util.EntityUtils;import android.os.AsyncTask;public class SubmitAsyncTask extends AsyncTask&lt;String, Integer, String&gt;&#123; String info = \"\"; protected String doInBackground(String... params) &#123; // TODO Auto-generated method stub String url = params[0]; String reps = \"\"; reps = doPost(url); return reps; &#125; protected void onPostExecute(String result) &#123; // TODO Auto-generated method stub MainActivity.res = result.trim(); super.onPostExecute(result); &#125;/** * 用Post方式跟服务器传递数据 * @param url * @return */private String doPost(String url)&#123; String responseStr = \"\"; try &#123; //发送post类型请求 HttpPost httpRequest = new HttpPost(url); HttpParams params = new BasicHttpParams(); ConnManagerParams.setTimeout(params, 1000); //从连接池中获取连接的超时时间 HttpConnectionParams.setConnectionTimeout(params, 3000);//通过网络与服务器建立连接的超时时间 HttpConnectionParams.setSoTimeout(params, 5000);//读响应数据的超时时间 httpRequest.setParams(params); //下面开始跟服务器传递数据，使用BasicNameValuePair List&lt;BasicNameValuePair&gt; paramsList = new ArrayList&lt;BasicNameValuePair&gt;(); paramsList.add(new BasicNameValuePair(\"NAME\", MainActivity.name)); paramsList.add(new BasicNameValuePair(\"CODE\", MainActivity.code)); paramsList.add(new BasicNameValuePair(\"PHONE\", MainActivity.phone)); paramsList.add(new BasicNameValuePair(\"CHOSE\", MainActivity.chose)); UrlEncodedFormEntity mUrlEncodeFormEntity = new UrlEncodedFormEntity(paramsList, HTTP.UTF_8); httpRequest.setEntity(mUrlEncodeFormEntity); //////////////////////////////////////////////// HttpClient httpClient = new DefaultHttpClient(); HttpResponse httpResponse = httpClient.execute(httpRequest); final int ret = httpResponse.getStatusLine().getStatusCode(); if(ret == HttpStatus.SC_OK)&#123; responseStr = EntityUtils.toString(httpResponse.getEntity(), HTTP.UTF_8); &#125;else&#123; responseStr = \"-1\"; &#125; &#125; catch (UnsupportedEncodingException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (ClientProtocolException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return responseStr; &#125;&#125; 以上为接口类，但写得可能有些不对（如返回变量那里是上次调用的），略忙，等再改一下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"fill_parent\" android:layout_height=\"fill_parent\" android:orientation=\"vertical\" android:background=\"@drawable/ab\" &gt; &lt;TextView android:id=\"@+id/FRppt01\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:text=\"@string/Title\" android:textSize=\"15pt\" android:textColor=\"#FFFFFF\" android:textStyle=\"bold\" android:layout_marginTop=\"40dp\" android:gravity=\"center_horizontal\" android:padding=\"3dip\"/&gt; &lt;EditText android:id=\"@+id/editUserName\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:hint=\"输入用户名\"/&gt; &lt;EditText android:id=\"@+id/editPassword\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:hint=\"输入密码\"/&gt; &lt;TableLayout android:layout_width=\"fill_parent\" android:layout_height=\"fill_parent\"&gt; &lt;TableRow&gt; &lt;Button android:id=\"@+id/btnConnect\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:text=\"登录\" android:layout_marginTop=\"260dp\" android:layout_weight=\"1\" /&gt; &lt;Button android:id=\"@+id/btnRegister\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:text=\"注册\" android:layout_marginTop=\"260dp\" android:layout_weight=\"1\"/&gt;\" &lt;/TableRow&gt; &lt;/TableLayout&gt;&lt;/LinearLayout&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"fill_parent\" android:layout_height=\"fill_parent\" android:orientation=\"vertical\" android:background=\"@drawable/ab\" &gt; &lt;TextView android:id=\"@+id/FRppt01\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:text=\"@string/RegisterPage\" android:textSize=\"15pt\" android:textColor=\"#FFFFFF\" android:textStyle=\"bold\" android:layout_marginTop=\"40dp\" android:gravity=\"center_horizontal\" android:padding=\"3dip\"/&gt; &lt;TableLayout android:layout_below=\"@id/FRppt01\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:stretchColumns=\"1\" android:layout_marginLeft=\"10dip\" android:layout_marginRight=\"10dip\" &gt; &lt;TableRow android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:layout_marginBottom=\"5dip\"&gt; &lt;TextView android:id=\"@+id/textUserName\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:textSize=\"12pt\" android:layout_weight=\"1\" android:layout_below=\"@id/FRppt01\" android:text=\"@string/userName\"/&gt;\" &lt;EditText android:id=\"@+id/editUserName\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:layout_weight=\"1\" /&gt; &lt;/TableRow&gt; &lt;TableRow android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:layout_marginBottom=\"5dip\"&gt; &lt;TextView android:id=\"@+id/textPassword\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_weight=\"1\" android:textSize=\"12pt\" android:text=\"@string/passWord\"/&gt; &lt;EditText android:id=\"@+id/editPassword\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:layout_weight=\"1\" /&gt; &lt;/TableRow&gt; &lt;TableRow android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:layout_marginBottom=\"5dip\"&gt; &lt;TextView android:id=\"@+id/textPhone\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_weight=\"1\" android:textSize=\"12pt\" android:text=\"@string/phone\"/&gt; &lt;EditText android:id=\"@+id/editPhone\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:layout_weight=\"1\" /&gt; &lt;/TableRow&gt; &lt;/TableLayout&gt; &lt;TableLayout android:layout_width=\"fill_parent\" android:layout_height=\"fill_parent\" &gt; &lt;TableRow&gt; &lt;Button android:id=\"@+id/btnBack\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:layout_marginTop=\"260dp\" android:layout_weight=\"1\" android:text=\"@string/Back\" /&gt; &lt;Button android:id=\"@+id/btnRegister\" android:layout_width=\"fill_parent\" android:layout_height=\"wrap_content\" android:layout_marginTop=\"260dp\" android:layout_weight=\"1\" android:text=\"@string/Register\" /&gt; &lt;/TableRow&gt; &lt;/TableLayout&gt;&lt;/RelativeLayout&gt; 以上两段代码为布局文件代码。","categories":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://enjoyhot.github.io/tags/Android/"},{"name":"tomcat","slug":"tomcat","permalink":"http://enjoyhot.github.io/tags/tomcat/"},{"name":"servlet","slug":"servlet","permalink":"http://enjoyhot.github.io/tags/servlet/"}]}]}